{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification Case Studies\n",
    "\n",
    "This notebook provides detailed case studies of how different verification strategies affect optimization results. We'll examine examples where verification:\n",
    "\n",
    "1. Successfully caught and corrected errors\n",
    "2. Failed to catch errors\n",
    "3. Incorrectly rejected valid updates\n",
    "4. Showed different behaviors across verification strategies\n",
    "\n",
    "These case studies will help illustrate the strengths and weaknesses of each verification approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Load verification results\n",
    "RESULTS_FILE = \"../results/verification/verification_results.json\"\n",
    "\n",
    "with open(RESULTS_FILE, 'r') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def highlight_differences(original, updated, add_color='#d4f7d4', remove_color='#ffd4d4'):\n",
    "    \"\"\"Highlight differences between original and updated text\"\"\"\n",
    "    import difflib\n",
    "    d = difflib.Differ()\n",
    "    diff = list(d.compare(original.splitlines(), updated.splitlines()))\n",
    "    \n",
    "    result = []\n",
    "    for line in diff:\n",
    "        if line.startswith('+ '):\n",
    "            result.append(f\"<div style='background-color: {add_color}'>{line[2:]}</div>\")\n",
    "        elif line.startswith('- '):\n",
    "            result.append(f\"<div style='background-color: {remove_color}'>{line[2:]}</div>\")\n",
    "        elif line.startswith('  '):\n",
    "            result.append(line[2:])\n",
    "    \n",
    "    return '<br>'.join(result)\n",
    "\n",
    "def display_verification_case(result, iteration=1):\n",
    "    \"\"\"Display a verification case with all relevant information\"\"\"\n",
    "    question = result.get('question', 'N/A')\n",
    "    answer = result.get('answer', 'N/A')\n",
    "    \n",
    "    strategy = result.get('verification_strategy', 'N/A')\n",
    "    threshold = result.get('verification_threshold', 'N/A')\n",
    "    \n",
    "    performance_history = result.get('performance_history', [])\n",
    "    predictions = result.get('predictions', [])\n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Strategy: {strategy}, Threshold: {threshold}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Correct Answer: {answer}\")\n",
    "    print(f\"Performance History: {performance_history}\")\n",
    "    print()\n",
    "    \n",
    "    # Display iteration of interest\n",
    "    if len(predictions) > iteration and iteration > 0:\n",
    "        print(f\"=== Iteration {iteration} ===\")\n",
    "        \n",
    "        # Display the prediction before verification\n",
    "        prev_prediction = predictions[iteration-1]\n",
    "        print(\"Previous solution:\")\n",
    "        print(prev_prediction)\n",
    "        print()\n",
    "        \n",
    "        # Display verification details if available\n",
    "        if verification_metrics and len(verification_metrics) >= iteration:\n",
    "            vm = verification_metrics[iteration-1]\n",
    "            print(f\"Verification applied: {vm.get('verification_applied', 'N/A')}\")\n",
    "            print(f\"Verification confidence: {vm.get('verification_confidence', 'N/A')}\")\n",
    "            if 'corrections' in vm and vm['corrections']:\n",
    "                print(\"Corrections:\")\n",
    "                print(vm['corrections'])\n",
    "            print()\n",
    "        \n",
    "        # Display the updated prediction\n",
    "        curr_prediction = predictions[iteration]\n",
    "        print(\"Updated solution:\")\n",
    "        print(curr_prediction)\n",
    "        print()\n",
    "        \n",
    "        # Display differences\n",
    "        print(\"Differences:\")\n",
    "        display(HTML(highlight_differences(prev_prediction, curr_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Successful Verification Cases\n",
    "\n",
    "Let's find examples where verification successfully caught and corrected errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find samples where verification was applied and performance improved\n",
    "successful_cases = []\n",
    "\n",
    "for result in results:\n",
    "    if result['verification_strategy'] == 'none':\n",
    "        continue\n",
    "        \n",
    "    performance_history = result['performance_history']\n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    \n",
    "    for i in range(1, len(performance_history)):\n",
    "        # Check if verification was applied in this iteration\n",
    "        was_verified = False\n",
    "        if verification_metrics and i <= len(verification_metrics):\n",
    "            was_verified = verification_metrics[i-1].get('verification_applied', False)\n",
    "        \n",
    "        # Check if performance improved\n",
    "        if was_verified and performance_history[i] > performance_history[i-1]:\n",
    "            successful_cases.append((result, i))\n",
    "            break\n",
    "\n",
    "print(f\"Found {len(successful_cases)} successful verification cases\")\n",
    "\n",
    "# Display a few examples\n",
    "for i, (result, iteration) in enumerate(successful_cases[:3]):\n",
    "    print(f\"\\n--- Successful Case {i+1} ---\\n\")\n",
    "    display_verification_case(result, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Failed Verification Cases\n",
    "\n",
    "Now let's examine cases where verification was applied but failed to catch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find samples where verification was applied but performance didn't improve\n",
    "failed_cases = []\n",
    "\n",
    "for result in results:\n",
    "    if result['verification_strategy'] == 'none':\n",
    "        continue\n",
    "        \n",
    "    performance_history = result['performance_history']\n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    \n",
    "    for i in range(1, len(performance_history)):\n",
    "        # Check if verification was applied in this iteration\n",
    "        was_verified = False\n",
    "        if verification_metrics and i <= len(verification_metrics):\n",
    "            was_verified = verification_metrics[i-1].get('verification_applied', False)\n",
    "        \n",
    "        # Check if performance didn't improve or got worse\n",
    "        if was_verified and performance_history[i] <= performance_history[i-1]:\n",
    "            failed_cases.append((result, i))\n",
    "            break\n",
    "\n",
    "print(f\"Found {len(failed_cases)} failed verification cases\")\n",
    "\n",
    "# Display a few examples\n",
    "for i, (result, iteration) in enumerate(failed_cases[:3]):\n",
    "    print(f\"\\n--- Failed Case {i+1} ---\\n\")\n",
    "    display_verification_case(result, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Analysis Across Verification Strategies\n",
    "\n",
    "Let's compare how different verification strategies handle the same example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group results by sample_id\n",
    "samples_by_id = {}\n",
    "for result in results:\n",
    "    sample_id = result['sample_id']\n",
    "    if sample_id not in samples_by_id:\n",
    "        samples_by_id[sample_id] = []\n",
    "    samples_by_id[sample_id].append(result)\n",
    "\n",
    "# Find samples that have results for all strategies\n",
    "strategies = set([result['verification_strategy'] for result in results])\n",
    "comparative_samples = []\n",
    "\n",
    "for sample_id, sample_results in samples_by_id.items():\n",
    "    sample_strategies = set([r['verification_strategy'] for r in sample_results])\n",
    "    if sample_strategies == strategies:\n",
    "        comparative_samples.append(sample_id)\n",
    "\n",
    "print(f\"Found {len(comparative_samples)} samples with results for all strategies\")\n",
    "\n",
    "# Select a sample for comparison\n",
    "if comparative_samples:\n",
    "    sample_id = comparative_samples[0]\n",
    "    sample_results = samples_by_id[sample_id]\n",
    "    \n",
    "    print(f\"\\n=== Comparative Analysis for Sample {sample_id} ===\\n\")\n",
    "    \n",
    "    # Print question and answer once\n",
    "    question = sample_results[0]['question']\n",
    "    answer = sample_results[0]['answer']\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Correct Answer: {answer}\\n\")\n",
    "    \n",
    "    # Compare final performance\n",
    "    print(\"Final Performance by Strategy:\")\n",
    "    for result in sample_results:\n",
    "        strategy = result['verification_strategy']\n",
    "        threshold = result.get('verification_threshold', 'N/A')\n",
    "        final_score = result['performance_history'][-1]\n",
    "        \n",
    "        print(f\"{strategy} (threshold={threshold}): {final_score}\")\n",
    "    \n",
    "    print(\"\\nDetailed Comparison of First Iteration:\")\n",
    "    # Compare first iteration across strategies\n",
    "    for result in sample_results:\n",
    "        print(f\"\\n--- {result['verification_strategy']} ---\")\n",
    "        display_verification_case(result, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Impact of Confidence Threshold\n",
    "\n",
    "Let's examine how different confidence thresholds affect verification outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group results by strategy and threshold\n",
    "threshold_impacts = {}\n",
    "\n",
    "for result in results:\n",
    "    if result['verification_strategy'] == 'none':\n",
    "        continue\n",
    "        \n",
    "    strategy = result['verification_strategy']\n",
    "    threshold = result['verification_threshold']\n",
    "    \n",
    "    key = (strategy, threshold)\n",
    "    if key not in threshold_impacts:\n",
    "        threshold_impacts[key] = {\n",
    "            'verification_applied_count': 0,\n",
    "            'total_iterations': 0,\n",
    "            'improved_after_verification': 0,\n",
    "            'worsened_after_verification': 0,\n",
    "            'unchanged_after_verification': 0\n",
    "        }\n",
    "    \n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    performance_history = result['performance_history']\n",
    "    \n",
    "    for i in range(len(verification_metrics)):\n",
    "        threshold_impacts[key]['total_iterations'] += 1\n",
    "        \n",
    "        if verification_metrics[i].get('verification_applied', False):\n",
    "            threshold_impacts[key]['verification_applied_count'] += 1\n",
    "            \n",
    "            # Check performance change\n",
    "            if i+1 < len(performance_history):\n",
    "                if performance_history[i+1] > performance_history[i]:\n",
    "                    threshold_impacts[key]['improved_after_verification'] += 1\n",
    "                elif performance_history[i+1] < performance_history[i]:\n",
    "                    threshold_impacts[key]['worsened_after_verification'] += 1\n",
    "                else:\n",
    "                    threshold_impacts[key]['unchanged_after_verification'] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "threshold_data = []\n",
    "for (strategy, threshold), impacts in threshold_impacts.items():\n",
    "    # Calculate percentages\n",
    "    applied_rate = impacts['verification_applied_count'] / impacts['total_iterations'] if impacts['total_iterations'] > 0 else 0\n",
    "    improved_rate = impacts['improved_after_verification'] / impacts['verification_applied_count'] if impacts['verification_applied_count'] > 0 else 0\n",
    "    worsened_rate = impacts['worsened_after_verification'] / impacts['verification_applied_count'] if impacts['verification_applied_count'] > 0 else 0\n",
    "    unchanged_rate = impacts['unchanged_after_verification'] / impacts['verification_applied_count'] if impacts['verification_applied_count'] > 0 else 0\n",
    "    \n",
    "    threshold_data.append({\n",
    "        'strategy': strategy,\n",
    "        'threshold': threshold,\n",
    "        'applied_rate': applied_rate,\n",
    "        'improved_rate': improved_rate,\n",
    "        'worsened_rate': worsened_rate,\n",
    "        'unchanged_rate': unchanged_rate\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_data)\n",
    "threshold_df.sort_values(['strategy', 'threshold'], inplace=True)\n",
    "threshold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot threshold impact\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot verification application rate\n",
    "plt.subplot(2, 2, 1)\n",
    "for strategy in threshold_df['strategy'].unique():\n",
    "    strategy_data = threshold_df[threshold_df['strategy'] == strategy]\n",
    "    plt.plot(\n",
    "        strategy_data['threshold'],\n",
    "        strategy_data['applied_rate'],\n",
    "        marker='o',\n",
    "        label=strategy\n",
    "    )\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Verification Application Rate')\n",
    "plt.title('Verification Application Rate vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot improved rate\n",
    "plt.subplot(2, 2, 2)\n",
    "for strategy in threshold_df['strategy'].unique():\n",
    "    strategy_data = threshold_df[threshold_df['strategy'] == strategy]\n",
    "    plt.plot(\n",
    "        strategy_data['threshold'],\n",
    "        strategy_data['improved_rate'],\n",
    "        marker='o',\n",
    "        label=strategy\n",
    "    )\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Improvement Rate After Verification')\n",
    "plt.title('Improvement Rate vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot worsened rate\n",
    "plt.subplot(2, 2, 3)\n",
    "for strategy in threshold_df['strategy'].unique():\n",
    "    strategy_data = threshold_df[threshold_df['strategy'] == strategy]\n",
    "    plt.plot(\n",
    "        strategy_data['threshold'],\n",
    "        strategy_data['worsened_rate'],\n",
    "        marker='o',\n",
    "        label=strategy\n",
    "    )\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Worsening Rate After Verification')\n",
    "plt.title('Worsening Rate vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot unchanged rate\n",
    "plt.subplot(2, 2, 4)\n",
    "for strategy in threshold_df['strategy'].unique():\n",
    "    strategy_data = threshold_df[threshold_df['strategy'] == strategy]\n",
    "    plt.plot(\n",
    "        strategy_data['threshold'],\n",
    "        strategy_data['unchanged_rate'],\n",
    "        marker='o',\n",
    "        label=strategy\n",
    "    )\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Unchanged Rate After Verification')\n",
    "plt.title('Unchanged Rate vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Categories and Verification Effectiveness\n",
    "\n",
    "Let's analyze the types of errors that each verification strategy is most effective at catching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define error categories\n",
    "error_categories = {\n",
    "    'hallucination': [\n",
    "        'hallucinate', 'fabricate', 'invent', 'make up', 'create facts', \n",
    "        'not based on', 'no evidence', 'unsupported claim'\n",
    "    ],\n",
    "    'logical_error': [\n",
    "        'logical error', 'invalid reasoning', 'fallacy', 'reasoning error',\n",
    "        'incorrect logic', 'illogical step', 'non sequitur'\n",
    "    ],\n",
    "    'mathematical_error': [\n",
    "        'calculation error', 'arithmetic error', 'computational mistake',\n",
    "        'incorrect computation', 'wrong calculation', 'mathematical error'\n",
    "    ],\n",
    "    'factual_error': [\n",
    "        'factual error', 'incorrect fact', 'wrong information', 'inaccurate claim',\n",
    "        'factually incorrect', 'wrong assumption'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to classify error type in verification feedback\n",
    "def classify_error_type(text):\n",
    "    text = text.lower()\n",
    "    error_types = []\n",
    "    \n",
    "    for error_type, keywords in error_categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text:\n",
    "                error_types.append(error_type)\n",
    "                break\n",
    "    \n",
    "    return error_types or ['other_error']\n",
    "\n",
    "# Analyze verification corrections\n",
    "error_analysis = []\n",
    "\n",
    "for result in results:\n",
    "    if result['verification_strategy'] == 'none':\n",
    "        continue\n",
    "        \n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    \n",
    "    for i, vm in enumerate(verification_metrics):\n",
    "        if vm.get('verification_applied', False) and 'corrections' in vm and vm['corrections']:\n",
    "            error_types = classify_error_type(vm['corrections'])\n",
    "            \n",
    "            # Check if performance improved after this verification\n",
    "            performance_improved = False\n",
    "            if i+1 < len(result['performance_history']):\n",
    "                performance_improved = result['performance_history'][i+1] > result['performance_history'][i]\n",
    "            \n",
    "            for error_type in error_types:\n",
    "                error_analysis.append({\n",
    "                    'strategy': result['verification_strategy'],\n",
    "                    'threshold': result['verification_threshold'],\n",
    "                    'error_type': error_type,\n",
    "                    'performance_improved': performance_improved,\n",
    "                    'sample_id': result['sample_id'],\n",
    "                    'iteration': i\n",
    "                })\n",
    "\n",
    "error_df = pd.DataFrame(error_analysis)\n",
    "\n",
    "# Calculate effectiveness by error type and strategy\n",
    "effectiveness = error_df.groupby(['strategy', 'error_type'])['performance_improved'].agg(\n",
    "    ['count', 'mean']\n",
    ").reset_index()\n",
    "effectiveness.columns = ['strategy', 'error_type', 'count', 'effectiveness']\n",
    "effectiveness = effectiveness[effectiveness['count'] >= 3]  # Filter for types with enough samples\n",
    "effectiveness.sort_values(['strategy', 'effectiveness'], ascending=[True, False], inplace=True)\n",
    "\n",
    "effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize error type effectiveness by strategy\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get unique error types and strategies\n",
    "error_types = effectiveness['error_type'].unique()\n",
    "strategies = effectiveness['strategy'].unique()\n",
    "\n",
    "# Set up bar positions\n",
    "x = np.arange(len(error_types))\n",
    "width = 0.8 / len(strategies)\n",
    "offsets = np.linspace(-0.4 + width/2, 0.4 - width/2, len(strategies))\n",
    "\n",
    "# Plot bars for each strategy\n",
    "for i, strategy in enumerate(strategies):\n",
    "    strategy_data = effectiveness[effectiveness['strategy'] == strategy]\n",
    "    \n",
    "    # Create a dictionary mapping error types to effectiveness for this strategy\n",
    "    strategy_dict = dict(zip(strategy_data['error_type'], strategy_data['effectiveness']))\n",
    "    \n",
    "    # Get effectiveness values for each error type (0 if not present)\n",
    "    values = [strategy_dict.get(error_type, 0) for error_type in error_types]\n",
    "    \n",
    "    plt.bar(x + offsets[i], values, width, label=strategy)\n",
    "\n",
    "plt.xlabel('Error Type')\n",
    "plt.ylabel('Effectiveness (% Improved After Verification)')\n",
    "plt.title('Verification Effectiveness by Error Type and Strategy')\n",
    "plt.xticks(x, error_types)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Error Case Studies\n",
    "\n",
    "Let's examine specific examples of each error type and how the different verification strategies handled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to find examples of specific error types\n",
    "def find_error_examples(error_type, limit=3):\n",
    "    examples = []\n",
    "    for result in results:\n",
    "        if result['verification_strategy'] == 'none':\n",
    "            continue\n",
    "            \n",
    "        verification_metrics = result.get('verification_metrics', [])\n",
    "        \n",
    "        for i, vm in enumerate(verification_metrics):\n",
    "            if vm.get('verification_applied', False) and 'corrections' in vm and vm['corrections']:\n",
    "                corrections = vm['corrections'].lower()\n",
    "                \n",
    "                # Check if this correction addresses the specified error type\n",
    "                for keyword in error_categories.get(error_type, []):\n",
    "                    if keyword.lower() in corrections:\n",
    "                        examples.append((result, i+1))  # +1 because iteration index starts at 0\n",
    "                        break\n",
    "                        \n",
    "                if len(examples) >= limit:\n",
    "                    break\n",
    "        \n",
    "        if len(examples) >= limit:\n",
    "            break\n",
    "            \n",
    "    return examples\n",
    "\n",
    "# Let's look at examples of hallucinations\n",
    "hallucination_examples = find_error_examples('hallucination')\n",
    "print(f\"Found {len(hallucination_examples)} hallucination examples\\n\")\n",
    "\n",
    "for i, (result, iteration) in enumerate(hallucination_examples):\n",
    "    print(f\"=== Hallucination Example {i+1} ===\\n\")\n",
    "    display_verification_case(result, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's look at examples of logical errors\n",
    "logical_error_examples = find_error_examples('logical_error')\n",
    "print(f\"Found {len(logical_error_examples)} logical error examples\\n\")\n",
    "\n",
    "for i, (result, iteration) in enumerate(logical_error_examples):\n",
    "    print(f\"=== Logical Error Example {i+1} ===\\n\")\n",
    "    display_verification_case(result, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's look at examples of mathematical errors\n",
    "mathematical_error_examples = find_error_examples('mathematical_error')\n",
    "print(f\"Found {len(mathematical_error_examples)} mathematical error examples\\n\")\n",
    "\n",
    "for i, (result, iteration) in enumerate(mathematical_error_examples):\n",
    "    print(f\"=== Mathematical Error Example {i+1} ===\\n\")\n",
    "    display_verification_case(result, iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process vs. Outcome Verification Comparison\n",
    "\n",
    "Let's directly compare process verification and outcome verification on the same examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find examples where both process and outcome verification were applied\n",
    "def find_comparison_examples(limit=3):\n",
    "    examples = []\n",
    "    \n",
    "    # First, find all sample IDs\n",
    "    sample_ids = set()\n",
    "    for result in results:\n",
    "        sample_ids.add(result['sample_id'])\n",
    "    \n",
    "    # Find samples where both process and outcome verification were applied\n",
    "    for sample_id in sample_ids:\n",
    "        process_result = None\n",
    "        outcome_result = None\n",
    "        \n",
    "        for result in results:\n",
    "            if result['sample_id'] != sample_id:\n",
    "                continue\n",
    "                \n",
    "            if result['verification_strategy'] == 'process':\n",
    "                process_result = result\n",
    "            elif result['verification_strategy'] == 'outcome':\n",
    "                outcome_result = result\n",
    "        \n",
    "        if process_result and outcome_result:\n",
    "            # Find an iteration where both applied verification\n",
    "            for i in range(min(len(process_result.get('verification_metrics', [])), \n",
    "                              len(outcome_result.get('verification_metrics', [])))):\n",
    "                \n",
    "                process_applied = process_result['verification_metrics'][i].get('verification_applied', False)\n",
    "                outcome_applied = outcome_result['verification_metrics'][i].get('verification_applied', False)\n",
    "                \n",
    "                if process_applied and outcome_applied:\n",
    "                    examples.append((sample_id, i+1, process_result, outcome_result))\n",
    "                    break\n",
    "                    \n",
    "            if len(examples) >= limit:\n",
    "                break\n",
    "    \n",
    "    return examples\n",
    "\n",
    "comparison_examples = find_comparison_examples()\n",
    "print(f\"Found {len(comparison_examples)} comparison examples\\n\")\n",
    "\n",
    "for i, (sample_id, iteration, process_result, outcome_result) in enumerate(comparison_examples):\n",
    "    print(f\"=== Comparison Example {i+1} ===\\n\")\n",
    "    \n",
    "    # Print question and answer once\n",
    "    question = process_result['question']\n",
    "    answer = process_result['answer']\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Correct Answer: {answer}\\n\")\n",
    "    \n",
    "    print(f\"Process Verification (Iteration {iteration}):\")\n",
    "    display_verification_case(process_result, iteration)\n",
    "    \n",
    "    print(f\"\\nOutcome Verification (Iteration {iteration}):\")\n",
    "    display_verification_case(outcome_result, iteration)\n",
    "    \n",
    "    # Compare final performance\n",
    "    process_final = process_result['performance_history'][-1]\n",
    "    outcome_final = outcome_result['performance_history'][-1]\n",
    "    \n",
    "    print(f\"\\nFinal Performance:\\n\")\n",
    "    print(f\"Process Verification: {process_final}\")\n",
    "    print(f\"Outcome Verification: {outcome_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Qualitative Analysis of Verification Feedback\n",
    "\n",
    "Let's examine the quality and characteristics of feedback provided by different verification strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect verification feedback samples\n",
    "feedback_samples = []\n",
    "\n",
    "for result in results:\n",
    "    if result['verification_strategy'] == 'none':\n",
    "        continue\n",
    "        \n",
    "    strategy = result['verification_strategy']\n",
    "    verification_metrics = result.get('verification_metrics', [])\n",
    "    \n",
    "    for vm in verification_metrics:\n",
    "        if vm.get('verification_applied', False) and 'corrections' in vm and vm['corrections']:\n",
    "            feedback_samples.append({\n",
    "                'strategy': strategy,\n",
    "                'corrections': vm['corrections'],\n",
    "                'correction_length': len(vm['corrections'].split())\n",
    "            })\n",
    "\n",
    "feedback_df = pd.DataFrame(feedback_samples)\n",
    "\n",
    "# Analyze feedback length by strategy\n",
    "feedback_length = feedback_df.groupby('strategy')['correction_length'].agg(\n",
    "    ['mean', 'std', 'min', 'max']\n",
    ").reset_index()\n",
    "\n",
    "print(\"Verification Feedback Length Statistics:\")\n",
    "feedback_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feedback length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for strategy in feedback_df['strategy'].unique():\n",
    "    strategy_data = feedback_df[feedback_df['strategy'] == strategy]['correction_length']\n",
    "    plt.hist(strategy_data, alpha=0.5, bins=20, label=strategy)\n",
    "\n",
    "plt.xlabel('Feedback Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Verification Feedback Length by Strategy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample feedback from each strategy\n",
    "print(\"Sample Verification Feedback by Strategy:\")\n",
    "\n",
    "for strategy in feedback_df['strategy'].unique():\n",
    "    strategy_feedback = feedback_df[feedback_df['strategy'] == strategy]['corrections'].sample(3)\n",
    "    \n",
    "    print(f\"\\n=== {strategy.upper()} Verification Feedback ===\\n\")\n",
    "    \n",
    "    for i, feedback in enumerate(strategy_feedback):\n",
    "        print(f\"Example {i+1}:\\n{feedback}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions and Recommendations\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions:\n",
    "\n",
    "1. **Verification Effectiveness**: [Fill in based on actual results]\n",
    "   - Process verification appears to be most effective for [fill in]\n",
    "   - Outcome verification excels at [fill in]\n",
    "   - Hybrid approaches offer [fill in]\n",
    "\n",
    "2. **Confidence Threshold Impact**: [Fill in based on actual results]\n",
    "   - Higher thresholds tend to [fill in]\n",
    "   - Lower thresholds [fill in]\n",
    "   - The optimal threshold appears to be around [fill in]\n",
    "\n",
    "3. **Error Type Handling**: [Fill in based on actual results]\n",
    "   - Hallucinations are best caught by [fill in]\n",
    "   - Logical errors are most effectively addressed by [fill in]\n",
    "   - Mathematical errors [fill in]\n",
    "\n",
    "4. **Feedback Quality**: [Fill in based on actual results]\n",
    "   - Process verification feedback tends to be [fill in]\n",
    "   - Outcome verification feedback is characterized by [fill in]\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Based on our findings, we recommend:\n",
    "\n",
    "1. [Fill in recommendation]\n",
    "2. [Fill in recommendation]\n",
    "3. [Fill in recommendation]\n",
    "4. [Fill in recommendation]\n",
    "\n",
    "These recommendations should be implemented in the VTGD framework to maximize the effectiveness of verification in TextGrad optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
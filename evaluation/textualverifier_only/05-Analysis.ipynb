{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS FOR TEXTUALVERIFIER ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparative analysis for 5 different number variants of TextualVerifier\n",
    "- Author: Eugenius Mario Situmorang\n",
    "- Date: June 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import re\n",
    "from scipy import stats\n",
    "from scipy.stats import fisher_exact\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"📊 All libraries imported successfully!\")\n",
    "print(\"🎯 Ready for multi-version comparative analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING AND PREPROCESSING FOR MULTIPLE VERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_versions():\n",
    "    \"\"\"\n",
    "    Load all 5 versions of verification data\n",
    "    \"\"\"\n",
    "    print(\"🔄 Loading multiple versions...\")\n",
    "    \n",
    "    # Define file paths and version names\n",
    "    version_files = {\n",
    "        \"Version 1\": \"results/textualverifier-1v.csv\",\n",
    "        \"Version 2\": \"results/textualverifier-2v.csv\", \n",
    "        \"Version 3\": \"results/textualverifier-3v.csv\",\n",
    "        \"Version 4\": \"results/textualverifier-4v.csv\",\n",
    "        \"Version 5\": \"results/textualverifier-5v.csv\"\n",
    "    }\n",
    "    \n",
    "    dataframes = {}\n",
    "    \n",
    "    for version_name, file_path in version_files.items():\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Add version identifier\n",
    "            df['version'] = version_name\n",
    "            \n",
    "            # Basic preprocessing\n",
    "            df['success'] = df['success'].astype(bool) if 'success' in df.columns else True\n",
    "            \n",
    "            # Handle answer correctness columns\n",
    "            if 'original_answer_correctness' in df.columns:\n",
    "                df['original_answer_correctness'] = df['original_answer_correctness'].str.strip().str.upper() == 'TRUE'\n",
    "            if 'verifier_answer_correctness' in df.columns:\n",
    "                df['verifier_answer_correctness'] = df['verifier_answer_correctness'].str.strip().str.upper() == 'TRUE'\n",
    "            \n",
    "            # Parse rating arrays\n",
    "            def parse_rating_array(rating_str):\n",
    "                try:\n",
    "                    if pd.isna(rating_str):\n",
    "                        return []\n",
    "                    rating_str = str(rating_str).strip('[]')\n",
    "                    return [int(x.strip()) for x in rating_str.split(',') if x.strip()]\n",
    "                except:\n",
    "                    return []\n",
    "            \n",
    "            if 'original_rating' in df.columns:\n",
    "                df['original_rating_parsed'] = df['original_rating'].apply(parse_rating_array)\n",
    "                df['original_avg_rating'] = df['original_rating_parsed'].apply(lambda x: np.mean(x) if x else np.nan)\n",
    "            \n",
    "            if 'verified_rating' in df.columns:\n",
    "                df['verified_rating_parsed'] = df['verified_rating'].apply(parse_rating_array)\n",
    "                df['verified_avg_rating'] = df['verified_rating_parsed'].apply(lambda x: np.mean(x) if x else np.nan)\n",
    "            \n",
    "            # Calculate derived metrics\n",
    "            if 'original_avg_rating' in df.columns and 'verified_avg_rating' in df.columns:\n",
    "                df['rating_improvement'] = df['verified_avg_rating'] - df['original_avg_rating']\n",
    "            \n",
    "            if 'verified_total_steps' in df.columns and 'original_total_steps' in df.columns:\n",
    "                df['step_change'] = df['verified_total_steps'] - df['original_total_steps']\n",
    "            \n",
    "            if 'total_output_tokens' in df.columns and 'total_input_tokens' in df.columns:\n",
    "                df['token_efficiency'] = df['total_output_tokens'] / (df['total_input_tokens'] + 1)\n",
    "            \n",
    "            dataframes[version_name] = df\n",
    "            print(f\"✅ {version_name}: {len(df)} records loaded\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {version_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes.values(), ignore_index=True)\n",
    "        print(f\"\\n🎯 Total combined dataset: {len(combined_df)} records across {len(dataframes)} versions\")\n",
    "        return combined_df, dataframes\n",
    "    else:\n",
    "        print(\"❌ No data loaded successfully\")\n",
    "        return None, {}\n",
    "\n",
    "# Load all versions\n",
    "combined_df, version_dfs = load_multiple_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-VERSION COMPARATIVE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparative_foundational_analysis(combined_df):\n",
    "    \"\"\"\n",
    "    Compare foundational metrics across all versions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📈 COMPARATIVE FOUNDATIONAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Group by version for analysis\n",
    "    version_stats = combined_df.groupby('version').agg({\n",
    "        'success': ['count', 'mean'],\n",
    "        'original_answer_correctness': 'mean',\n",
    "        'verifier_answer_correctness': 'mean', \n",
    "        'processing_time_ms': ['mean', 'median', 'std'],\n",
    "        'total_llm_calls': ['mean', 'median'],\n",
    "        'total_input_tokens': 'mean',\n",
    "        'total_output_tokens': 'mean',\n",
    "        'token_efficiency': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    # Flatten column names\n",
    "    version_stats.columns = ['_'.join(col).strip() for col in version_stats.columns]\n",
    "    \n",
    "    # Calculate accuracy improvements\n",
    "    accuracy_improvements = {}\n",
    "    for version in combined_df['version'].unique():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        original_acc = version_data['original_answer_correctness'].mean() * 100\n",
    "        verified_acc = version_data['verifier_answer_correctness'].mean() * 100\n",
    "        accuracy_improvements[version] = verified_acc - original_acc\n",
    "    \n",
    "    results['version_stats'] = version_stats\n",
    "    results['accuracy_improvements'] = accuracy_improvements\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"📊 VERSION COMPARISON SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for version in combined_df['version'].unique():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        success_rate = version_data['success'].mean() * 100\n",
    "        accuracy_improvement = accuracy_improvements[version]\n",
    "        avg_time = version_data['processing_time_ms'].mean()\n",
    "        avg_llm_calls = version_data['total_llm_calls'].mean()\n",
    "        \n",
    "        print(f\"\\n{version}:\")\n",
    "        print(f\"  • Success Rate: {success_rate:.1f}%\")\n",
    "        print(f\"  • Accuracy Improvement: {accuracy_improvement:+.1f} pp\")\n",
    "        print(f\"  • Avg Processing Time: {avg_time:.1f} ms\")\n",
    "        print(f\"  • Avg LLM Calls: {avg_llm_calls:.1f}\")\n",
    "    \n",
    "    # Statistical significance tests\n",
    "    print(f\"\\n🔬 STATISTICAL SIGNIFICANCE TESTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    versions = list(combined_df['version'].unique())\n",
    "    \n",
    "    # Compare success rates using Fisher's exact test (more robust than chi-square)\n",
    "    if len(versions) >= 2:\n",
    "        for i in range(len(versions)):\n",
    "            for j in range(i+1, len(versions)):\n",
    "                v1_success = combined_df[combined_df['version'] == versions[i]]['success']\n",
    "                v2_success = combined_df[combined_df['version'] == versions[j]]['success']\n",
    "                \n",
    "                if len(v1_success) > 0 and len(v2_success) > 0:\n",
    "                    try:\n",
    "                        # Create contingency table\n",
    "                        v1_successes = v1_success.sum()\n",
    "                        v1_failures = len(v1_success) - v1_successes\n",
    "                        v2_successes = v2_success.sum()\n",
    "                        v2_failures = len(v2_success) - v2_successes\n",
    "                        \n",
    "                        contingency_table = [[v1_successes, v1_failures], \n",
    "                                           [v2_successes, v2_failures]]\n",
    "                        \n",
    "                        # Use Fisher's exact test for small samples or when chi-square assumptions violated\n",
    "                        if any(cell < 5 for row in contingency_table for cell in row):\n",
    "                            odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "                            test_used = \"Fisher's exact\"\n",
    "                        else:\n",
    "                            # Check if any expected frequencies are zero\n",
    "                            total = sum(sum(row) for row in contingency_table)\n",
    "                            row_totals = [sum(row) for row in contingency_table]\n",
    "                            col_totals = [sum(contingency_table[i][j] for i in range(2)) for j in range(2)]\n",
    "                            \n",
    "                            expected_freq_valid = all(\n",
    "                                (row_totals[i] * col_totals[j]) / total >= 1\n",
    "                                for i in range(2) for j in range(2)\n",
    "                            )\n",
    "                            \n",
    "                            if expected_freq_valid:\n",
    "                                stat, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "                                test_used = \"Chi-square\"\n",
    "                            else:\n",
    "                                odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "                                test_used = \"Fisher's exact\"\n",
    "                        \n",
    "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                        print(f\"  Success Rate {versions[i]} vs {versions[j]}: p={p_value:.4f} {significance} ({test_used})\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        # Fallback to simple proportion comparison\n",
    "                        v1_rate = v1_success.mean()\n",
    "                        v2_rate = v2_success.mean()\n",
    "                        rate_diff = abs(v1_rate - v2_rate)\n",
    "                        print(f\"  Success Rate {versions[i]} vs {versions[j]}: Rate diff={rate_diff:.3f} (comparison failed: {str(e)[:50]}...)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparative foundational analysis\n",
    "comp_results_1 = comparative_foundational_analysis(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization for Multi-Version Comparison\n",
    "def visualize_version_comparison(combined_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing all versions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('🔍 Multi-Version Comparison Dashboard', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(versions)))\n",
    "    \n",
    "    # 1. Success Rate Comparison\n",
    "    success_rates = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        success_rates.append(version_data['success'].mean() * 100)\n",
    "    \n",
    "    bars1 = axes[0,0].bar(versions, success_rates, color=colors)\n",
    "    axes[0,0].set_ylabel('Success Rate (%)')\n",
    "    axes[0,0].set_title('Success Rate by Version')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars1, success_rates):\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                      f'{value:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Accuracy Improvement Comparison  \n",
    "    accuracy_improvements = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        original_acc = version_data['original_answer_correctness'].mean() * 100\n",
    "        verified_acc = version_data['verifier_answer_correctness'].mean() * 100\n",
    "        accuracy_improvements.append(verified_acc - original_acc)\n",
    "    \n",
    "    bars2 = axes[0,1].bar(versions, accuracy_improvements, color=colors)\n",
    "    axes[0,1].set_ylabel('Accuracy Improvement (pp)')\n",
    "    axes[0,1].set_title('Accuracy Improvement by Version')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar, value in zip(bars2, accuracy_improvements):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if value >= 0 else -0.3), \n",
    "                      f'{value:+.1f}', ha='center', va='bottom' if value >= 0 else 'top')\n",
    "    \n",
    "    # 3. Processing Time Distribution\n",
    "    processing_times_by_version = []\n",
    "    labels = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        times = version_data['processing_time_ms'].dropna()\n",
    "        if len(times) > 0:\n",
    "            processing_times_by_version.append(times)\n",
    "            labels.append(version)\n",
    "    \n",
    "    if processing_times_by_version:\n",
    "        bp1 = axes[0,2].boxplot(processing_times_by_version, labels=labels, patch_artist=True)\n",
    "        for patch, color in zip(bp1['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "    axes[0,2].set_ylabel('Processing Time (ms)')\n",
    "    axes[0,2].set_title('Processing Time Distribution')\n",
    "    axes[0,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. LLM Calls Comparison\n",
    "    llm_calls_avg = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        llm_calls_avg.append(version_data['total_llm_calls'].mean())\n",
    "    \n",
    "    bars3 = axes[1,0].bar(versions, llm_calls_avg, color=colors)\n",
    "    axes[1,0].set_ylabel('Average LLM Calls')\n",
    "    axes[1,0].set_title('LLM Calls by Version')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars3, llm_calls_avg):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                      f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 5. Token Efficiency Comparison\n",
    "    token_efficiency_avg = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        token_efficiency_avg.append(version_data['token_efficiency'].mean())\n",
    "    \n",
    "    bars4 = axes[1,1].bar(versions, token_efficiency_avg, color=colors)\n",
    "    axes[1,1].set_ylabel('Average Token Efficiency')\n",
    "    axes[1,1].set_title('Token Efficiency by Version')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars4, token_efficiency_avg):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                      f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 6. Rating Improvement Comparison\n",
    "    rating_improvements = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        rating_imp = version_data['rating_improvement'].dropna().mean()\n",
    "        rating_improvements.append(rating_imp if not pd.isna(rating_imp) else 0)\n",
    "    \n",
    "    bars5 = axes[1,2].bar(versions, rating_improvements, color=colors)\n",
    "    axes[1,2].set_ylabel('Average Rating Improvement')\n",
    "    axes[1,2].set_title('Rating Improvement by Version')\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    axes[1,2].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar, value in zip(bars5, rating_improvements):\n",
    "        axes[1,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.01 if value >= 0 else -0.02), \n",
    "                      f'{value:+.3f}', ha='center', va='bottom' if value >= 0 else 'top')\n",
    "    \n",
    "    # 7. Step Change Comparison\n",
    "    step_changes = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        step_change = version_data['step_change'].dropna().mean()\n",
    "        step_changes.append(step_change if not pd.isna(step_change) else 0)\n",
    "    \n",
    "    bars6 = axes[2,0].bar(versions, step_changes, color=colors)\n",
    "    axes[2,0].set_ylabel('Average Step Change')\n",
    "    axes[2,0].set_title('Step Change by Version')\n",
    "    axes[2,0].tick_params(axis='x', rotation=45)\n",
    "    axes[2,0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for bar, value in zip(bars6, step_changes):\n",
    "        axes[2,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.1 if value >= 0 else -0.2), \n",
    "                      f'{value:+.1f}', ha='center', va='bottom' if value >= 0 else 'top')\n",
    "    \n",
    "    # 8. Performance Efficiency Scatter (Time vs Token Efficiency)\n",
    "    for i, version in enumerate(versions):\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['processing_time_ms', 'token_efficiency'])\n",
    "        if len(valid_data) > 0:\n",
    "            axes[2,1].scatter(valid_data['processing_time_ms'], valid_data['token_efficiency'], \n",
    "                            label=version, alpha=0.6, color=colors[i])\n",
    "    \n",
    "    axes[2,1].set_xlabel('Processing Time (ms)')\n",
    "    axes[2,1].set_ylabel('Token Efficiency')\n",
    "    axes[2,1].set_title('Performance Efficiency by Version')\n",
    "    axes[2,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 9. Overall Performance Radar Chart (simplified)\n",
    "    metrics = ['Success Rate', 'Accuracy Imp', 'Speed', 'Efficiency', 'Rating Imp']\n",
    "    \n",
    "    # Normalize metrics to 0-100 scale for comparison\n",
    "    normalized_data = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        \n",
    "        # Normalize each metric\n",
    "        success_norm = version_data['success'].mean() * 100\n",
    "        acc_imp_norm = max(0, (version_data['verifier_answer_correctness'].mean() - \n",
    "                              version_data['original_answer_correctness'].mean()) * 100 + 50)\n",
    "        speed_norm = max(0, 100 - (version_data['processing_time_ms'].mean() / 5))  # Inverse for speed\n",
    "        efficiency_norm = min(100, version_data['token_efficiency'].mean() * 100)\n",
    "        rating_norm = max(0, version_data['rating_improvement'].dropna().mean() * 50 + 50)\n",
    "        \n",
    "        normalized_data.append([success_norm, acc_imp_norm, speed_norm, efficiency_norm, rating_norm])\n",
    "    \n",
    "    # Create a simple bar chart instead of radar for better visibility\n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, (version, data) in enumerate(zip(versions, normalized_data)):\n",
    "        axes[2,2].bar(x_pos + i * width, data, width, label=version, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    axes[2,2].set_xlabel('Metrics')\n",
    "    axes[2,2].set_ylabel('Normalized Score (0-100)')\n",
    "    axes[2,2].set_title('Overall Performance Comparison')\n",
    "    axes[2,2].set_xticks(x_pos + width * 2)\n",
    "    axes[2,2].set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    axes[2,2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_version_comparison(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETAILED VERSION RANKING ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_version_ranking(combined_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive ranking system for all versions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🏆 COMPREHENSIVE VERSION RANKING ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    ranking_data = {}\n",
    "    \n",
    "    # Define metrics and their weights\n",
    "    metrics = {\n",
    "        'success_rate': {'weight': 0.25, 'higher_better': True},\n",
    "        'accuracy_improvement': {'weight': 0.25, 'higher_better': True},\n",
    "        'processing_speed': {'weight': 0.20, 'higher_better': True},  # Inverse of time\n",
    "        'token_efficiency': {'weight': 0.15, 'higher_better': True},\n",
    "        'rating_improvement': {'weight': 0.15, 'higher_better': True}\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics for each version\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        \n",
    "        # Success rate (0-100)\n",
    "        success_rate = version_data['success'].mean() * 100\n",
    "        \n",
    "        # Accuracy improvement (-100 to +100)\n",
    "        accuracy_improvement = (version_data['verifier_answer_correctness'].mean() - \n",
    "                               version_data['original_answer_correctness'].mean()) * 100\n",
    "        \n",
    "        # Processing speed (inverse of time, normalized)\n",
    "        avg_time = version_data['processing_time_ms'].mean()\n",
    "        processing_speed = 1000 / avg_time if avg_time > 0 else 0  # Operations per second\n",
    "        \n",
    "        # Token efficiency\n",
    "        token_efficiency = version_data['token_efficiency'].mean()\n",
    "        \n",
    "        # Rating improvement\n",
    "        rating_improvement = version_data['rating_improvement'].dropna().mean()\n",
    "        if pd.isna(rating_improvement):\n",
    "            rating_improvement = 0\n",
    "        \n",
    "        ranking_data[version] = {\n",
    "            'success_rate': success_rate,\n",
    "            'accuracy_improvement': accuracy_improvement,\n",
    "            'processing_speed': processing_speed,\n",
    "            'token_efficiency': token_efficiency,\n",
    "            'rating_improvement': rating_improvement,\n",
    "            'avg_processing_time': avg_time,\n",
    "            'total_problems': len(version_data)\n",
    "        }\n",
    "    \n",
    "    # Normalize metrics to 0-100 scale\n",
    "    normalized_data = {}\n",
    "    \n",
    "    for metric in metrics.keys():\n",
    "        values = [ranking_data[version][metric] for version in versions]\n",
    "        \n",
    "        if metrics[metric]['higher_better']:\n",
    "            min_val, max_val = min(values), max(values)\n",
    "            if max_val > min_val:\n",
    "                for version in versions:\n",
    "                    if version not in normalized_data:\n",
    "                        normalized_data[version] = {}\n",
    "                    normalized_data[version][metric] = ((ranking_data[version][metric] - min_val) / \n",
    "                                                       (max_val - min_val)) * 100\n",
    "            else:\n",
    "                for version in versions:\n",
    "                    if version not in normalized_data:\n",
    "                        normalized_data[version] = {}\n",
    "                    normalized_data[version][metric] = 100\n",
    "    \n",
    "    # Calculate weighted scores\n",
    "    final_scores = {}\n",
    "    for version in versions:\n",
    "        total_score = 0\n",
    "        for metric, config in metrics.items():\n",
    "            total_score += normalized_data[version][metric] * config['weight']\n",
    "        final_scores[version] = total_score\n",
    "    \n",
    "    # Create ranking\n",
    "    sorted_versions = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Print detailed ranking\n",
    "    print(\"🥇 FINAL RANKING:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for rank, (version, score) in enumerate(sorted_versions, 1):\n",
    "        medal = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else f\"{rank}.\"\n",
    "        print(f\"\\n{medal} {version} - Overall Score: {score:.1f}/100\")\n",
    "        print(f\"   Success Rate: {ranking_data[version]['success_rate']:.1f}%\")\n",
    "        print(f\"   Accuracy Improvement: {ranking_data[version]['accuracy_improvement']:+.1f} pp\")\n",
    "        print(f\"   Avg Processing Time: {ranking_data[version]['avg_processing_time']:.1f} ms\")\n",
    "        print(f\"   Token Efficiency: {ranking_data[version]['token_efficiency']:.3f}\")\n",
    "        print(f\"   Rating Improvement: {ranking_data[version]['rating_improvement']:+.3f}\")\n",
    "        print(f\"   Total Problems: {ranking_data[version]['total_problems']}\")\n",
    "    \n",
    "    # Detailed metric breakdown\n",
    "    print(f\"\\n📊 DETAILED METRIC BREAKDOWN:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    metric_rankings = {}\n",
    "    for metric in metrics.keys():\n",
    "        metric_scores = [(version, normalized_data[version][metric]) for version in versions]\n",
    "        metric_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        metric_rankings[metric] = metric_scores\n",
    "        \n",
    "        print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "        for rank, (version, score) in enumerate(metric_scores, 1):\n",
    "            print(f\"   {rank}. {version}: {score:.1f}/100\")\n",
    "    \n",
    "    return {\n",
    "        'final_ranking': sorted_versions,\n",
    "        'metric_rankings': metric_rankings,\n",
    "        'raw_data': ranking_data,\n",
    "        'normalized_data': normalized_data\n",
    "    }\n",
    "\n",
    "# Run comprehensive ranking\n",
    "ranking_results = comprehensive_version_ranking(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION HEATMAP ANALYSIS (14x14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_heatmap_analysis(combined_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive 14x14 correlation heatmap analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔥 CORRELATION HEATMAP ANALYSIS (14x14)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Define the 14 columns for correlation analysis\n",
    "    correlation_columns = [\n",
    "        'original_answer_correctness',\n",
    "        'original_total_steps', \n",
    "        'original_neg1',\n",
    "        'original_zero',\n",
    "        'original_pos1',\n",
    "        'verifier_answer_correctness',\n",
    "        'verified_total_steps',\n",
    "        'verifier_neg1', \n",
    "        'verifier_zero',\n",
    "        'verifier_pos1',\n",
    "        'processing_time_ms',\n",
    "        'total_llm_calls',\n",
    "        'total_input_tokens',\n",
    "        'total_output_tokens'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns exist in the dataset\n",
    "    available_columns = []\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in correlation_columns:\n",
    "        if col in combined_df.columns:\n",
    "            available_columns.append(col)\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Available columns: {len(available_columns)}/{len(correlation_columns)}\")\n",
    "    if missing_columns:\n",
    "        print(f\"⚠️  Missing columns: {missing_columns}\")\n",
    "    \n",
    "    # Prepare data for correlation analysis\n",
    "    correlation_data = combined_df[available_columns].copy()\n",
    "    \n",
    "    # Convert boolean columns to numeric\n",
    "    bool_columns = ['original_answer_correctness', 'verifier_answer_correctness']\n",
    "    for col in bool_columns:\n",
    "        if col in correlation_data.columns:\n",
    "            correlation_data[col] = correlation_data[col].astype(int)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    # Create the heatmap visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('🔥 Comprehensive Correlation Analysis (14x14)', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Main correlation heatmap (top left)\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                mask=mask,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=axes[0,0])\n",
    "    axes[0,0].set_title('Correlation Heatmap (Lower Triangle)', fontweight='bold')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    axes[0,0].tick_params(axis='y', rotation=0, labelsize=9)\n",
    "    \n",
    "    # 2. Full correlation heatmap without mask (top right)\n",
    "    sns.heatmap(correlation_matrix,\n",
    "                annot=False,  # No annotations for better visibility\n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=axes[0,1])\n",
    "    axes[0,1].set_title('Full Correlation Heatmap', fontweight='bold')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    axes[0,1].tick_params(axis='y', rotation=0, labelsize=9)\n",
    "    \n",
    "    # 3. Correlation strength distribution (bottom left)\n",
    "    correlation_values = correlation_matrix.values\n",
    "    correlation_flat = correlation_values[np.triu_indices_from(correlation_values, k=1)]  # Upper triangle excluding diagonal\n",
    "    \n",
    "    axes[1,0].hist(correlation_flat, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero correlation')\n",
    "    axes[1,0].axvline(np.mean(correlation_flat), color='green', linestyle='--', alpha=0.7, \n",
    "                     label=f'Mean: {np.mean(correlation_flat):.3f}')\n",
    "    axes[1,0].set_xlabel('Correlation Coefficient')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].set_title('Distribution of Correlation Values')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Top correlations analysis (bottom right)\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    # Find strongest positive and negative correlations\n",
    "    correlation_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if not np.isnan(corr_value):\n",
    "                correlation_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_value\n",
    "                ))\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Display top correlations\n",
    "    top_correlations_text = \"🔝 STRONGEST CORRELATIONS:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "    \n",
    "    top_correlations_text += \"📈 STRONGEST POSITIVE:\\n\"\n",
    "    positive_corrs = [pair for pair in correlation_pairs if pair[2] > 0][:5]\n",
    "    for i, (col1, col2, corr) in enumerate(positive_corrs, 1):\n",
    "        col1_short = col1.replace('_', ' ').title()[:20]\n",
    "        col2_short = col2.replace('_', ' ').title()[:20]\n",
    "        top_correlations_text += f\"{i}. {col1_short} ↔ {col2_short}\\n   r = {corr:.3f}\\n\\n\"\n",
    "    \n",
    "    top_correlations_text += \"📉 STRONGEST NEGATIVE:\\n\"\n",
    "    negative_corrs = [pair for pair in correlation_pairs if pair[2] < 0][:5]\n",
    "    for i, (col1, col2, corr) in enumerate(negative_corrs, 1):\n",
    "        col1_short = col1.replace('_', ' ').title()[:20]\n",
    "        col2_short = col2.replace('_', ' ').title()[:20]\n",
    "        top_correlations_text += f\"{i}. {col1_short} ↔ {col2_short}\\n   r = {corr:.3f}\\n\\n\"\n",
    "    \n",
    "    # Calculate correlation strength categories\n",
    "    strong_pos = len([pair for pair in correlation_pairs if pair[2] >= 0.7])\n",
    "    moderate_pos = len([pair for pair in correlation_pairs if 0.3 <= pair[2] < 0.7])\n",
    "    weak_pos = len([pair for pair in correlation_pairs if 0.1 <= pair[2] < 0.3])\n",
    "    negligible = len([pair for pair in correlation_pairs if -0.1 < pair[2] < 0.1])\n",
    "    weak_neg = len([pair for pair in correlation_pairs if -0.3 < pair[2] <= -0.1])\n",
    "    moderate_neg = len([pair for pair in correlation_pairs if -0.7 < pair[2] <= -0.3])\n",
    "    strong_neg = len([pair for pair in correlation_pairs if pair[2] <= -0.7])\n",
    "    \n",
    "    top_correlations_text += f\"\\n📊 CORRELATION STRENGTH SUMMARY:\\n\"\n",
    "    top_correlations_text += f\"Strong Positive (≥0.7): {strong_pos}\\n\"\n",
    "    top_correlations_text += f\"Moderate Positive (0.3-0.7): {moderate_pos}\\n\"\n",
    "    top_correlations_text += f\"Weak Positive (0.1-0.3): {weak_pos}\\n\"\n",
    "    top_correlations_text += f\"Negligible (-0.1 to 0.1): {negligible}\\n\"\n",
    "    top_correlations_text += f\"Weak Negative (-0.3 to -0.1): {weak_neg}\\n\"\n",
    "    top_correlations_text += f\"Moderate Negative (-0.7 to -0.3): {moderate_neg}\\n\"\n",
    "    top_correlations_text += f\"Strong Negative (≤-0.7): {strong_neg}\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.05, 0.95, top_correlations_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed correlation analysis\n",
    "    print(f\"\\n📋 DETAILED CORRELATION ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"📊 Dataset Statistics:\")\n",
    "    print(f\"   • Total correlations calculated: {len(correlation_pairs)}\")\n",
    "    print(f\"   • Mean correlation: {np.mean([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Std correlation: {np.std([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Max correlation: {max([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Min correlation: {min([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    \n",
    "    # Version-specific correlation analysis\n",
    "    print(f\"\\n🔍 VERSION-SPECIFIC CORRELATION INSIGHTS:\")\n",
    "    for version in combined_df['version'].unique():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        version_corr_data = version_data[available_columns].copy()\n",
    "        \n",
    "        # Convert boolean columns to numeric for this version\n",
    "        for col in bool_columns:\n",
    "            if col in version_corr_data.columns:\n",
    "                version_corr_data[col] = version_corr_data[col].astype(int)\n",
    "        \n",
    "        if len(version_corr_data) > 2:  # Need sufficient data for correlation\n",
    "            version_corr_matrix = version_corr_data.corr()\n",
    "            \n",
    "            # Find strongest correlation for this version\n",
    "            version_pairs = []\n",
    "            for i in range(len(version_corr_matrix.columns)):\n",
    "                for j in range(i+1, len(version_corr_matrix.columns)):\n",
    "                    corr_value = version_corr_matrix.iloc[i, j]\n",
    "                    if not np.isnan(corr_value):\n",
    "                        version_pairs.append((\n",
    "                            version_corr_matrix.columns[i],\n",
    "                            version_corr_matrix.columns[j], \n",
    "                            corr_value\n",
    "                        ))\n",
    "            \n",
    "            if version_pairs:\n",
    "                strongest_corr = max(version_pairs, key=lambda x: abs(x[2]))\n",
    "                mean_corr = np.mean([abs(pair[2]) for pair in version_pairs])\n",
    "                print(f\"   • {version}: Strongest |r|={abs(strongest_corr[2]):.3f}, Mean |r|={mean_corr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'correlation_matrix': correlation_matrix,\n",
    "        'correlation_pairs': correlation_pairs,\n",
    "        'available_columns': available_columns,\n",
    "        'missing_columns': missing_columns,\n",
    "        'correlation_stats': {\n",
    "            'mean': np.mean([pair[2] for pair in correlation_pairs]),\n",
    "            'std': np.std([pair[2] for pair in correlation_pairs]),\n",
    "            'max': max([pair[2] for pair in correlation_pairs]) if correlation_pairs else 0,\n",
    "            'min': min([pair[2] for pair in correlation_pairs]) if correlation_pairs else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run correlation heatmap analysis\n",
    "correlation_results = create_correlation_heatmap_analysis(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_and_recommendations(combined_df, ranking_results):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis and generate actionable recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 STATISTICAL ANALYSIS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    print(\"🔬 STATISTICAL SIGNIFICANCE TESTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ANOVA tests for continuous variables\n",
    "    continuous_vars = ['processing_time_ms', 'total_llm_calls', 'token_efficiency', 'rating_improvement']\n",
    "    \n",
    "    for var in continuous_vars:\n",
    "        if var in combined_df.columns:\n",
    "            groups = []\n",
    "            group_names = []\n",
    "            for version in versions:\n",
    "                version_data = combined_df[combined_df['version'] == version][var].dropna()\n",
    "                if len(version_data) > 1:  # Need at least 2 data points\n",
    "                    groups.append(version_data)\n",
    "                    group_names.append(version)\n",
    "            \n",
    "            if len(groups) >= 2:\n",
    "                try:\n",
    "                    # Check if there's any variance in the data\n",
    "                    all_data = np.concatenate(groups)\n",
    "                    if np.var(all_data) > 1e-10:  # Avoid division by zero\n",
    "                        f_stat, p_value = stats.f_oneway(*groups)\n",
    "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                        print(f\"  {var}: F={f_stat:.3f}, p={p_value:.4f} {significance}\")\n",
    "                    else:\n",
    "                        print(f\"  {var}: No variance detected (all values approximately equal)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  {var}: ANOVA failed ({str(e)[:30]}...)\")\n",
    "            else:\n",
    "                print(f\"  {var}: Insufficient data for ANOVA (need ≥2 groups with ≥2 data points each)\")\n",
    "    \n",
    "    # Effect size analysis (Cohen's d between best and worst versions)\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    worst_version = ranking_results['final_ranking'][-1][0]\n",
    "    \n",
    "    print(f\"\\n📏 EFFECT SIZE ANALYSIS ({best_version} vs {worst_version}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def cohens_d(group1, group2):\n",
    "        \"\"\"Calculate Cohen's d effect size with error handling\"\"\"\n",
    "        try:\n",
    "            if len(group1) < 2 or len(group2) < 2:\n",
    "                return np.nan\n",
    "            \n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            s1, s2 = group1.std(), group2.std()\n",
    "            \n",
    "            # Handle case where standard deviations are zero\n",
    "            if s1 == 0 and s2 == 0:\n",
    "                return 0.0 if group1.mean() == group2.mean() else np.inf\n",
    "            \n",
    "            pooled_std = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2) / (n1+n2-2))\n",
    "            \n",
    "            if pooled_std == 0:\n",
    "                return 0.0 if group1.mean() == group2.mean() else np.inf\n",
    "            \n",
    "            return (group1.mean() - group2.mean()) / pooled_std\n",
    "        except Exception as e:\n",
    "            return np.nan\n",
    "    \n",
    "    for var in continuous_vars:\n",
    "        if var in combined_df.columns:\n",
    "            best_data = combined_df[combined_df['version'] == best_version][var].dropna()\n",
    "            worst_data = combined_df[combined_df['version'] == worst_version][var].dropna()\n",
    "            \n",
    "            if len(best_data) > 0 and len(worst_data) > 0:\n",
    "                effect_size = cohens_d(best_data, worst_data)\n",
    "                magnitude = (\"Large\" if abs(effect_size) >= 0.8 else \n",
    "                           \"Medium\" if abs(effect_size) >= 0.5 else \"Small\")\n",
    "                print(f\"  {var}: Cohen's d = {effect_size:.3f} ({magnitude})\")\n",
    "    \n",
    "    # Confidence intervals for key metrics\n",
    "    print(f\"\\n🎯 95% CONFIDENCE INTERVALS FOR KEY METRICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        print(f\"\\n{version}:\")\n",
    "        \n",
    "        # Success rate CI\n",
    "        n = len(version_data)\n",
    "        p = version_data['success'].mean()\n",
    "        se = np.sqrt(p * (1-p) / n)\n",
    "        ci_lower = p - 1.96 * se\n",
    "        ci_upper = p + 1.96 * se\n",
    "        print(f\"  Success Rate: {p*100:.1f}% [{ci_lower*100:.1f}%, {ci_upper*100:.1f}%]\")\n",
    "        \n",
    "        # Processing time CI\n",
    "        if 'processing_time_ms' in version_data.columns:\n",
    "            times = version_data['processing_time_ms'].dropna()\n",
    "            if len(times) > 1:\n",
    "                mean_time = times.mean()\n",
    "                se_time = times.std() / np.sqrt(len(times))\n",
    "                ci_lower_time = mean_time - 1.96 * se_time\n",
    "                ci_upper_time = mean_time + 1.96 * se_time\n",
    "                print(f\"  Processing Time: {mean_time:.1f}ms [{ci_lower_time:.1f}ms, {ci_upper_time:.1f}ms]\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(f\"\\n🎯 ACTIONABLE RECOMMENDATIONS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Best performer analysis\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    best_score = ranking_results['final_ranking'][0][1]\n",
    "    \n",
    "    recommendations.append(f\"🥇 ADOPT BEST PRACTICES: {best_version} achieved the highest overall score ({best_score:.1f}/100)\")\n",
    "    \n",
    "    # Identify strongest metrics per version\n",
    "    metric_winners = {}\n",
    "    for metric in ranking_results['metric_rankings']:\n",
    "        winner = ranking_results['metric_rankings'][metric][0]\n",
    "        metric_winners[metric] = winner\n",
    "    \n",
    "    print(f\"\\n🏆 METRIC CHAMPIONS:\")\n",
    "    for metric, (version, score) in metric_winners.items():\n",
    "        print(f\"  • {metric.replace('_', ' ').title()}: {version} ({score:.1f}/100)\")\n",
    "        \n",
    "        if metric == 'processing_speed':\n",
    "            recommendations.append(f\"⚡ SPEED OPTIMIZATION: Learn from {version}'s processing optimizations\")\n",
    "        elif metric == 'accuracy_improvement':\n",
    "            recommendations.append(f\"🎯 ACCURACY FOCUS: Adopt {version}'s verification strategies\")\n",
    "        elif metric == 'token_efficiency':\n",
    "            recommendations.append(f\"💰 COST OPTIMIZATION: Implement {version}'s token efficiency techniques\")\n",
    "    \n",
    "    # Identify improvement opportunities\n",
    "    print(f\"\\n🔍 IMPROVEMENT OPPORTUNITIES:\")\n",
    "    \n",
    "    for version in versions:\n",
    "        version_scores = {metric: ranking_results['normalized_data'][version][metric] \n",
    "                         for metric in ranking_results['normalized_data'][version]}\n",
    "        weakest_metric = min(version_scores, key=version_scores.get)\n",
    "        weakest_score = version_scores[weakest_metric]\n",
    "        \n",
    "        if weakest_score < 50:  # Below average performance\n",
    "            print(f\"  • {version}: Improve {weakest_metric.replace('_', ' ')} (current: {weakest_score:.1f}/100)\")\n",
    "            recommendations.append(f\"📈 {version} FOCUS: Prioritize {weakest_metric.replace('_', ' ')} improvements\")\n",
    "    \n",
    "    # Version-specific insights\n",
    "    print(f\"\\n💡 VERSION-SPECIFIC INSIGHTS:\")\n",
    "    \n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        insights = []\n",
    "        \n",
    "        # Success rate insight\n",
    "        success_rate = version_data['success'].mean() * 100\n",
    "        if success_rate >= 95:\n",
    "            insights.append(\"Very high reliability\")\n",
    "        elif success_rate < 80:\n",
    "            insights.append(\"Reliability concerns\")\n",
    "        \n",
    "        # Processing time insight\n",
    "        avg_time = version_data['processing_time_ms'].mean()\n",
    "        if avg_time < 50:\n",
    "            insights.append(\"Fast processing\")\n",
    "        elif avg_time > 100:\n",
    "            insights.append(\"Slow processing\")\n",
    "        \n",
    "        # Token efficiency insight\n",
    "        token_eff = version_data['token_efficiency'].mean()\n",
    "        if token_eff > 0.3:\n",
    "            insights.append(\"High token efficiency\")\n",
    "        elif token_eff < 0.15:\n",
    "            insights.append(\"Low token efficiency\")\n",
    "        \n",
    "        if insights:\n",
    "            print(f\"  • {version}: {', '.join(insights)}\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\n📋 FINAL RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Strategic recommendations\n",
    "    strategic_recs = [\n",
    "        \"🔄 Implement A/B testing framework to continuously evaluate version performance\",\n",
    "        \"📊 Set up monitoring dashboards to track key metrics in real-time\", \n",
    "        \"🎯 Focus development efforts on the weakest performing metrics\",\n",
    "        \"🔬 Conduct detailed analysis of failure cases to improve reliability\",\n",
    "        \"💰 Optimize token usage based on best-performing version's strategies\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🚀 STRATEGIC RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(strategic_recs, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "\n",
    "    return {\n",
    "        'recommendations': recommendations,\n",
    "        'strategic_recommendations': strategic_recs,\n",
    "        'metric_winners': metric_winners\n",
    "    }\n",
    "\n",
    "# Run statistical analysis and recommendations\n",
    "analysis_recommendations = statistical_analysis_and_recommendations(combined_df, ranking_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION HEATMAP ANALYSIS (14x14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_heatmap_analysis(combined_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive 14x14 correlation heatmap analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔥 CORRELATION HEATMAP ANALYSIS (14x14)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Define the 14 columns for correlation analysis\n",
    "    correlation_columns = [\n",
    "        'original_answer_correctness',\n",
    "        'original_total_steps', \n",
    "        'original_neg1',\n",
    "        'original_zero',\n",
    "        'original_pos1',\n",
    "        'verifier_answer_correctness',\n",
    "        'verified_total_steps',\n",
    "        'verifier_neg1', \n",
    "        'verifier_zero',\n",
    "        'verifier_pos1',\n",
    "        'processing_time_ms',\n",
    "        'total_llm_calls',\n",
    "        'total_input_tokens',\n",
    "        'total_output_tokens'\n",
    "    ]\n",
    "    \n",
    "    # Check which columns exist in the dataset\n",
    "    available_columns = []\n",
    "    missing_columns = []\n",
    "    \n",
    "    for col in correlation_columns:\n",
    "        if col in combined_df.columns:\n",
    "            available_columns.append(col)\n",
    "        else:\n",
    "            missing_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Available columns: {len(available_columns)}/{len(correlation_columns)}\")\n",
    "    if missing_columns:\n",
    "        print(f\"⚠️  Missing columns: {missing_columns}\")\n",
    "    \n",
    "    # Prepare data for correlation analysis\n",
    "    correlation_data = combined_df[available_columns].copy()\n",
    "    \n",
    "    # Convert boolean columns to numeric\n",
    "    bool_columns = ['original_answer_correctness', 'verifier_answer_correctness']\n",
    "    for col in bool_columns:\n",
    "        if col in correlation_data.columns:\n",
    "            correlation_data[col] = correlation_data[col].astype(int)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = correlation_data.corr()\n",
    "    \n",
    "    # Create the heatmap visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('🔥 Comprehensive Correlation Analysis (14x14)', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Main correlation heatmap (top left)\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.3f',\n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                mask=mask,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=axes[0,0])\n",
    "    axes[0,0].set_title('Correlation Heatmap (Lower Triangle)', fontweight='bold')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    axes[0,0].tick_params(axis='y', rotation=0, labelsize=9)\n",
    "    \n",
    "    # 2. Full correlation heatmap without mask (top right)\n",
    "    sns.heatmap(correlation_matrix,\n",
    "                annot=False,  # No annotations for better visibility\n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Correlation Coefficient'},\n",
    "                ax=axes[0,1])\n",
    "    axes[0,1].set_title('Full Correlation Heatmap', fontweight='bold')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45, labelsize=9)\n",
    "    axes[0,1].tick_params(axis='y', rotation=0, labelsize=9)\n",
    "    \n",
    "    # 3. Correlation strength distribution (bottom left)\n",
    "    correlation_values = correlation_matrix.values\n",
    "    correlation_flat = correlation_values[np.triu_indices_from(correlation_values, k=1)]  # Upper triangle excluding diagonal\n",
    "    \n",
    "    axes[1,0].hist(correlation_flat, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1,0].axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero correlation')\n",
    "    axes[1,0].axvline(np.mean(correlation_flat), color='green', linestyle='--', alpha=0.7, \n",
    "                     label=f'Mean: {np.mean(correlation_flat):.3f}')\n",
    "    axes[1,0].set_xlabel('Correlation Coefficient')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].set_title('Distribution of Correlation Values')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Top correlations analysis (bottom right)\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    # Find strongest positive and negative correlations\n",
    "    correlation_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if not np.isnan(corr_value):\n",
    "                correlation_pairs.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_value\n",
    "                ))\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Display top correlations\n",
    "    top_correlations_text = \"🔝 STRONGEST CORRELATIONS:\\n\" + \"=\"*40 + \"\\n\\n\"\n",
    "    \n",
    "    top_correlations_text += \"📈 STRONGEST POSITIVE:\\n\"\n",
    "    positive_corrs = [pair for pair in correlation_pairs if pair[2] > 0][:5]\n",
    "    for i, (col1, col2, corr) in enumerate(positive_corrs, 1):\n",
    "        col1_short = col1.replace('_', ' ').title()[:20]\n",
    "        col2_short = col2.replace('_', ' ').title()[:20]\n",
    "        top_correlations_text += f\"{i}. {col1_short} ↔ {col2_short}\\n   r = {corr:.3f}\\n\\n\"\n",
    "    \n",
    "    top_correlations_text += \"📉 STRONGEST NEGATIVE:\\n\"\n",
    "    negative_corrs = [pair for pair in correlation_pairs if pair[2] < 0][:5]\n",
    "    for i, (col1, col2, corr) in enumerate(negative_corrs, 1):\n",
    "        col1_short = col1.replace('_', ' ').title()[:20]\n",
    "        col2_short = col2.replace('_', ' ').title()[:20]\n",
    "        top_correlations_text += f\"{i}. {col1_short} ↔ {col2_short}\\n   r = {corr:.3f}\\n\\n\"\n",
    "    \n",
    "    # Calculate correlation strength categories\n",
    "    strong_pos = len([pair for pair in correlation_pairs if pair[2] >= 0.7])\n",
    "    moderate_pos = len([pair for pair in correlation_pairs if 0.3 <= pair[2] < 0.7])\n",
    "    weak_pos = len([pair for pair in correlation_pairs if 0.1 <= pair[2] < 0.3])\n",
    "    negligible = len([pair for pair in correlation_pairs if -0.1 < pair[2] < 0.1])\n",
    "    weak_neg = len([pair for pair in correlation_pairs if -0.3 < pair[2] <= -0.1])\n",
    "    moderate_neg = len([pair for pair in correlation_pairs if -0.7 < pair[2] <= -0.3])\n",
    "    strong_neg = len([pair for pair in correlation_pairs if pair[2] <= -0.7])\n",
    "    \n",
    "    top_correlations_text += f\"\\n📊 CORRELATION STRENGTH SUMMARY:\\n\"\n",
    "    top_correlations_text += f\"Strong Positive (≥0.7): {strong_pos}\\n\"\n",
    "    top_correlations_text += f\"Moderate Positive (0.3-0.7): {moderate_pos}\\n\"\n",
    "    top_correlations_text += f\"Weak Positive (0.1-0.3): {weak_pos}\\n\"\n",
    "    top_correlations_text += f\"Negligible (-0.1 to 0.1): {negligible}\\n\"\n",
    "    top_correlations_text += f\"Weak Negative (-0.3 to -0.1): {weak_neg}\\n\"\n",
    "    top_correlations_text += f\"Moderate Negative (-0.7 to -0.3): {moderate_neg}\\n\"\n",
    "    top_correlations_text += f\"Strong Negative (≤-0.7): {strong_neg}\\n\"\n",
    "    \n",
    "    axes[1,1].text(0.05, 0.95, top_correlations_text, transform=axes[1,1].transAxes, \n",
    "                   fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed correlation analysis\n",
    "    print(f\"\\n📋 DETAILED CORRELATION ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"📊 Dataset Statistics:\")\n",
    "    print(f\"   • Total correlations calculated: {len(correlation_pairs)}\")\n",
    "    print(f\"   • Mean correlation: {np.mean([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Std correlation: {np.std([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Max correlation: {max([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    print(f\"   • Min correlation: {min([pair[2] for pair in correlation_pairs]):.3f}\")\n",
    "    \n",
    "    # Version-specific correlation analysis\n",
    "    print(f\"\\n🔍 VERSION-SPECIFIC CORRELATION INSIGHTS:\")\n",
    "    for version in combined_df['version'].unique():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        version_corr_data = version_data[available_columns].copy()\n",
    "        \n",
    "        # Convert boolean columns to numeric for this version\n",
    "        for col in bool_columns:\n",
    "            if col in version_corr_data.columns:\n",
    "                version_corr_data[col] = version_corr_data[col].astype(int)\n",
    "        \n",
    "        if len(version_corr_data) > 2:  # Need sufficient data for correlation\n",
    "            version_corr_matrix = version_corr_data.corr()\n",
    "            \n",
    "            # Find strongest correlation for this version\n",
    "            version_pairs = []\n",
    "            for i in range(len(version_corr_matrix.columns)):\n",
    "                for j in range(i+1, len(version_corr_matrix.columns)):\n",
    "                    corr_value = version_corr_matrix.iloc[i, j]\n",
    "                    if not np.isnan(corr_value):\n",
    "                        version_pairs.append((\n",
    "                            version_corr_matrix.columns[i],\n",
    "                            version_corr_matrix.columns[j], \n",
    "                            corr_value\n",
    "                        ))\n",
    "            \n",
    "            if version_pairs:\n",
    "                strongest_corr = max(version_pairs, key=lambda x: abs(x[2]))\n",
    "                mean_corr = np.mean([abs(pair[2]) for pair in version_pairs])\n",
    "                print(f\"   • {version}: Strongest |r|={abs(strongest_corr[2]):.3f}, Mean |r|={mean_corr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'correlation_matrix': correlation_matrix,\n",
    "        'correlation_pairs': correlation_pairs,\n",
    "        'available_columns': available_columns,\n",
    "        'missing_columns': missing_columns,\n",
    "        'correlation_stats': {\n",
    "            'mean': np.mean([pair[2] for pair in correlation_pairs]),\n",
    "            'std': np.std([pair[2] for pair in correlation_pairs]),\n",
    "            'max': max([pair[2] for pair in correlation_pairs]) if correlation_pairs else 0,\n",
    "            'min': min([pair[2] for pair in correlation_pairs]) if correlation_pairs else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run correlation heatmap analysis\n",
    "correlation_results = create_correlation_heatmap_analysis(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_steps_llm_relationship_report(steps_llm_results, combined_df, save_to_file=False, filename='steps_llm_relationship_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive text report of steps vs LLM calls relationship analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"ORIGINAL STEPS vs LLM CALLS RELATIONSHIP ANALYSIS REPORT\")\n",
    "    report.append(\"Resource Scaling and Predictability Analysis Across Verification Versions\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    version_correlations = steps_llm_results['version_correlations']\n",
    "    mean_correlation = steps_llm_results['mean_correlation']\n",
    "    correlation_range = steps_llm_results['correlation_range']\n",
    "    \n",
    "    report.append(f\"Versions analyzed: {len(version_correlations)}\")\n",
    "    report.append(f\"Overall relationship strength: {mean_correlation:.3f}\")\n",
    "    report.append(f\"Correlation range: {correlation_range[0]:.3f} to {correlation_range[1]:.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Interpret overall relationship strength\n",
    "    if mean_correlation >= 0.7:\n",
    "        relationship_strength = \"STRONG\"\n",
    "        predictability = \"Highly predictable\"\n",
    "        implication = \"LLM usage can be reliably estimated from problem complexity\"\n",
    "    elif mean_correlation >= 0.5:\n",
    "        relationship_strength = \"MODERATE\"\n",
    "        predictability = \"Moderately predictable\"\n",
    "        implication = \"LLM usage somewhat predictable from problem complexity\"\n",
    "    elif mean_correlation >= 0.3:\n",
    "        relationship_strength = \"WEAK\"\n",
    "        predictability = \"Weakly predictable\"\n",
    "        implication = \"Limited predictability of LLM usage from step count\"\n",
    "    else:\n",
    "        relationship_strength = \"MINIMAL\"\n",
    "        predictability = \"Unpredictable\"\n",
    "        implication = \"LLM usage not well predicted by step count alone\"\n",
    "    \n",
    "    report.append(f\"Overall Relationship: {relationship_strength}\")\n",
    "    report.append(f\"Predictability: {predictability}\")\n",
    "    report.append(f\"Practical Implication: {implication}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Individual version analysis\n",
    "    report.append(\"INDIVIDUAL VERSION ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    # Sort versions by correlation strength\n",
    "    sorted_versions = sorted(version_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for rank, (version, correlation) in enumerate(sorted_versions, 1):\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            # Calculate additional statistics\n",
    "            steps_mean = valid_data['original_total_steps'].mean()\n",
    "            steps_std = valid_data['original_total_steps'].std()\n",
    "            llm_mean = valid_data['total_llm_calls'].mean()\n",
    "            llm_std = valid_data['total_llm_calls'].std()\n",
    "            \n",
    "            # Calculate linear regression parameters\n",
    "            slope, intercept = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            \n",
    "            # Interpret correlation strength for this version\n",
    "            if abs(correlation) >= 0.7:\n",
    "                strength = \"Strong\"\n",
    "                interpretation = \"Highly predictable scaling\"\n",
    "            elif abs(correlation) >= 0.5:\n",
    "                strength = \"Moderate\"\n",
    "                interpretation = \"Moderately predictable scaling\"\n",
    "            elif abs(correlation) >= 0.3:\n",
    "                strength = \"Weak\"\n",
    "                interpretation = \"Limited predictability\"\n",
    "            else:\n",
    "                strength = \"Minimal\"\n",
    "                interpretation = \"Unpredictable scaling\"\n",
    "            \n",
    "            report.append(f\"{rank}. {version}\")\n",
    "            report.append(f\"   Correlation coefficient (r): {correlation:6.3f}\")\n",
    "            report.append(f\"   Relationship strength: {strength}\")\n",
    "            report.append(f\"   Interpretation: {interpretation}\")\n",
    "            report.append(f\"   Sample size: {len(valid_data)} problems\")\n",
    "            report.append(\"\")\n",
    "            report.append(f\"   Descriptive Statistics:\")\n",
    "            report.append(f\"     Original Steps: Mean = {steps_mean:5.1f}, SD = {steps_std:5.1f}\")\n",
    "            report.append(f\"     LLM Calls:      Mean = {llm_mean:5.1f}, SD = {llm_std:5.1f}\")\n",
    "            report.append(\"\")\n",
    "            report.append(f\"   Linear Relationship:\")\n",
    "            report.append(f\"     LLM Calls = {slope:.3f} × Original Steps + {intercept:.3f}\")\n",
    "            report.append(f\"     Slope interpretation: {slope:.3f} additional LLM calls per step\")\n",
    "            if intercept > 0:\n",
    "                report.append(f\"     Baseline: {intercept:.1f} LLM calls (fixed overhead)\")\n",
    "            else:\n",
    "                report.append(f\"     Baseline: {abs(intercept):.1f} LLM calls saved (efficiency)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            r_squared = correlation ** 2\n",
    "            report.append(f\"   Explained Variance (R²): {r_squared:.3f} ({r_squared*100:.1f}%)\")\n",
    "            report.append(f\"   → {r_squared*100:.1f}% of LLM call variation explained by step count\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Practical implications for this version\n",
    "            if abs(correlation) >= 0.5:\n",
    "                report.append(f\"   Resource Planning Capability: HIGH\")\n",
    "                report.append(f\"   → Can estimate LLM usage with {r_squared*100:.0f}% accuracy\")\n",
    "                report.append(f\"   → Suitable for cost prediction and capacity planning\")\n",
    "            elif abs(correlation) >= 0.3:\n",
    "                report.append(f\"   Resource Planning Capability: MODERATE\")\n",
    "                report.append(f\"   → Some predictability but other factors also important\")\n",
    "                report.append(f\"   → Use with caution for resource estimation\")\n",
    "            else:\n",
    "                report.append(f\"   Resource Planning Capability: LOW\")\n",
    "                report.append(f\"   → Poor predictability from step count alone\")\n",
    "                report.append(f\"   → Consider additional complexity metrics\")\n",
    "            report.append(\"\")\n",
    "            report.append(\"-\" * 40)\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Cross-version comparison\n",
    "    report.append(\"CROSS-VERSION COMPARISON\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    best_version = max(version_correlations.items(), key=lambda x: abs(x[1]))\n",
    "    worst_version = min(version_correlations.items(), key=lambda x: abs(x[1]))\n",
    "    \n",
    "    report.append(f\"Most Predictable Version: {best_version[0]}\")\n",
    "    report.append(f\"  Correlation: {best_version[1]:.3f}\")\n",
    "    report.append(f\"  Resource planning: Highly reliable\")\n",
    "    report.append(\"\")\n",
    "    report.append(f\"Least Predictable Version: {worst_version[0]}\")\n",
    "    report.append(f\"  Correlation: {worst_version[1]:.3f}\")\n",
    "    report.append(f\"  Resource planning: Unreliable\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Calculate coefficient of variation across versions\n",
    "    correlations_list = list(version_correlations.values())\n",
    "    cv = np.std(correlations_list) / np.mean(np.abs(correlations_list)) if np.mean(np.abs(correlations_list)) > 0 else 0\n",
    "    \n",
    "    report.append(f\"Version Consistency Analysis:\")\n",
    "    report.append(f\"  Correlation range: {correlation_range[0]:.3f} to {correlation_range[1]:.3f}\")\n",
    "    report.append(f\"  Coefficient of variation: {cv:.3f}\")\n",
    "    \n",
    "    if cv < 0.2:\n",
    "        report.append(f\"  → HIGH CONSISTENCY: All versions show similar scaling patterns\")\n",
    "    elif cv < 0.5:\n",
    "        report.append(f\"  → MODERATE CONSISTENCY: Some variation in scaling patterns\")\n",
    "    else:\n",
    "        report.append(f\"  → LOW CONSISTENCY: Significant differences in scaling patterns\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    report.append(\"EFFICIENCY SCALING ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    for version, correlation in sorted_versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            slope, intercept = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            \n",
    "            # Efficiency interpretation\n",
    "            if slope < 1.0:\n",
    "                efficiency_rating = \"HIGHLY EFFICIENT\"\n",
    "                efficiency_desc = f\"Uses <1 LLM call per step on average\"\n",
    "            elif slope < 1.5:\n",
    "                efficiency_rating = \"EFFICIENT\"\n",
    "                efficiency_desc = f\"Uses ~1-1.5 LLM calls per step\"\n",
    "            elif slope < 2.0:\n",
    "                efficiency_rating = \"MODERATE\"\n",
    "                efficiency_desc = f\"Uses ~1.5-2 LLM calls per step\"\n",
    "            else:\n",
    "                efficiency_rating = \"INEFFICIENT\"\n",
    "                efficiency_desc = f\"Uses >2 LLM calls per step\"\n",
    "            \n",
    "            report.append(f\"{version}:\")\n",
    "            report.append(f\"  Scaling rate: {slope:.3f} LLM calls per step\")\n",
    "            report.append(f\"  Efficiency: {efficiency_rating}\")\n",
    "            report.append(f\"  Description: {efficiency_desc}\")\n",
    "            \n",
    "            # Calculate cost implications (assuming cost per LLM call)\n",
    "            if intercept > 0:\n",
    "                report.append(f\"  Fixed overhead: {intercept:.1f} LLM calls per problem\")\n",
    "            else:\n",
    "                report.append(f\"  Efficiency bonus: {abs(intercept):.1f} LLM calls saved\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Predictive modeling insights\n",
    "    report.append(\"PREDICTIVE MODELING INSIGHTS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    high_predictability_versions = [v for v, r in version_correlations.items() if abs(r) >= 0.5]\n",
    "    moderate_predictability_versions = [v for v, r in version_correlations.items() if 0.3 <= abs(r) < 0.5]\n",
    "    low_predictability_versions = [v for v, r in version_correlations.items() if abs(r) < 0.3]\n",
    "    \n",
    "    report.append(f\"High Predictability Versions ({len(high_predictability_versions)}):\")\n",
    "    for version in high_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Moderate Predictability Versions ({len(moderate_predictability_versions)}):\")\n",
    "    for version in moderate_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Low Predictability Versions ({len(low_predictability_versions)}):\")\n",
    "    for version in low_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    report.append(\"PRACTICAL RECOMMENDATIONS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    report.append(\"Resource Planning:\")\n",
    "    if len(high_predictability_versions) > 0:\n",
    "        report.append(f\"  • Use {', '.join(high_predictability_versions)} for reliable cost estimation\")\n",
    "        report.append(f\"  • Step count provides good proxy for LLM usage\")\n",
    "        report.append(f\"  • Implement step-based capacity planning\")\n",
    "    else:\n",
    "        report.append(f\"  • Step count alone insufficient for resource planning\")\n",
    "        report.append(f\"  • Consider additional complexity metrics\")\n",
    "        report.append(f\"  • Implement dynamic resource allocation\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Version Selection:\")\n",
    "    if best_version[1] >= 0.5:\n",
    "        report.append(f\"  • Recommended: {best_version[0]} (most predictable scaling)\")\n",
    "        report.append(f\"  • Provides {(best_version[1]**2)*100:.0f}% resource predictability\")\n",
    "    report.append(f\"  • Avoid: {worst_version[0]} (unpredictable scaling)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Cost Optimization:\")\n",
    "    # Find most efficient version (lowest slope)\n",
    "    efficiency_ranking = []\n",
    "    for version, correlation in version_correlations.items():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        if len(valid_data) > 1:\n",
    "            slope, _ = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            efficiency_ranking.append((version, slope))\n",
    "    \n",
    "    if efficiency_ranking:\n",
    "        most_efficient = min(efficiency_ranking, key=lambda x: x[1])\n",
    "        report.append(f\"  • Most efficient: {most_efficient[0]} ({most_efficient[1]:.3f} LLM calls per step)\")\n",
    "        report.append(f\"  • Focus optimization efforts on inefficient versions\")\n",
    "        report.append(f\"  • Monitor scaling rates for cost control\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"Quality Assurance:\")\n",
    "    report.append(f\"  • Set up monitoring for step-to-LLM call ratios\")\n",
    "    report.append(f\"  • Alert on deviations from expected scaling patterns\")\n",
    "    report.append(f\"  • Regular recalibration of prediction models\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"END OF STEPS vs LLM CALLS RELATIONSHIP ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Convert to string\n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Print to console\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"\\n✅ Steps vs LLM calls report saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving steps vs LLM calls report: {e}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "# Run steps vs LLM calls analysis\n",
    "steps_llm_results = generate_steps_llm_relationship_report(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORRELATION HEATMAP TEXT REPORT GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_heatmap_report(correlation_results, combined_df, save_to_file=False, filename='correlation_analysis_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive text report of correlation heatmap analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"CORRELATION HEATMAP ANALYSIS REPORT\")\n",
    "    report.append(\"Multi-Variable Relationship Analysis Across Verification System Metrics\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    correlation_matrix = correlation_results['correlation_matrix']\n",
    "    correlation_pairs = correlation_results['correlation_pairs']\n",
    "    available_columns = correlation_results['available_columns']\n",
    "    missing_columns = correlation_results['missing_columns']\n",
    "    stats = correlation_results['correlation_stats']\n",
    "    \n",
    "    report.append(f\"Total variables analyzed: {len(available_columns)}\")\n",
    "    report.append(f\"Total correlation pairs: {len(correlation_pairs)}\")\n",
    "    report.append(f\"Missing variables: {len(missing_columns)}\")\n",
    "    if missing_columns:\n",
    "        report.append(f\"  Missing: {', '.join(missing_columns)}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Correlation Statistics:\")\n",
    "    report.append(f\"  Mean correlation: {stats['mean']:.3f}\")\n",
    "    report.append(f\"  Standard deviation: {stats['std']:.3f}\")\n",
    "    report.append(f\"  Range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Correlation strength distribution\n",
    "    strong_pos = len([pair for pair in correlation_pairs if pair[2] >= 0.7])\n",
    "    moderate_pos = len([pair for pair in correlation_pairs if 0.3 <= pair[2] < 0.7])\n",
    "    weak_pos = len([pair for pair in correlation_pairs if 0.1 <= pair[2] < 0.3])\n",
    "    negligible = len([pair for pair in correlation_pairs if -0.1 < pair[2] < 0.1])\n",
    "    weak_neg = len([pair for pair in correlation_pairs if -0.3 < pair[2] <= -0.1])\n",
    "    moderate_neg = len([pair for pair in correlation_pairs if -0.7 < pair[2] <= -0.3])\n",
    "    strong_neg = len([pair for pair in correlation_pairs if pair[2] <= -0.7])\n",
    "    \n",
    "    total_pairs = len(correlation_pairs)\n",
    "    \n",
    "    report.append(\"Correlation Strength Distribution:\")\n",
    "    report.append(f\"  Strong Positive (≥0.7):     {strong_pos:3d} ({(strong_pos/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Moderate Positive (0.3-0.7): {moderate_pos:3d} ({(moderate_pos/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Weak Positive (0.1-0.3):     {weak_pos:3d} ({(weak_pos/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Negligible (-0.1 to 0.1):    {negligible:3d} ({(negligible/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Weak Negative (-0.3 to -0.1): {weak_neg:3d} ({(weak_neg/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Moderate Negative (-0.7 to -0.3): {moderate_neg:3d} ({(moderate_neg/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(f\"  Strong Negative (≤-0.7):     {strong_neg:3d} ({(strong_neg/total_pairs)*100:5.1f}%)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Top correlations\n",
    "    report.append(\"STRONGEST CORRELATIONS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    sorted_pairs = sorted(correlation_pairs, key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    report.append(\"Top 10 Strongest Positive Correlations:\")\n",
    "    positive_corrs = [pair for pair in sorted_pairs if pair[2] > 0][:10]\n",
    "    for i, (col1, col2, corr) in enumerate(positive_corrs, 1):\n",
    "        report.append(f\"  {i:2d}. {col1:<25} ↔ {col2:<25} r = {corr:6.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Top 10 Strongest Negative Correlations:\")\n",
    "    negative_corrs = [pair for pair in sorted_pairs if pair[2] < 0][:10]\n",
    "    for i, (col1, col2, corr) in enumerate(negative_corrs, 1):\n",
    "        report.append(f\"  {i:2d}. {col1:<25} ↔ {col2:<25} r = {corr:6.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Complete correlation matrix\n",
    "    report.append(\"COMPLETE CORRELATION MATRIX\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    # Create formatted correlation matrix table\n",
    "    matrix_df = correlation_matrix.round(3)\n",
    "    \n",
    "    # Header\n",
    "    col_names = list(matrix_df.columns)\n",
    "    header = \"Variable\".ljust(25)\n",
    "    for i, col in enumerate(col_names):\n",
    "        header += f\"{i+1:>8}\"\n",
    "    report.append(header)\n",
    "    report.append(\"-\" * len(header))\n",
    "    \n",
    "    # Matrix rows\n",
    "    for i, (index, row) in enumerate(matrix_df.iterrows()):\n",
    "        row_str = f\"{i+1:2d}. {index[:20]:<20}\"\n",
    "        for j, val in enumerate(row):\n",
    "            if i == j:\n",
    "                row_str += f\"{'1.000':>8}\"\n",
    "            elif i > j:\n",
    "                row_str += f\"{val:8.3f}\"\n",
    "            else:\n",
    "                row_str += f\"{'':>8}\"  # Empty for upper triangle\n",
    "        report.append(row_str)\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"Variable Legend:\")\n",
    "    for i, col in enumerate(col_names):\n",
    "        report.append(f\"  {i+1:2d}. {col}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Key relationship analysis\n",
    "    report.append(\"KEY RELATIONSHIP ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    # Performance relationships\n",
    "    performance_vars = ['original_answer_correctness', 'verifier_answer_correctness']\n",
    "    efficiency_vars = ['processing_time_ms', 'total_llm_calls', 'total_input_tokens', 'total_output_tokens']\n",
    "    quality_vars = ['original_total_steps', 'verified_total_steps']\n",
    "    \n",
    "    report.append(\"Performance Correlations:\")\n",
    "    for perf_var in performance_vars:\n",
    "        if perf_var in correlation_matrix.columns:\n",
    "            report.append(f\"  {perf_var}:\")\n",
    "            perf_corrs = correlation_matrix[perf_var].drop(perf_var).sort_values(key=abs, ascending=False)\n",
    "            for var, corr in perf_corrs.head(5).items():\n",
    "                if abs(corr) > 0.1:  # Only show meaningful correlations\n",
    "                    report.append(f\"    → {var:<25}: {corr:6.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Efficiency Correlations:\")\n",
    "    for eff_var in efficiency_vars:\n",
    "        if eff_var in correlation_matrix.columns:\n",
    "            report.append(f\"  {eff_var}:\")\n",
    "            eff_corrs = correlation_matrix[eff_var].drop(eff_var).sort_values(key=abs, ascending=False)\n",
    "            for var, corr in eff_corrs.head(3).items():\n",
    "                if abs(corr) > 0.1:\n",
    "                    report.append(f\"    → {var:<25}: {corr:6.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Version-specific analysis\n",
    "    report.append(\"VERSION-SPECIFIC CORRELATION PATTERNS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        version_corr_data = version_data[available_columns].copy()\n",
    "        \n",
    "        # Convert boolean columns to numeric\n",
    "        bool_columns = ['original_answer_correctness', 'verifier_answer_correctness']\n",
    "        for col in bool_columns:\n",
    "            if col in version_corr_data.columns:\n",
    "                version_corr_data[col] = version_corr_data[col].astype(int)\n",
    "        \n",
    "        if len(version_corr_data) > 2:\n",
    "            version_corr_matrix = version_corr_data.corr()\n",
    "            \n",
    "            # Find strongest correlations for this version\n",
    "            version_pairs = []\n",
    "            for i in range(len(version_corr_matrix.columns)):\n",
    "                for j in range(i+1, len(version_corr_matrix.columns)):\n",
    "                    corr_value = version_corr_matrix.iloc[i, j]\n",
    "                    if not np.isnan(corr_value) and abs(corr_value) > 0.3:  # Only meaningful correlations\n",
    "                        version_pairs.append((\n",
    "                            version_corr_matrix.columns[i],\n",
    "                            version_corr_matrix.columns[j], \n",
    "                            corr_value\n",
    "                        ))\n",
    "            \n",
    "            version_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "            \n",
    "            report.append(f\"{version}:\")\n",
    "            report.append(f\"  Sample size: {len(version_data)}\")\n",
    "            if version_pairs:\n",
    "                report.append(f\"  Top correlations (|r| > 0.3):\")\n",
    "                for col1, col2, corr in version_pairs[:5]:\n",
    "                    report.append(f\"    {col1:<20} ↔ {col2:<20}: {corr:6.3f}\")\n",
    "            else:\n",
    "                report.append(f\"  No strong correlations found (|r| > 0.3)\")\n",
    "            \n",
    "            # Calculate average correlation strength\n",
    "            all_corrs = [abs(pair[2]) for pair in version_pairs]\n",
    "            if all_corrs:\n",
    "                avg_strength = np.mean(all_corrs)\n",
    "                report.append(f\"  Average correlation strength: {avg_strength:.3f}\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Statistical insights\n",
    "    report.append(\"STATISTICAL INSIGHTS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    if strong_pos + strong_neg > total_pairs * 0.1:\n",
    "        report.append(\"🔍 HIGH MULTICOLLINEARITY DETECTED\")\n",
    "        report.append(\"  Multiple strong correlations (|r| ≥ 0.7) found.\")\n",
    "        report.append(\"  Consider dimensionality reduction or variable selection.\")\n",
    "    elif moderate_pos + moderate_neg > total_pairs * 0.3:\n",
    "        report.append(\"📊 MODERATE CORRELATION STRUCTURE\")\n",
    "        report.append(\"  Moderate correlations suggest meaningful relationships.\")\n",
    "        report.append(\"  Variables show expected interdependencies.\")\n",
    "    else:\n",
    "        report.append(\"✅ LOW CORRELATION STRUCTURE\")\n",
    "        report.append(\"  Most variables are relatively independent.\")\n",
    "        report.append(\"  Good for regression modeling and analysis.\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Practical implications\n",
    "    report.append(\"PRACTICAL IMPLICATIONS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    report.append(\"Model Building Considerations:\")\n",
    "    if strong_pos + strong_neg > 5:\n",
    "        report.append(\"  • High multicollinearity detected - consider feature selection\")\n",
    "        report.append(\"  • Use regularization techniques (Ridge/Lasso) for regression\")\n",
    "        report.append(\"  • Consider principal component analysis for dimensionality reduction\")\n",
    "    else:\n",
    "        report.append(\"  • Low multicollinearity - most variables suitable for modeling\")\n",
    "        report.append(\"  • Standard regression techniques applicable\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"System Optimization Insights:\")\n",
    "    # Find key efficiency relationships\n",
    "    efficiency_relationships = []\n",
    "    for pair in correlation_pairs:\n",
    "        col1, col2, corr = pair\n",
    "        if any(eff in col1 for eff in ['time', 'token', 'llm']) and any(eff in col2 for eff in ['time', 'token', 'llm']):\n",
    "            if abs(corr) > 0.5:\n",
    "                efficiency_relationships.append(pair)\n",
    "    \n",
    "    if efficiency_relationships:\n",
    "        report.append(\"  Key efficiency relationships identified:\")\n",
    "        for col1, col2, corr in efficiency_relationships[:3]:\n",
    "            if corr > 0:\n",
    "                report.append(f\"    • {col1} and {col2} increase together (r={corr:.3f})\")\n",
    "            else:\n",
    "                report.append(f\"    • {col1} increases as {col2} decreases (r={corr:.3f})\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"Quality Monitoring Recommendations:\")\n",
    "    report.append(\"  • Monitor strongly correlated variables together\")\n",
    "    report.append(\"  • Set up alerts for unusual correlation pattern changes\")\n",
    "    report.append(\"  • Use correlation patterns for anomaly detection\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"END OF CORRELATION HEATMAP ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Convert to string\n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Print to console\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"\\n✅ Correlation report saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving correlation report: {e}\")\n",
    "    \n",
    "    return report_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEPS VS LLM CALLS RELATIONSHIP TEXT REPORT GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_steps_llm_relationship_report(steps_llm_results, combined_df, save_to_file=False, filename='steps_llm_relationship_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive text report of steps vs LLM calls relationship analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"ORIGINAL STEPS vs LLM CALLS RELATIONSHIP ANALYSIS REPORT\")\n",
    "    report.append(\"Resource Scaling and Predictability Analysis Across Verification Versions\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    version_correlations = steps_llm_results['version_correlations']\n",
    "    mean_correlation = steps_llm_results['mean_correlation']\n",
    "    correlation_range = steps_llm_results['correlation_range']\n",
    "    \n",
    "    report.append(f\"Versions analyzed: {len(version_correlations)}\")\n",
    "    report.append(f\"Overall relationship strength: {mean_correlation:.3f}\")\n",
    "    report.append(f\"Correlation range: {correlation_range[0]:.3f} to {correlation_range[1]:.3f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Interpret overall relationship strength\n",
    "    if mean_correlation >= 0.7:\n",
    "        relationship_strength = \"STRONG\"\n",
    "        predictability = \"Highly predictable\"\n",
    "        implication = \"LLM usage can be reliably estimated from problem complexity\"\n",
    "    elif mean_correlation >= 0.5:\n",
    "        relationship_strength = \"MODERATE\"\n",
    "        predictability = \"Moderately predictable\"\n",
    "        implication = \"LLM usage somewhat predictable from problem complexity\"\n",
    "    elif mean_correlation >= 0.3:\n",
    "        relationship_strength = \"WEAK\"\n",
    "        predictability = \"Weakly predictable\"\n",
    "        implication = \"Limited predictability of LLM usage from step count\"\n",
    "    else:\n",
    "        relationship_strength = \"MINIMAL\"\n",
    "        predictability = \"Unpredictable\"\n",
    "        implication = \"LLM usage not well predicted by step count alone\"\n",
    "    \n",
    "    report.append(f\"Overall Relationship: {relationship_strength}\")\n",
    "    report.append(f\"Predictability: {predictability}\")\n",
    "    report.append(f\"Practical Implication: {implication}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Individual version analysis\n",
    "    report.append(\"INDIVIDUAL VERSION ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    # Sort versions by correlation strength\n",
    "    sorted_versions = sorted(version_correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for rank, (version, correlation) in enumerate(sorted_versions, 1):\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            # Calculate additional statistics\n",
    "            steps_mean = valid_data['original_total_steps'].mean()\n",
    "            steps_std = valid_data['original_total_steps'].std()\n",
    "            llm_mean = valid_data['total_llm_calls'].mean()\n",
    "            llm_std = valid_data['total_llm_calls'].std()\n",
    "            \n",
    "            # Calculate linear regression parameters\n",
    "            slope, intercept = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            \n",
    "            # Interpret correlation strength for this version\n",
    "            if abs(correlation) >= 0.7:\n",
    "                strength = \"Strong\"\n",
    "                interpretation = \"Highly predictable scaling\"\n",
    "            elif abs(correlation) >= 0.5:\n",
    "                strength = \"Moderate\"\n",
    "                interpretation = \"Moderately predictable scaling\"\n",
    "            elif abs(correlation) >= 0.3:\n",
    "                strength = \"Weak\"\n",
    "                interpretation = \"Limited predictability\"\n",
    "            else:\n",
    "                strength = \"Minimal\"\n",
    "                interpretation = \"Unpredictable scaling\"\n",
    "            \n",
    "            report.append(f\"{rank}. {version}\")\n",
    "            report.append(f\"   Correlation coefficient (r): {correlation:6.3f}\")\n",
    "            report.append(f\"   Relationship strength: {strength}\")\n",
    "            report.append(f\"   Interpretation: {interpretation}\")\n",
    "            report.append(f\"   Sample size: {len(valid_data)} problems\")\n",
    "            report.append(\"\")\n",
    "            report.append(f\"   Descriptive Statistics:\")\n",
    "            report.append(f\"     Original Steps: Mean = {steps_mean:5.1f}, SD = {steps_std:5.1f}\")\n",
    "            report.append(f\"     LLM Calls:      Mean = {llm_mean:5.1f}, SD = {llm_std:5.1f}\")\n",
    "            report.append(\"\")\n",
    "            report.append(f\"   Linear Relationship:\")\n",
    "            report.append(f\"     LLM Calls = {slope:.3f} × Original Steps + {intercept:.3f}\")\n",
    "            report.append(f\"     Slope interpretation: {slope:.3f} additional LLM calls per step\")\n",
    "            if intercept > 0:\n",
    "                report.append(f\"     Baseline: {intercept:.1f} LLM calls (fixed overhead)\")\n",
    "            else:\n",
    "                report.append(f\"     Baseline: {abs(intercept):.1f} LLM calls saved (efficiency)\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Calculate R-squared\n",
    "            r_squared = correlation ** 2\n",
    "            report.append(f\"   Explained Variance (R²): {r_squared:.3f} ({r_squared*100:.1f}%)\")\n",
    "            report.append(f\"   → {r_squared*100:.1f}% of LLM call variation explained by step count\")\n",
    "            report.append(\"\")\n",
    "            \n",
    "            # Practical implications for this version\n",
    "            if abs(correlation) >= 0.5:\n",
    "                report.append(f\"   Resource Planning Capability: HIGH\")\n",
    "                report.append(f\"   → Can estimate LLM usage with {r_squared*100:.0f}% accuracy\")\n",
    "                report.append(f\"   → Suitable for cost prediction and capacity planning\")\n",
    "            elif abs(correlation) >= 0.3:\n",
    "                report.append(f\"   Resource Planning Capability: MODERATE\")\n",
    "                report.append(f\"   → Some predictability but other factors also important\")\n",
    "                report.append(f\"   → Use with caution for resource estimation\")\n",
    "            else:\n",
    "                report.append(f\"   Resource Planning Capability: LOW\")\n",
    "                report.append(f\"   → Poor predictability from step count alone\")\n",
    "                report.append(f\"   → Consider additional complexity metrics\")\n",
    "            report.append(\"\")\n",
    "            report.append(\"-\" * 40)\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Cross-version comparison\n",
    "    report.append(\"CROSS-VERSION COMPARISON\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    best_version = max(version_correlations.items(), key=lambda x: abs(x[1]))\n",
    "    worst_version = min(version_correlations.items(), key=lambda x: abs(x[1]))\n",
    "    \n",
    "    report.append(f\"Most Predictable Version: {best_version[0]}\")\n",
    "    report.append(f\"  Correlation: {best_version[1]:.3f}\")\n",
    "    report.append(f\"  Resource planning: Highly reliable\")\n",
    "    report.append(\"\")\n",
    "    report.append(f\"Least Predictable Version: {worst_version[0]}\")\n",
    "    report.append(f\"  Correlation: {worst_version[1]:.3f}\")\n",
    "    report.append(f\"  Resource planning: Unreliable\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Calculate coefficient of variation across versions\n",
    "    correlations_list = list(version_correlations.values())\n",
    "    cv = np.std(correlations_list) / np.mean(np.abs(correlations_list)) if np.mean(np.abs(correlations_list)) > 0 else 0\n",
    "    \n",
    "    report.append(f\"Version Consistency Analysis:\")\n",
    "    report.append(f\"  Correlation range: {correlation_range[0]:.3f} to {correlation_range[1]:.3f}\")\n",
    "    report.append(f\"  Coefficient of variation: {cv:.3f}\")\n",
    "    \n",
    "    if cv < 0.2:\n",
    "        report.append(f\"  → HIGH CONSISTENCY: All versions show similar scaling patterns\")\n",
    "    elif cv < 0.5:\n",
    "        report.append(f\"  → MODERATE CONSISTENCY: Some variation in scaling patterns\")\n",
    "    else:\n",
    "        report.append(f\"  → LOW CONSISTENCY: Significant differences in scaling patterns\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    report.append(\"EFFICIENCY SCALING ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    for version, correlation in sorted_versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        \n",
    "        if len(valid_data) > 1:\n",
    "            slope, intercept = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            \n",
    "            # Efficiency interpretation\n",
    "            if slope < 1.0:\n",
    "                efficiency_rating = \"HIGHLY EFFICIENT\"\n",
    "                efficiency_desc = f\"Uses <1 LLM call per step on average\"\n",
    "            elif slope < 1.5:\n",
    "                efficiency_rating = \"EFFICIENT\"\n",
    "                efficiency_desc = f\"Uses ~1-1.5 LLM calls per step\"\n",
    "            elif slope < 2.0:\n",
    "                efficiency_rating = \"MODERATE\"\n",
    "                efficiency_desc = f\"Uses ~1.5-2 LLM calls per step\"\n",
    "            else:\n",
    "                efficiency_rating = \"INEFFICIENT\"\n",
    "                efficiency_desc = f\"Uses >2 LLM calls per step\"\n",
    "            \n",
    "            report.append(f\"{version}:\")\n",
    "            report.append(f\"  Scaling rate: {slope:.3f} LLM calls per step\")\n",
    "            report.append(f\"  Efficiency: {efficiency_rating}\")\n",
    "            report.append(f\"  Description: {efficiency_desc}\")\n",
    "            \n",
    "            # Calculate cost implications (assuming cost per LLM call)\n",
    "            if intercept > 0:\n",
    "                report.append(f\"  Fixed overhead: {intercept:.1f} LLM calls per problem\")\n",
    "            else:\n",
    "                report.append(f\"  Efficiency bonus: {abs(intercept):.1f} LLM calls saved\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Predictive modeling insights\n",
    "    report.append(\"PREDICTIVE MODELING INSIGHTS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    high_predictability_versions = [v for v, r in version_correlations.items() if abs(r) >= 0.5]\n",
    "    moderate_predictability_versions = [v for v, r in version_correlations.items() if 0.3 <= abs(r) < 0.5]\n",
    "    low_predictability_versions = [v for v, r in version_correlations.items() if abs(r) < 0.3]\n",
    "    \n",
    "    report.append(f\"High Predictability Versions ({len(high_predictability_versions)}):\")\n",
    "    for version in high_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Moderate Predictability Versions ({len(moderate_predictability_versions)}):\")\n",
    "    for version in moderate_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Low Predictability Versions ({len(low_predictability_versions)}):\")\n",
    "    for version in low_predictability_versions:\n",
    "        corr = version_correlations[version]\n",
    "        report.append(f\"  • {version}: r = {corr:.3f} (R² = {corr**2:.3f})\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    report.append(\"PRACTICAL RECOMMENDATIONS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    report.append(\"Resource Planning:\")\n",
    "    if len(high_predictability_versions) > 0:\n",
    "        report.append(f\"  • Use {', '.join(high_predictability_versions)} for reliable cost estimation\")\n",
    "        report.append(f\"  • Step count provides good proxy for LLM usage\")\n",
    "        report.append(f\"  • Implement step-based capacity planning\")\n",
    "    else:\n",
    "        report.append(f\"  • Step count alone insufficient for resource planning\")\n",
    "        report.append(f\"  • Consider additional complexity metrics\")\n",
    "        report.append(f\"  • Implement dynamic resource allocation\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Version Selection:\")\n",
    "    if best_version[1] >= 0.5:\n",
    "        report.append(f\"  • Recommended: {best_version[0]} (most predictable scaling)\")\n",
    "        report.append(f\"  • Provides {(best_version[1]**2)*100:.0f}% resource predictability\")\n",
    "    report.append(f\"  • Avoid: {worst_version[0]} (unpredictable scaling)\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"Cost Optimization:\")\n",
    "    # Find most efficient version (lowest slope)\n",
    "    efficiency_ranking = []\n",
    "    for version, correlation in version_correlations.items():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        valid_data = version_data.dropna(subset=['original_total_steps', 'total_llm_calls'])\n",
    "        if len(valid_data) > 1:\n",
    "            slope, _ = np.polyfit(valid_data['original_total_steps'], valid_data['total_llm_calls'], 1)\n",
    "            efficiency_ranking.append((version, slope))\n",
    "    \n",
    "    if efficiency_ranking:\n",
    "        most_efficient = min(efficiency_ranking, key=lambda x: x[1])\n",
    "        report.append(f\"  • Most efficient: {most_efficient[0]} ({most_efficient[1]:.3f} LLM calls per step)\")\n",
    "        report.append(f\"  • Focus optimization efforts on inefficient versions\")\n",
    "        report.append(f\"  • Monitor scaling rates for cost control\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"Quality Assurance:\")\n",
    "    report.append(f\"  • Set up monitoring for step-to-LLM call ratios\")\n",
    "    report.append(f\"  • Alert on deviations from expected scaling patterns\")\n",
    "    report.append(f\"  • Regular recalibration of prediction models\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"END OF STEPS vs LLM CALLS RELATIONSHIP ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Convert to string\n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Print to console\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"\\n✅ Steps vs LLM calls report saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving steps vs LLM calls report: {e}\")\n",
    "    \n",
    "    return report_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STUART-MAXWELL TEST ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stuart_maxwell_analysis(combined_df):\n",
    "    \"\"\"\n",
    "    Perform Stuart-Maxwell test analysis on rating transition matrices for each version\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🧪 STUART-MAXWELL TEST ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Define transition columns\n",
    "    transition_columns = [\n",
    "        'neg1_to_neg1', 'neg1_to_zero', 'neg1_to_pos1',\n",
    "        'zero_to_neg1', 'zero_to_zero', 'zero_to_pos1', \n",
    "        'pos1_to_neg1', 'pos1_to_zero', 'pos1_to_pos1'\n",
    "    ]\n",
    "    \n",
    "    # Check which transition columns exist\n",
    "    available_transition_cols = [col for col in transition_columns if col in combined_df.columns]\n",
    "    missing_transition_cols = [col for col in transition_columns if col not in combined_df.columns]\n",
    "    \n",
    "    print(f\"📊 Available transition columns: {len(available_transition_cols)}/{len(transition_columns)}\")\n",
    "    if missing_transition_cols:\n",
    "        print(f\"⚠️  Missing columns: {missing_transition_cols}\")\n",
    "    \n",
    "    if len(available_transition_cols) < 9:\n",
    "        print(f\"❌ Cannot perform complete Stuart-Maxwell analysis - need all 9 transition columns\")\n",
    "        return None\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    stuart_maxwell_results = {}\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    fig.suptitle('🧪 Stuart-Maxwell Test Analysis: Rating Transition Matrices', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Stuart-Maxwell test function\n",
    "    def stuart_maxwell_test(transition_matrix):\n",
    "        \"\"\"\n",
    "        Perform Stuart-Maxwell test for marginal homogeneity\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.stats import chi2\n",
    "            \n",
    "            # Reshape 3x3 matrix to vectors\n",
    "            matrix = np.array(transition_matrix).reshape(3, 3)\n",
    "            \n",
    "            # Calculate marginal totals\n",
    "            row_marginals = matrix.sum(axis=1)  # Original ratings\n",
    "            col_marginals = matrix.sum(axis=0)  # Verified ratings\n",
    "            \n",
    "            # Calculate differences\n",
    "            d = row_marginals - col_marginals\n",
    "            \n",
    "            # Calculate variance-covariance matrix\n",
    "            n = matrix.sum()\n",
    "            if n == 0:\n",
    "                return np.nan, np.nan, \"No data\"\n",
    "            \n",
    "            # Simplified Stuart-Maxwell statistic calculation\n",
    "            # For 3x3 table, we use the first k-1=2 differences\n",
    "            d_reduced = d[:-1]  # Remove last element (redundant)\n",
    "            \n",
    "            # Calculate covariance matrix elements\n",
    "            V = np.zeros((2, 2))\n",
    "            \n",
    "            # Diagonal elements\n",
    "            V[0, 0] = (row_marginals[0] + col_marginals[0] - 2*matrix[0, 0]) / n\n",
    "            V[1, 1] = (row_marginals[1] + col_marginals[1] - 2*matrix[1, 1]) / n\n",
    "            \n",
    "            # Off-diagonal elements  \n",
    "            V[0, 1] = V[1, 0] = -(matrix[0, 1] + matrix[1, 0]) / n\n",
    "            \n",
    "            # Check if V is invertible\n",
    "            det_V = np.linalg.det(V)\n",
    "            if abs(det_V) < 1e-10:\n",
    "                return np.nan, np.nan, \"Singular covariance matrix\"\n",
    "            \n",
    "            # Calculate test statistic\n",
    "            V_inv = np.linalg.inv(V)\n",
    "            test_statistic = n * d_reduced.T @ V_inv @ d_reduced\n",
    "            \n",
    "            # Degrees of freedom (k-1 = 2 for 3x3 table)\n",
    "            df = 2\n",
    "            \n",
    "            # Calculate p-value\n",
    "            p_value = 1 - chi2.cdf(test_statistic, df)\n",
    "            \n",
    "            return test_statistic, p_value, \"Success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return np.nan, np.nan, f\"Error: {str(e)}\"\n",
    "    \n",
    "    # Analyze each version\n",
    "    for idx, version in enumerate(versions):\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        \n",
    "        # Sum transition counts for this version\n",
    "        transition_counts = version_data[available_transition_cols].sum()\n",
    "        \n",
    "        # Reshape to 3x3 matrix\n",
    "        transition_matrix = transition_counts.values.reshape(3, 3)\n",
    "        \n",
    "        # Perform Stuart-Maxwell test\n",
    "        test_stat, p_value, status = stuart_maxwell_test(transition_matrix)\n",
    "        \n",
    "        # Store results\n",
    "        stuart_maxwell_results[version] = {\n",
    "            'transition_matrix': transition_matrix,\n",
    "            'test_statistic': test_stat,\n",
    "            'p_value': p_value,\n",
    "            'status': status,\n",
    "            'sample_size': transition_matrix.sum(),\n",
    "            'marginal_homogeneity': 'Rejected' if (not np.isnan(p_value) and p_value < 0.05) else 'Not Rejected'\n",
    "        }\n",
    "        \n",
    "        # Create subplot for this version's transition matrix\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        \n",
    "        if row < 3:  # Only plot if we have space\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Create heatmap\n",
    "            im = ax.imshow(transition_matrix, cmap='Blues', aspect='auto')\n",
    "            \n",
    "            # Add text annotations\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    text = ax.text(j, i, int(transition_matrix[i, j]),\n",
    "                                 ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "            \n",
    "            ax.set_xticks([0, 1, 2])\n",
    "            ax.set_yticks([0, 1, 2])\n",
    "            ax.set_xticklabels(['To -1', 'To 0', 'To +1'])\n",
    "            ax.set_yticklabels(['From -1', 'From 0', 'From +1'])\n",
    "            ax.set_title(f'{version}\\nStuart-Maxwell: p={p_value:.4f}' if not np.isnan(p_value) else f'{version}\\nTest Failed')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=ax, shrink=0.6)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(versions), 6):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        if row < 3:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"\\n📋 STUART-MAXWELL TEST RESULTS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for version, results in stuart_maxwell_results.items():\n",
    "        print(f\"\\n{version}:\")\n",
    "        print(f\"  • Sample Size: {results['sample_size']}\")\n",
    "        print(f\"  • Test Statistic: {results['test_statistic']:.4f}\" if not np.isnan(results['test_statistic']) else \"  • Test Statistic: Failed\")\n",
    "        print(f\"  • P-value: {results['p_value']:.4f}\" if not np.isnan(results['p_value']) else \"  • P-value: N/A\")\n",
    "        print(f\"  • Marginal Homogeneity: {results['marginal_homogeneity']}\")\n",
    "        print(f\"  • Status: {results['status']}\")\n",
    "        \n",
    "        # Calculate marginal totals for interpretation\n",
    "        matrix = results['transition_matrix']\n",
    "        row_marginals = matrix.sum(axis=1)\n",
    "        col_marginals = matrix.sum(axis=0)\n",
    "        \n",
    "        print(f\"  • Original Ratings: [-1: {row_marginals[0]}, 0: {row_marginals[1]}, +1: {row_marginals[2]}]\")\n",
    "        print(f\"  • Verified Ratings: [-1: {col_marginals[0]}, 0: {col_marginals[1]}, +1: {col_marginals[2]}]\")\n",
    "    \n",
    "    # Cross-version comparison analysis\n",
    "    print(f\"\\n🔍 CROSS-VERSION COMPARISON:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Compare test statistics\n",
    "    valid_results = {v: r for v, r in stuart_maxwell_results.items() \n",
    "                    if not np.isnan(r['test_statistic'])}\n",
    "    \n",
    "    if len(valid_results) >= 2:\n",
    "        # Find version with strongest evidence against marginal homogeneity\n",
    "        max_test_stat = max(valid_results.values(), key=lambda x: x['test_statistic'])\n",
    "        min_test_stat = min(valid_results.values(), key=lambda x: x['test_statistic'])\n",
    "        \n",
    "        max_version = [v for v, r in valid_results.items() if r['test_statistic'] == max_test_stat['test_statistic']][0]\n",
    "        min_version = [v for v, r in valid_results.items() if r['test_statistic'] == min_test_stat['test_statistic']][0]\n",
    "        \n",
    "        print(f\"🔥 Most asymmetric transitions: {max_version} (χ²={max_test_stat['test_statistic']:.3f})\")\n",
    "        print(f\"✅ Most symmetric transitions: {min_version} (χ²={min_test_stat['test_statistic']:.3f})\")\n",
    "        \n",
    "        # Count significant results\n",
    "        significant_versions = [v for v, r in valid_results.items() \n",
    "                              if not np.isnan(r['p_value']) and r['p_value'] < 0.05]\n",
    "        \n",
    "        print(f\"\\n📊 SUMMARY STATISTICS:\")\n",
    "        print(f\"  • Versions with significant asymmetry: {len(significant_versions)}/{len(valid_results)}\")\n",
    "        if significant_versions:\n",
    "            print(f\"  • Asymmetric versions: {', '.join(significant_versions)}\")\n",
    "        \n",
    "        # Calculate improvement/degradation patterns\n",
    "        print(f\"\\n📈 RATING CHANGE PATTERNS:\")\n",
    "        for version, results in stuart_maxwell_results.items():\n",
    "            matrix = results['transition_matrix']\n",
    "            total_transitions = matrix.sum()\n",
    "            \n",
    "            if total_transitions > 0:\n",
    "                # Calculate improvement (moving to higher rating)\n",
    "                improvements = matrix[0, 1] + matrix[0, 2] + matrix[1, 2]  # -1→0, -1→+1, 0→+1\n",
    "                degradations = matrix[1, 0] + matrix[2, 0] + matrix[2, 1]  # 0→-1, +1→-1, +1→0\n",
    "                unchanged = matrix[0, 0] + matrix[1, 1] + matrix[2, 2]     # -1→-1, 0→0, +1→+1\n",
    "                \n",
    "                improvement_rate = (improvements / total_transitions) * 100\n",
    "                degradation_rate = (degradations / total_transitions) * 100\n",
    "                stability_rate = (unchanged / total_transitions) * 100\n",
    "                \n",
    "                print(f\"  {version}:\")\n",
    "                print(f\"    • Improvements: {improvement_rate:.1f}% ({improvements}/{total_transitions})\")\n",
    "                print(f\"    • Degradations: {degradation_rate:.1f}% ({degradations}/{total_transitions})\")\n",
    "                print(f\"    • Stable: {stability_rate:.1f}% ({unchanged}/{total_transitions})\")\n",
    "    \n",
    "    # Net change analysis\n",
    "    print(f\"\\n⚖️  NET RATING CHANGES:\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    net_changes = {}\n",
    "    for version, results in stuart_maxwell_results.items():\n",
    "        matrix = results['transition_matrix']\n",
    "        \n",
    "        # Calculate net change for each rating category\n",
    "        neg1_net = (matrix[:, 0].sum() - matrix[0, :].sum())  # Net flow into -1\n",
    "        zero_net = (matrix[:, 1].sum() - matrix[1, :].sum())  # Net flow into 0  \n",
    "        pos1_net = (matrix[:, 2].sum() - matrix[2, :].sum())  # Net flow into +1\n",
    "        \n",
    "        net_changes[version] = {\n",
    "            'neg1_net': neg1_net,\n",
    "            'zero_net': zero_net, \n",
    "            'pos1_net': pos1_net\n",
    "        }\n",
    "        \n",
    "        print(f\"{version}:\")\n",
    "        print(f\"  • Net change to -1: {neg1_net:+d}\")\n",
    "        print(f\"  • Net change to  0: {zero_net:+d}\")\n",
    "        print(f\"  • Net change to +1: {pos1_net:+d}\")\n",
    "    \n",
    "    # Overall insights\n",
    "    print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "    print(\"=\"*25)\n",
    "    \n",
    "    if valid_results:\n",
    "        avg_test_stat = np.mean([r['test_statistic'] for r in valid_results.values()])\n",
    "        significant_count = len([v for v, r in valid_results.items() \n",
    "                               if not np.isnan(r['p_value']) and r['p_value'] < 0.05])\n",
    "        \n",
    "        if significant_count > len(valid_results) / 2:\n",
    "            print(\"🔄 MAJOR FINDING: Most versions show significant rating asymmetries\")\n",
    "            print(\"   → Verification process systematically changes rating distributions\")\n",
    "        elif significant_count > 0:\n",
    "            print(\"📊 MIXED FINDINGS: Some versions show rating asymmetries\")\n",
    "            print(\"   → Version-specific effects on rating patterns\")\n",
    "        else:\n",
    "            print(\"✅ BALANCED FINDINGS: No strong evidence of systematic rating bias\")\n",
    "            print(\"   → Verification maintains rating distributions well\")\n",
    "        \n",
    "        # Best performing version\n",
    "        best_balance = min(valid_results.items(), key=lambda x: x[1]['test_statistic'])\n",
    "        print(f\"\\n🏆 Most balanced rating transitions: {best_balance[0]}\")\n",
    "        print(f\"   → Lowest asymmetry score: {best_balance[1]['test_statistic']:.3f}\")\n",
    "    \n",
    "    return stuart_maxwell_results, net_changes\n",
    "\n",
    "# Run Stuart-Maxwell analysis\n",
    "stuart_maxwell_results, net_changes = create_stuart_maxwell_analysis(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STUART-MAXWELL TEST REPORT GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stuart_maxwell_report(stuart_maxwell_results, net_changes, save_to_file=False, filename='stuart_maxwell_report.txt'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive text report of Stuart-Maxwell test analysis for thesis documentation\n",
    "    \"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"STUART-MAXWELL TEST ANALYSIS REPORT\")\n",
    "    report.append(\"Marginal Homogeneity Testing for Rating Transition Matrices\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    valid_results = {v: r for v, r in stuart_maxwell_results.items() \n",
    "                    if not np.isnan(r['test_statistic'])}\n",
    "    \n",
    "    if len(valid_results) > 0:\n",
    "        # Calculate summary statistics\n",
    "        significant_versions = [v for v, r in valid_results.items() \n",
    "                              if not np.isnan(r['p_value']) and r['p_value'] < 0.05]\n",
    "        \n",
    "        avg_test_stat = np.mean([r['test_statistic'] for r in valid_results.values()])\n",
    "        total_versions = len(valid_results)\n",
    "        significant_count = len(significant_versions)\n",
    "        \n",
    "        report.append(f\"Total versions analyzed: {total_versions}\")\n",
    "        report.append(f\"Versions with significant asymmetry (p < 0.05): {significant_count}/{total_versions} ({(significant_count/total_versions)*100:.1f}%)\")\n",
    "        report.append(f\"Average test statistic across versions: {avg_test_stat:.3f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Overall interpretation\n",
    "        if significant_count > total_versions / 2:\n",
    "            report.append(\"OVERALL FINDING: SYSTEMATIC RATING BIAS DETECTED\")\n",
    "            report.append(\"The majority of versions show statistically significant evidence of\")\n",
    "            report.append(\"non-symmetric rating transitions, indicating systematic bias in the\")\n",
    "            report.append(\"verification process.\")\n",
    "        elif significant_count > 0:\n",
    "            report.append(\"OVERALL FINDING: MIXED EVIDENCE OF RATING BIAS\")\n",
    "            report.append(\"Some versions show evidence of rating asymmetries while others\")\n",
    "            report.append(\"maintain balanced transitions.\")\n",
    "        else:\n",
    "            report.append(\"OVERALL FINDING: NO SYSTEMATIC RATING BIAS\")\n",
    "            report.append(\"All versions maintain marginal homogeneity, indicating balanced\")\n",
    "            report.append(\"rating transitions without systematic bias.\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Individual Version Results\n",
    "    report.append(\"DETAILED VERSION ANALYSIS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    for version, results in stuart_maxwell_results.items():\n",
    "        report.append(f\"{version.upper()}\")\n",
    "        report.append(\"─\" * len(version))\n",
    "        \n",
    "        # Basic statistics\n",
    "        report.append(f\"Sample size: {results['sample_size']} transitions\")\n",
    "        \n",
    "        if not np.isnan(results['test_statistic']):\n",
    "            report.append(f\"Stuart-Maxwell χ² statistic: {results['test_statistic']:.4f}\")\n",
    "            report.append(f\"Degrees of freedom: 2\")\n",
    "            report.append(f\"P-value: {results['p_value']:.4f}\")\n",
    "            \n",
    "            # Significance interpretation\n",
    "            if results['p_value'] < 0.001:\n",
    "                significance_level = \"p < 0.001 (***)\"\n",
    "                interpretation = \"Highly significant evidence against marginal homogeneity\"\n",
    "            elif results['p_value'] < 0.01:\n",
    "                significance_level = \"p < 0.01 (**)\"\n",
    "                interpretation = \"Strong evidence against marginal homogeneity\"\n",
    "            elif results['p_value'] < 0.05:\n",
    "                significance_level = \"p < 0.05 (*)\"\n",
    "                interpretation = \"Significant evidence against marginal homogeneity\"\n",
    "            else:\n",
    "                significance_level = \"p ≥ 0.05 (ns)\"\n",
    "                interpretation = \"No significant evidence against marginal homogeneity\"\n",
    "            \n",
    "            report.append(f\"Significance: {significance_level}\")\n",
    "            report.append(f\"Interpretation: {interpretation}\")\n",
    "            \n",
    "        else:\n",
    "            report.append(f\"Test status: {results['status']}\")\n",
    "            report.append(\"Unable to compute test statistic\")\n",
    "        \n",
    "        # Marginal distributions\n",
    "        matrix = results['transition_matrix']\n",
    "        row_marginals = matrix.sum(axis=1)\n",
    "        col_marginals = matrix.sum(axis=0)\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"Transition Matrix Details:\")\n",
    "        report.append(\"  From -1 → To -1: {:4d}   From -1 → To  0: {:4d}   From -1 → To +1: {:4d}\".format(\n",
    "            int(matrix[0, 0]), int(matrix[0, 1]), int(matrix[0, 2])))\n",
    "        report.append(\"  From  0 → To -1: {:4d}   From  0 → To  0: {:4d}   From  0 → To +1: {:4d}\".format(\n",
    "            int(matrix[1, 0]), int(matrix[1, 1]), int(matrix[1, 2])))\n",
    "        report.append(\"  From +1 → To -1: {:4d}   From +1 → To  0: {:4d}   From +1 → To +1: {:4d}\".format(\n",
    "            int(matrix[2, 0]), int(matrix[2, 1]), int(matrix[2, 2])))\n",
    "        \n",
    "        # Calculate percentages for each transition\n",
    "        total_transitions = matrix.sum()\n",
    "        if total_transitions > 0:\n",
    "            report.append(\"\")\n",
    "            report.append(\"Transition Percentages:\")\n",
    "            for i, from_rating in enumerate(['-1', ' 0', '+1']):\n",
    "                for j, to_rating in enumerate(['-1', ' 0', '+1']):\n",
    "                    count = int(matrix[i, j])\n",
    "                    percentage = (count / total_transitions) * 100\n",
    "                    report.append(\"  From {} → To {}: {:4d} ({:5.1f}%)\".format(\n",
    "                        from_rating, to_rating, count, percentage))\n",
    "        \n",
    "        # Row-wise percentages (conditional probabilities)\n",
    "        report.append(\"\")\n",
    "        report.append(\"Conditional Transition Probabilities (by original rating):\")\n",
    "        for i, from_rating in enumerate(['-1', ' 0', '+1']):\n",
    "            row_total = row_marginals[i]\n",
    "            if row_total > 0:\n",
    "                report.append(\"  Given original rating {}:\".format(from_rating))\n",
    "                for j, to_rating in enumerate(['-1', ' 0', '+1']):\n",
    "                    count = int(matrix[i, j])\n",
    "                    prob = (count / row_total) * 100\n",
    "                    report.append(\"    → To {}: {:4d}/{:4d} ({:5.1f}%)\".format(\n",
    "                        to_rating, count, int(row_total), prob))\n",
    "            else:\n",
    "                report.append(\"  Given original rating {}: No data\".format(from_rating))\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"Marginal Distributions:\")\n",
    "        report.append(f\"  Original ratings:  -1: {int(row_marginals[0])}, 0: {int(row_marginals[1])}, +1: {int(row_marginals[2])}\")\n",
    "        report.append(f\"  Verified ratings:  -1: {int(col_marginals[0])}, 0: {int(col_marginals[1])}, +1: {int(col_marginals[2])}\")\n",
    "        \n",
    "        # Net changes\n",
    "        if version in net_changes:\n",
    "            net = net_changes[version]\n",
    "            report.append(\"\")\n",
    "            report.append(\"Net Rating Changes:\")\n",
    "            report.append(f\"  Net flow to -1: {net['neg1_net']:+d}\")\n",
    "            report.append(f\"  Net flow to  0: {net['zero_net']:+d}\")\n",
    "            report.append(f\"  Net flow to +1: {net['pos1_net']:+d}\")\n",
    "            \n",
    "            # Interpret net changes\n",
    "            total_net_change = abs(net['neg1_net']) + abs(net['zero_net']) + abs(net['pos1_net'])\n",
    "            if total_net_change > 0:\n",
    "                if net['pos1_net'] > 0:\n",
    "                    report.append(\"  → Overall tendency: Ratings improve (positive bias)\")\n",
    "                elif net['neg1_net'] > 0:\n",
    "                    report.append(\"  → Overall tendency: Ratings degrade (negative bias)\")\n",
    "                else:\n",
    "                    report.append(\"  → Overall tendency: Ratings shift toward neutral\")\n",
    "            else:\n",
    "                report.append(\"  → Overall tendency: Perfectly balanced transitions\")\n",
    "        \n",
    "        # Transition pattern analysis\n",
    "        total_transitions = matrix.sum()\n",
    "        if total_transitions > 0:\n",
    "            improvements = matrix[0, 1] + matrix[0, 2] + matrix[1, 2]\n",
    "            degradations = matrix[1, 0] + matrix[2, 0] + matrix[2, 1]\n",
    "            unchanged = matrix[0, 0] + matrix[1, 1] + matrix[2, 2]\n",
    "            \n",
    "            improvement_rate = (improvements / total_transitions) * 100\n",
    "            degradation_rate = (degradations / total_transitions) * 100\n",
    "            stability_rate = (unchanged / total_transitions) * 100\n",
    "            \n",
    "            report.append(\"\")\n",
    "            report.append(\"Transition Patterns:\")\n",
    "            report.append(f\"  Improvements: {improvement_rate:.1f}% ({improvements}/{total_transitions})\")\n",
    "            report.append(f\"  Degradations: {degradation_rate:.1f}% ({degradations}/{total_transitions})\")\n",
    "            report.append(f\"  Unchanged:    {stability_rate:.1f}% ({unchanged}/{total_transitions})\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Cross-version comparison\n",
    "    if len(valid_results) >= 2:\n",
    "        report.append(\"CROSS-VERSION COMPARISON\")\n",
    "        report.append(\"-\" * 50)\n",
    "        \n",
    "        # Rank versions by test statistic\n",
    "        version_rankings = sorted(valid_results.items(), key=lambda x: x[1]['test_statistic'])\n",
    "        \n",
    "        report.append(\"Version Rankings (by asymmetry level, low to high):\")\n",
    "        for rank, (version, results) in enumerate(version_rankings, 1):\n",
    "            status = \"Symmetric\" if results['p_value'] >= 0.05 else \"Asymmetric\"\n",
    "            report.append(f\"  {rank}. {version}: χ² = {results['test_statistic']:.3f} ({status})\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Best and worst performers\n",
    "        best_version, best_results = version_rankings[0]\n",
    "        worst_version, worst_results = version_rankings[-1]\n",
    "        \n",
    "        report.append(f\"Most balanced transitions: {best_version}\")\n",
    "        report.append(f\"  χ² = {best_results['test_statistic']:.3f}, p = {best_results['p_value']:.4f}\")\n",
    "        report.append(\"\")\n",
    "        report.append(f\"Most asymmetric transitions: {worst_version}\")\n",
    "        report.append(f\"  χ² = {worst_results['test_statistic']:.3f}, p = {worst_results['p_value']:.4f}\")\n",
    "        report.append(\"\")\n",
    "        \n",
    "        # Effect size analysis\n",
    "        effect_sizes = []\n",
    "        for version, results in valid_results.items():\n",
    "            # Calculate effect size (Cramér's V approximation)\n",
    "            n = results['sample_size']\n",
    "            chi_sq = results['test_statistic']\n",
    "            if n > 0:\n",
    "                cramers_v = np.sqrt(chi_sq / (n * 2))  # 2 = min(rows-1, cols-1) for 3x3\n",
    "                effect_sizes.append((version, cramers_v))\n",
    "        \n",
    "        if effect_sizes:\n",
    "            effect_sizes.sort(key=lambda x: x[1])\n",
    "            report.append(\"Effect Sizes (Cramér's V approximation):\")\n",
    "            for version, effect_size in effect_sizes:\n",
    "                if effect_size < 0.1:\n",
    "                    magnitude = \"Negligible\"\n",
    "                elif effect_size < 0.3:\n",
    "                    magnitude = \"Small\"\n",
    "                elif effect_size < 0.5:\n",
    "                    magnitude = \"Medium\"\n",
    "                else:\n",
    "                    magnitude = \"Large\"\n",
    "                report.append(f\"  {version}: {effect_size:.3f} ({magnitude})\")\n",
    "            report.append(\"\")\n",
    "    \n",
    "    # Statistical interpretation guide\n",
    "    report.append(\"STATISTICAL INTERPRETATION GUIDE\")\n",
    "    report.append(\"-\" * 50)\n",
    "    report.append(\"Stuart-Maxwell Test:\")\n",
    "    report.append(\"  H₀: Marginal distributions are homogeneous (no systematic bias)\")\n",
    "    report.append(\"  H₁: Marginal distributions are not homogeneous (systematic bias exists)\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Significance Levels:\")\n",
    "    report.append(\"  p < 0.001 (***): Highly significant evidence of bias\")\n",
    "    report.append(\"  p < 0.01  (**):  Strong evidence of bias\")\n",
    "    report.append(\"  p < 0.05  (*):   Significant evidence of bias\")\n",
    "    report.append(\"  p ≥ 0.05  (ns):  No significant evidence of bias\")\n",
    "    report.append(\"\")\n",
    "    report.append(\"Effect Size Interpretation (Cramér's V):\")\n",
    "    report.append(\"  < 0.1: Negligible effect\")\n",
    "    report.append(\"  0.1-0.3: Small effect\")\n",
    "    report.append(\"  0.3-0.5: Medium effect\")\n",
    "    report.append(\"  > 0.5: Large effect\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Practical implications\n",
    "    report.append(\"PRACTICAL IMPLICATIONS\")\n",
    "    report.append(\"-\" * 50)\n",
    "    \n",
    "    if len(valid_results) > 0:\n",
    "        if significant_count > total_versions / 2:\n",
    "            report.append(\"RECOMMENDATION: SYSTEMATIC BIAS CORRECTION NEEDED\")\n",
    "            report.append(\"• Multiple versions show significant rating asymmetries\")\n",
    "            report.append(\"• Consider implementing bias correction mechanisms\")\n",
    "            report.append(\"• Investigate root causes of systematic rating shifts\")\n",
    "            report.append(f\"• Use {best_version} as benchmark for balanced transitions\")\n",
    "            \n",
    "        elif significant_count > 0:\n",
    "            report.append(\"RECOMMENDATION: VERSION-SPECIFIC OPTIMIZATION\")\n",
    "            report.append(\"• Some versions maintain balanced transitions while others show bias\")\n",
    "            report.append(f\"• Prioritize deployment of balanced versions: {', '.join([v for v, r in version_rankings[:2]])}\")\n",
    "            report.append(f\"• Investigate and correct biased versions: {', '.join(significant_versions)}\")\n",
    "            \n",
    "        else:\n",
    "            report.append(\"RECOMMENDATION: MAINTAIN CURRENT APPROACH\")\n",
    "            report.append(\"• All versions demonstrate balanced rating transitions\")\n",
    "            report.append(\"• No evidence of systematic bias across the verification system\")\n",
    "            report.append(\"• Continue monitoring for potential bias development\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "        report.append(\"Quality Assurance Guidelines:\")\n",
    "        report.append(\"• Monitor transition matrices regularly for bias development\")\n",
    "        report.append(\"• Set up alerts for p-values < 0.05 in production systems\")\n",
    "        report.append(\"• Consider periodic recalibration if systematic bias emerges\")\n",
    "        report.append(f\"• Use {best_version} configuration as the gold standard\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"END OF STUART-MAXWELL TEST ANALYSIS REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    \n",
    "    # Convert to string\n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Print to console\n",
    "    print(report_text)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(report_text)\n",
    "            print(f\"\\n✅ Report saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error saving report: {e}\")\n",
    "    \n",
    "    return report_text\n",
    "\n",
    "# Generate the Stuart-Maxwell report\n",
    "if stuart_maxwell_results is not None:\n",
    "    stuart_maxwell_report = generate_stuart_maxwell_report(\n",
    "        stuart_maxwell_results, \n",
    "        net_changes, \n",
    "        save_to_file=False,  # Set to True to save to file\n",
    "        filename='stuart_maxwell_thesis_report.txt'\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ Stuart-Maxwell analysis results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Calls & Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the reports\n",
    "if correlation_results is not None:\n",
    "    correlation_report = generate_correlation_heatmap_report(\n",
    "        correlation_results, \n",
    "        combined_df,\n",
    "        save_to_file=False,  # Set to True to save to file\n",
    "        filename='correlation_heatmap_thesis_report.txt'\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ Correlation analysis results not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "if steps_llm_results is not None:\n",
    "    steps_llm_report = generate_steps_llm_relationship_report(\n",
    "        steps_llm_results,\n",
    "        combined_df,\n",
    "        save_to_file=False,  # Set to True to save to file\n",
    "        filename='steps_llm_relationship_thesis_report.txt'\n",
    "    )\n",
    "else:\n",
    "    print(\"❌ Steps vs LLM calls analysis results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_and_recommendations(combined_df, ranking_results):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis and generate actionable recommendations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 STATISTICAL ANALYSIS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    \n",
    "    # Statistical significance testing\n",
    "    print(\"🔬 STATISTICAL SIGNIFICANCE TESTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # ANOVA tests for continuous variables\n",
    "    continuous_vars = ['processing_time_ms', 'total_llm_calls', 'token_efficiency', 'rating_improvement']\n",
    "    \n",
    "    for var in continuous_vars:\n",
    "        if var in combined_df.columns:\n",
    "            groups = []\n",
    "            group_names = []\n",
    "            for version in versions:\n",
    "                version_data = combined_df[combined_df['version'] == version][var].dropna()\n",
    "                if len(version_data) > 1:  # Need at least 2 data points\n",
    "                    groups.append(version_data)\n",
    "                    group_names.append(version)\n",
    "            \n",
    "            if len(groups) >= 2:\n",
    "                try:\n",
    "                    # Check if there's any variance in the data\n",
    "                    all_data = np.concatenate(groups)\n",
    "                    if np.var(all_data) > 1e-10:  # Avoid division by zero\n",
    "                        f_stat, p_value = stats.f_oneway(*groups)\n",
    "                        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                        print(f\"  {var}: F={f_stat:.3f}, p={p_value:.4f} {significance}\")\n",
    "                    else:\n",
    "                        print(f\"  {var}: No variance detected (all values approximately equal)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  {var}: ANOVA failed ({str(e)[:30]}...)\")\n",
    "            else:\n",
    "                print(f\"  {var}: Insufficient data for ANOVA (need ≥2 groups with ≥2 data points each)\")\n",
    "    \n",
    "    # Effect size analysis (Cohen's d between best and worst versions)\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    worst_version = ranking_results['final_ranking'][-1][0]\n",
    "    \n",
    "    print(f\"\\n📏 EFFECT SIZE ANALYSIS ({best_version} vs {worst_version}):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    def cohens_d(group1, group2):\n",
    "        \"\"\"Calculate Cohen's d effect size with error handling\"\"\"\n",
    "        try:\n",
    "            if len(group1) < 2 or len(group2) < 2:\n",
    "                return np.nan\n",
    "            \n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            s1, s2 = group1.std(), group2.std()\n",
    "            \n",
    "            # Handle case where standard deviations are zero\n",
    "            if s1 == 0 and s2 == 0:\n",
    "                return 0.0 if group1.mean() == group2.mean() else np.inf\n",
    "            \n",
    "            pooled_std = np.sqrt(((n1-1)*s1**2 + (n2-1)*s2**2) / (n1+n2-2))\n",
    "            \n",
    "            if pooled_std == 0:\n",
    "                return 0.0 if group1.mean() == group2.mean() else np.inf\n",
    "            \n",
    "            return (group1.mean() - group2.mean()) / pooled_std\n",
    "        except Exception as e:\n",
    "            return np.nan\n",
    "    \n",
    "    for var in continuous_vars:\n",
    "        if var in combined_df.columns:\n",
    "            best_data = combined_df[combined_df['version'] == best_version][var].dropna()\n",
    "            worst_data = combined_df[combined_df['version'] == worst_version][var].dropna()\n",
    "            \n",
    "            if len(best_data) > 0 and len(worst_data) > 0:\n",
    "                effect_size = cohens_d(best_data, worst_data)\n",
    "                magnitude = (\"Large\" if abs(effect_size) >= 0.8 else \n",
    "                           \"Medium\" if abs(effect_size) >= 0.5 else \"Small\")\n",
    "                print(f\"  {var}: Cohen's d = {effect_size:.3f} ({magnitude})\")\n",
    "    \n",
    "    # Confidence intervals for key metrics\n",
    "    print(f\"\\n🎯 95% CONFIDENCE INTERVALS FOR KEY METRICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        print(f\"\\n{version}:\")\n",
    "        \n",
    "        # Success rate CI\n",
    "        n = len(version_data)\n",
    "        p = version_data['success'].mean()\n",
    "        se = np.sqrt(p * (1-p) / n)\n",
    "        ci_lower = p - 1.96 * se\n",
    "        ci_upper = p + 1.96 * se\n",
    "        print(f\"  Success Rate: {p*100:.1f}% [{ci_lower*100:.1f}%, {ci_upper*100:.1f}%]\")\n",
    "        \n",
    "        # Processing time CI\n",
    "        if 'processing_time_ms' in version_data.columns:\n",
    "            times = version_data['processing_time_ms'].dropna()\n",
    "            if len(times) > 1:\n",
    "                mean_time = times.mean()\n",
    "                se_time = times.std() / np.sqrt(len(times))\n",
    "                ci_lower_time = mean_time - 1.96 * se_time\n",
    "                ci_upper_time = mean_time + 1.96 * se_time\n",
    "                print(f\"  Processing Time: {mean_time:.1f}ms [{ci_lower_time:.1f}ms, {ci_upper_time:.1f}ms]\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(f\"\\n🎯 ACTIONABLE RECOMMENDATIONS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Best performer analysis\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    best_score = ranking_results['final_ranking'][0][1]\n",
    "    \n",
    "    recommendations.append(f\"🥇 ADOPT BEST PRACTICES: {best_version} achieved the highest overall score ({best_score:.1f}/100)\")\n",
    "    \n",
    "    # Identify strongest metrics per version\n",
    "    metric_winners = {}\n",
    "    for metric in ranking_results['metric_rankings']:\n",
    "        winner = ranking_results['metric_rankings'][metric][0]\n",
    "        metric_winners[metric] = winner\n",
    "    \n",
    "    print(f\"\\n🏆 METRIC CHAMPIONS:\")\n",
    "    for metric, (version, score) in metric_winners.items():\n",
    "        print(f\"  • {metric.replace('_', ' ').title()}: {version} ({score:.1f}/100)\")\n",
    "        \n",
    "        if metric == 'processing_speed':\n",
    "            recommendations.append(f\"⚡ SPEED OPTIMIZATION: Learn from {version}'s processing optimizations\")\n",
    "        elif metric == 'accuracy_improvement':\n",
    "            recommendations.append(f\"🎯 ACCURACY FOCUS: Adopt {version}'s verification strategies\")\n",
    "        elif metric == 'token_efficiency':\n",
    "            recommendations.append(f\"💰 COST OPTIMIZATION: Implement {version}'s token efficiency techniques\")\n",
    "    \n",
    "    # Identify improvement opportunities\n",
    "    print(f\"\\n🔍 IMPROVEMENT OPPORTUNITIES:\")\n",
    "    \n",
    "    for version in versions:\n",
    "        version_scores = {metric: ranking_results['normalized_data'][version][metric] \n",
    "                         for metric in ranking_results['normalized_data'][version]}\n",
    "        weakest_metric = min(version_scores, key=version_scores.get)\n",
    "        weakest_score = version_scores[weakest_metric]\n",
    "        \n",
    "        if weakest_score < 50:  # Below average performance\n",
    "            print(f\"  • {version}: Improve {weakest_metric.replace('_', ' ')} (current: {weakest_score:.1f}/100)\")\n",
    "            recommendations.append(f\"📈 {version} FOCUS: Prioritize {weakest_metric.replace('_', ' ')} improvements\")\n",
    "    \n",
    "    # Version-specific insights\n",
    "    print(f\"\\n💡 VERSION-SPECIFIC INSIGHTS:\")\n",
    "    \n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        insights = []\n",
    "        \n",
    "        # Success rate insight\n",
    "        success_rate = version_data['success'].mean() * 100\n",
    "        if success_rate >= 95:\n",
    "            insights.append(\"Very high reliability\")\n",
    "        elif success_rate < 80:\n",
    "            insights.append(\"Reliability concerns\")\n",
    "        \n",
    "        # Processing time insight\n",
    "        avg_time = version_data['processing_time_ms'].mean()\n",
    "        if avg_time < 50:\n",
    "            insights.append(\"Fast processing\")\n",
    "        elif avg_time > 100:\n",
    "            insights.append(\"Slow processing\")\n",
    "        \n",
    "        # Token efficiency insight\n",
    "        token_eff = version_data['token_efficiency'].mean()\n",
    "        if token_eff > 0.3:\n",
    "            insights.append(\"High token efficiency\")\n",
    "        elif token_eff < 0.15:\n",
    "            insights.append(\"Low token efficiency\")\n",
    "        \n",
    "        if insights:\n",
    "            print(f\"  • {version}: {', '.join(insights)}\")\n",
    "    \n",
    "    # Final recommendations\n",
    "    print(f\"\\n📋 FINAL RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Strategic recommendations\n",
    "    strategic_recs = [\n",
    "        \"🔄 Implement A/B testing framework to continuously evaluate version performance\",\n",
    "        \"📊 Set up monitoring dashboards to track key metrics in real-time\", \n",
    "        \"🎯 Focus development efforts on the weakest performing metrics\",\n",
    "        \"🔬 Conduct detailed analysis of failure cases to improve reliability\",\n",
    "        \"💰 Optimize token usage based on best-performing version's strategies\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🚀 STRATEGIC RECOMMENDATIONS:\")\n",
    "    for i, rec in enumerate(strategic_recs, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "\n",
    "    return {\n",
    "        'recommendations': recommendations,\n",
    "        'strategic_recommendations': strategic_recs,\n",
    "        'metric_winners': metric_winners\n",
    "    }\n",
    "\n",
    "# Run statistical analysis and recommendations\n",
    "analysis_recommendations = statistical_analysis_and_recommendations(combined_df, ranking_results)\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# EXPORT AND SUMMARY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_executive_summary(combined_df, ranking_results, analysis_recommendations):\n",
    "    \"\"\"\n",
    "    Create an executive summary for stakeholders\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📋 EXECUTIVE SUMMARY - VERIFICATION SYSTEM VERSION COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Key findings\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    best_score = ranking_results['final_ranking'][0][1]\n",
    "    worst_version = ranking_results['final_ranking'][-1][0]\n",
    "    worst_score = ranking_results['final_ranking'][-1][1]\n",
    "    \n",
    "    total_problems = len(combined_df)\n",
    "    num_versions = len(combined_df['version'].unique())\n",
    "    \n",
    "    print(f\"📊 ANALYSIS OVERVIEW:\")\n",
    "    print(f\"   • Total Problems Analyzed: {total_problems:,}\")\n",
    "    print(f\"   • Number of Versions: {num_versions}\")\n",
    "    print(f\"   • Analysis Date: June 2025\")\n",
    "    \n",
    "    print(f\"\\n🏆 KEY FINDINGS:\")\n",
    "    print(f\"   • Best Performing Version: {best_version} (Score: {best_score:.1f}/100)\")\n",
    "    print(f\"   • Lowest Performing Version: {worst_version} (Score: {worst_score:.1f}/100)\")\n",
    "    print(f\"   • Performance Gap: {best_score - worst_score:.1f} points\")\n",
    "    \n",
    "    # Metric champions\n",
    "    print(f\"\\n🥇 METRIC LEADERS:\")\n",
    "    for metric, (version, score) in analysis_recommendations['metric_winners'].items():\n",
    "        print(f\"   • {metric.replace('_', ' ').title()}: {version}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    overall_stats = combined_df.groupby('version').agg({\n",
    "        'success': 'mean',\n",
    "        'processing_time_ms': 'mean',\n",
    "        'total_llm_calls': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\n📈 PERFORMANCE SUMMARY:\")\n",
    "    print(f\"   • Average Success Rate: {combined_df['success'].mean()*100:.1f}%\")\n",
    "    print(f\"   • Average Processing Time: {combined_df['processing_time_ms'].mean():.1f}ms\")\n",
    "    print(f\"   • Average LLM Calls: {combined_df['total_llm_calls'].mean():.1f}\")\n",
    "    \n",
    "    # Recommendations summary\n",
    "    print(f\"\\n💡 TOP 3 RECOMMENDATIONS:\")\n",
    "    top_recs = analysis_recommendations['recommendations'][:3]\n",
    "    for i, rec in enumerate(top_recs, 1):\n",
    "        print(f\"   {i}. {rec}\")\n",
    "\n",
    "def export_multi_version_results(combined_df, ranking_results, analysis_recommendations, \n",
    "                                filename_prefix='multi_version_analysis'):\n",
    "    \"\"\"\n",
    "    Export comprehensive multi-version analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Export summary JSON\n",
    "    summary_data = {\n",
    "        'analysis_info': {\n",
    "            'total_problems': len(combined_df),\n",
    "            'num_versions': len(combined_df['version'].unique()),\n",
    "            'analysis_date': '2025-06-17'\n",
    "        },\n",
    "        'version_ranking': {\n",
    "            'final_ranking': ranking_results['final_ranking'],\n",
    "            'metric_rankings': ranking_results['metric_rankings']\n",
    "        },\n",
    "        'recommendations': {\n",
    "            'actionable_recommendations': analysis_recommendations['recommendations'],\n",
    "            'strategic_recommendations': analysis_recommendations['strategic_recommendations'],\n",
    "            'metric_winners': analysis_recommendations['metric_winners']\n",
    "        },\n",
    "        'version_statistics': {}\n",
    "    }\n",
    "    \n",
    "    # Add version statistics\n",
    "    for version in combined_df['version'].unique():\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        summary_data['version_statistics'][version] = {\n",
    "            'total_problems': len(version_data),\n",
    "            'success_rate': version_data['success'].mean(),\n",
    "            'avg_processing_time': version_data['processing_time_ms'].mean(),\n",
    "            'avg_llm_calls': version_data['total_llm_calls'].mean(),\n",
    "            'avg_token_efficiency': version_data['token_efficiency'].mean()\n",
    "        }\n",
    "    \n",
    "    # Convert numpy types for JSON serialization\n",
    "    def convert_numpy_types(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_types(item) for item in obj]\n",
    "        return obj\n",
    "    \n",
    "    summary_data = convert_numpy_types(summary_data)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{filename_prefix}_summary.json', 'w') as f:\n",
    "            json.dump(summary_data, f, indent=2, default=str)\n",
    "        print(f\"✅ Summary exported to {filename_prefix}_summary.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting summary: {e}\")\n",
    "    \n",
    "    # 2. Export detailed CSV\n",
    "    try:\n",
    "        combined_df.to_csv(f'{filename_prefix}_detailed.csv', index=False)\n",
    "        print(f\"✅ Detailed data exported to {filename_prefix}_detailed.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting detailed data: {e}\")\n",
    "    \n",
    "    # 3. Export ranking table\n",
    "    try:\n",
    "        ranking_df = pd.DataFrame([\n",
    "            {\n",
    "                'Rank': rank,\n",
    "                'Version': version,\n",
    "                'Overall_Score': score,\n",
    "                **ranking_results['raw_data'][version]\n",
    "            }\n",
    "            for rank, (version, score) in enumerate(ranking_results['final_ranking'], 1)\n",
    "        ])\n",
    "        \n",
    "        ranking_df.to_csv(f'{filename_prefix}_ranking.csv', index=False)\n",
    "        print(f\"✅ Ranking table exported to {filename_prefix}_ranking.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting ranking: {e}\")\n",
    "\n",
    "# Create executive summary\n",
    "create_executive_summary(combined_df, ranking_results, analysis_recommendations)\n",
    "\n",
    "# %%\n",
    "# Final Comparison Dashboard\n",
    "def create_final_comparison_dashboard(combined_df, ranking_results):\n",
    "    \"\"\"\n",
    "    Create a comprehensive final dashboard for version comparison\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    fig.suptitle('🎯 FINAL VERSION COMPARISON DASHBOARD', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Create a complex grid layout\n",
    "    gs = fig.add_gridspec(3, 4, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1, 1])\n",
    "    \n",
    "    versions = combined_df['version'].unique()\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(versions)))\n",
    "    \n",
    "    # 1. Overall Ranking (Top Left)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ranking_scores = [score for _, score in ranking_results['final_ranking']]\n",
    "    ranking_versions = [version for version, _ in ranking_results['final_ranking']]\n",
    "    \n",
    "    bars = ax1.barh(range(len(ranking_versions)), ranking_scores, color=colors)\n",
    "    ax1.set_yticks(range(len(ranking_versions)))\n",
    "    ax1.set_yticklabels(ranking_versions)\n",
    "    ax1.set_xlabel('Overall Score')\n",
    "    ax1.set_title('🏆 Final Rankings', fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    for i, (bar, score) in enumerate(zip(bars, ranking_scores)):\n",
    "        ax1.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "                f'{score:.1f}', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Success Rate Comparison (Top Middle)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    success_rates = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        success_rates.append(version_data['success'].mean() * 100)\n",
    "    \n",
    "    bars2 = ax2.bar(versions, success_rates, color=colors)\n",
    "    ax2.set_ylabel('Success Rate (%)')\n",
    "    ax2.set_title('✅ Success Rates', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars2, success_rates):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Processing Time Box Plot (Top Right)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    time_data = []\n",
    "    time_labels = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        times = version_data['processing_time_ms'].dropna()\n",
    "        if len(times) > 0:\n",
    "            time_data.append(times)\n",
    "            time_labels.append(version.replace('Version ', 'V'))\n",
    "    \n",
    "    if time_data:\n",
    "        bp = ax3.boxplot(time_data, labels=time_labels, patch_artist=True)\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "    ax3.set_ylabel('Processing Time (ms)')\n",
    "    ax3.set_title('⏱️ Processing Time', fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Token Efficiency (Top Far Right)\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    efficiency_data = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        efficiency_data.append(version_data['token_efficiency'].mean())\n",
    "    \n",
    "    bars4 = ax4.bar(versions, efficiency_data, color=colors)\n",
    "    ax4.set_ylabel('Token Efficiency')\n",
    "    ax4.set_title('💰 Token Efficiency', fontweight='bold')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars4, efficiency_data):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Performance Radar Chart (Middle Left & Middle)\n",
    "    ax5 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    metrics = ['Success\\nRate', 'Accuracy\\nImprovement', 'Processing\\nSpeed', 'Token\\nEfficiency', 'Rating\\nImprovement']\n",
    "    \n",
    "    # Normalize data for radar chart\n",
    "    normalized_scores = {}\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        \n",
    "        success_norm = version_data['success'].mean() * 100\n",
    "        acc_imp = (version_data['verifier_answer_correctness'].mean() - \n",
    "                   version_data['original_answer_correctness'].mean()) * 100\n",
    "        acc_imp_norm = max(0, min(100, acc_imp + 50))  # Shift to positive scale\n",
    "        speed_norm = max(0, min(100, 100 - (version_data['processing_time_ms'].mean() / 2)))\n",
    "        efficiency_norm = min(100, version_data['token_efficiency'].mean() * 300)\n",
    "        rating_imp = version_data['rating_improvement'].dropna().mean()\n",
    "        rating_norm = max(0, min(100, (rating_imp * 100) + 50)) if not pd.isna(rating_imp) else 50\n",
    "        \n",
    "        normalized_scores[version] = [success_norm, acc_imp_norm, speed_norm, efficiency_norm, rating_norm]\n",
    "    \n",
    "    # Create grouped bar chart instead of radar for better readability\n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, (version, scores) in enumerate(normalized_scores.items()):\n",
    "        ax5.bar(x_pos + i * width, scores, width, label=version, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax5.set_xlabel('Performance Metrics')\n",
    "    ax5.set_ylabel('Normalized Score (0-100)')\n",
    "    ax5.set_title('📊 Multi-Metric Performance Comparison', fontweight='bold')\n",
    "    ax5.set_xticks(x_pos + width * 2)\n",
    "    ax5.set_xticklabels(metrics)\n",
    "    ax5.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Key Insights Text (Middle Right & Far Right)\n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Generate key insights text\n",
    "    best_version = ranking_results['final_ranking'][0][0]\n",
    "    best_score = ranking_results['final_ranking'][0][1]\n",
    "    \n",
    "    insights_text = f\"\"\"\n",
    "🏆 WINNER: {best_version}\n",
    "Overall Score: {best_score:.1f}/100\n",
    "\n",
    "🎯 KEY INSIGHTS:\n",
    "• Best Success Rate: {max(success_rates):.1f}%\n",
    "• Fastest Average Time: {min([combined_df[combined_df['version'] == v]['processing_time_ms'].mean() for v in versions]):.1f}ms\n",
    "• Most Efficient: {max(efficiency_data):.3f} tokens\n",
    "\n",
    "📈 RECOMMENDATIONS:\n",
    "• Adopt {best_version}'s strategies\n",
    "• Focus on weakest metrics\n",
    "• Implement continuous monitoring\n",
    "• A/B test improvements\n",
    "\n",
    "🚀 NEXT STEPS:\n",
    "1. Deploy best version\n",
    "2. Analyze failure patterns  \n",
    "3. Optimize weak performers\n",
    "4. Set up monitoring dashboard\n",
    "\"\"\"\n",
    "    \n",
    "    ax6.text(0.05, 0.95, insights_text, transform=ax6.transAxes, fontsize=11,\n",
    "             verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.3))\n",
    "    \n",
    "    # 7. Statistical Summary Table (Bottom)\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Create summary statistics table\n",
    "    summary_stats = []\n",
    "    for version in versions:\n",
    "        version_data = combined_df[combined_df['version'] == version]\n",
    "        stats_row = [\n",
    "            version,\n",
    "            f\"{len(version_data)}\",\n",
    "            f\"{version_data['success'].mean()*100:.1f}%\",\n",
    "            f\"{version_data['processing_time_ms'].mean():.1f}ms\",\n",
    "            f\"{version_data['total_llm_calls'].mean():.1f}\",\n",
    "            f\"{version_data['token_efficiency'].mean():.3f}\",\n",
    "            f\"{ranking_results['raw_data'][version]['accuracy_improvement']:+.1f}pp\"\n",
    "        ]\n",
    "        summary_stats.append(stats_row)\n",
    "    \n",
    "    # Create table\n",
    "    table_data = summary_stats\n",
    "    col_labels = ['Version', 'Problems', 'Success Rate', 'Avg Time', 'Avg LLM Calls', 'Token Efficiency', 'Accuracy Δ']\n",
    "    \n",
    "    table = ax7.table(cellText=table_data, colLabels=col_labels, loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Color code the table based on ranking\n",
    "    for i, (version, _) in enumerate(ranking_results['final_ranking']):\n",
    "        row_idx = versions.tolist().index(version) + 1  # +1 for header\n",
    "        if i == 0:  # Best performer\n",
    "            for j in range(len(col_labels)):\n",
    "                table[(row_idx, j)].set_facecolor('#90EE90')  # Light green\n",
    "        elif i == len(ranking_results['final_ranking']) - 1:  # Worst performer\n",
    "            for j in range(len(col_labels)):\n",
    "                table[(row_idx, j)].set_facecolor('#FFB6C1')  # Light pink\n",
    "    \n",
    "    ax7.set_title('📋 Detailed Statistics Summary', fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_final_comparison_dashboard(combined_df, ranking_results)\n",
    "\n",
    "# %%\n",
    "# Export results (uncomment to save files)\n",
    "# export_multi_version_results(combined_df, ranking_results, analysis_recommendations)\n",
    "\n",
    "print(\"\\n🎉 MULTI-VERSION ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Comprehensive comparison of all 5 versions completed\")\n",
    "print(\"✅ Statistical analysis and recommendations generated\")\n",
    "print(\"✅ Performance rankings and insights provided\")\n",
    "print(\"✅ Executive summary and dashboard created\")\n",
    "print(\"\\n📁 Uncomment export function to save detailed results\")\n",
    "print(\"🎯 Ready for implementation of recommendations!\")\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "MULTI-VERSION ANALYSIS SUMMARY:\n",
    "===============================\n",
    "\n",
    "This notebook provides comprehensive analysis of 5 verification system versions including:\n",
    "\n",
    "📊 ANALYSIS COMPONENTS:\n",
    "1. Multi-version data loading and preprocessing\n",
    "2. Comparative foundational analysis with statistical testing\n",
    "3. Comprehensive ranking system with weighted metrics\n",
    "4. Performance heatmaps and visualizations\n",
    "5. Statistical significance testing and effect size analysis\n",
    "6. Actionable recommendations and strategic insights\n",
    "7. Executive summary for stakeholders\n",
    "8. Export functions for detailed reporting\n",
    "\n",
    "🎯 KEY FEATURES:\n",
    "• Automated ranking across multiple performance dimensions\n",
    "• Statistical significance testing between versions\n",
    "• Visual performance comparisons and heatmaps\n",
    "• Confidence intervals for key metrics\n",
    "• Actionable recommendations based on data\n",
    "• Executive-ready summary and insights\n",
    "\n",
    "🚀 USAGE:\n",
    "1. Update file paths in load_multiple_versions() function\n",
    "2. Run all cells sequentially\n",
    "3. Review comparative analysis and rankings\n",
    "4. Implement recommendations from best-performing versions\n",
    "5. Use export functions to save results for stakeholders\n",
    "\n",
    "📈 OUTPUTS:\n",
    "• Version performance rankings\n",
    "• Statistical analysis reports\n",
    "• Comprehensive visualizations\n",
    "• Actionable improvement recommendations\n",
    "• Exportable summary data and detailed results\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit ('3.11.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b649c18c59123e9cb819750d1a320031f2b93bee7a3106a6e5d9c7574eec0da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of TextGrad with TextualVerifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparative analysis for 6 different versiosn of TextGrad & TextualVerifier combination\n",
    "- Author: Eugenius Mario Situmorang\n",
    "- Date: June 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TextGrad Experiment Analysis ===\")\n",
    "print(\"Comparing 6 different configurations:\\n\")\n",
    "print(\"1. textgrad-only: Basic TextGrad optimization\")\n",
    "print(\"2. textgrad-tv-l: TextGrad + TextualVerifier for Loss only\")\n",
    "print(\"3. textgrad-tv-o: TextGrad + TextualVerifier for Optimizer\")\n",
    "print(\"6. textgrad-tv-lo: TextGrad + TextualVerifier for Loss + Optimizer\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Load all datasets\n",
    "datasets = {\n",
    "    'textgrad-only': 'results/textgrad-only.csv',\n",
    "    'textgrad-tv-l': 'results/textgrad-tv-l-1v-step.csv',\n",
    "    'textgrad-tv-o': 'results/textgrad-tv-o-1v.csv',\n",
    "    'textgrad-tv-lo': 'results/textgrad-tv-lo-1v-lstep.csv',\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for name, path in datasets.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        df['method'] = name\n",
    "        dfs[name] = df\n",
    "        print(f\"✓ Loaded {name}: {len(df)} samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ File not found: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(dfs)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRIC DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_solution(solution_text):\n",
    "    \"\"\"Extract the final answer (A, B, C, or D) from solution text\"\"\"\n",
    "    if pd.isna(solution_text) or solution_text == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Look for \"Answer: X\" pattern at the end\n",
    "    import re\n",
    "    answer_pattern = r'Answer:\\s*([ABCD])'\n",
    "    matches = re.findall(answer_pattern, str(solution_text))\n",
    "    if matches:\n",
    "        return matches[-1]  # Take the last occurrence\n",
    "    \n",
    "    # Alternative: look for just a single letter at the end\n",
    "    letter_pattern = r'\\b([ABCD])\\s*$'\n",
    "    matches = re.findall(letter_pattern, str(solution_text).strip())\n",
    "    if matches:\n",
    "        return matches[-1]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def calculate_accuracy(df):\n",
    "    \"\"\"Calculate accuracy based on correct vs final answer\"\"\"\n",
    "    if 'final_solution_answer' not in df.columns:\n",
    "        return 0.0\n",
    "    \n",
    "    # Extract answers from both columns\n",
    "    df['extracted_final'] = df['final_solution_answer'].apply(extract_answer_from_solution)\n",
    "    df['extracted_correct'] = df['correct_answer'].apply(lambda x: x.strip() if pd.notna(x) else None)\n",
    "    \n",
    "    # Count correct answers\n",
    "    correct_mask = (df['extracted_final'] == df['extracted_correct']) & df['extracted_final'].notna()\n",
    "    return correct_mask.sum() / len(df) if len(df) > 0 else 0.0\n",
    "\n",
    "def analyze_method_performance(method_df):\n",
    "    \"\"\"Analyze performance metrics for a single method\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic success metrics\n",
    "    total_samples = len(method_df)\n",
    "    successful_samples = method_df['success'].sum() if 'success' in method_df.columns else 0\n",
    "    failed_samples = total_samples - successful_samples\n",
    "    \n",
    "    metrics['total_samples'] = total_samples\n",
    "    metrics['successful_samples'] = successful_samples\n",
    "    metrics['failed_samples'] = failed_samples\n",
    "    metrics['success_rate'] = successful_samples / total_samples if total_samples > 0 else 0.0\n",
    "    \n",
    "    # Accuracy (for successful samples only)\n",
    "    if successful_samples > 0:\n",
    "        successful_df = method_df[method_df['success'] == True].copy()\n",
    "        metrics['accuracy'] = calculate_accuracy(successful_df)\n",
    "        metrics['accuracy_overall'] = calculate_accuracy(method_df)\n",
    "    else:\n",
    "        metrics['accuracy'] = 0.0\n",
    "        metrics['accuracy_overall'] = 0.0\n",
    "    \n",
    "    # Performance metrics (for successful samples)\n",
    "    if successful_samples > 0:\n",
    "        perf_df = method_df[method_df['success'] == True]\n",
    "        \n",
    "        metrics['avg_processing_time'] = perf_df['processing_time_ms'].mean()\n",
    "        metrics['median_processing_time'] = perf_df['processing_time_ms'].median()\n",
    "        metrics['std_processing_time'] = perf_df['processing_time_ms'].std()\n",
    "        \n",
    "        metrics['avg_llm_calls'] = perf_df['total_llm_calls'].mean()\n",
    "        metrics['median_llm_calls'] = perf_df['total_llm_calls'].median()\n",
    "        metrics['std_llm_calls'] = perf_df['total_llm_calls'].std()\n",
    "        \n",
    "        metrics['avg_input_tokens'] = perf_df['total_input_tokens'].mean()\n",
    "        metrics['avg_output_tokens'] = perf_df['total_output_tokens'].mean()\n",
    "        metrics['avg_total_tokens'] = metrics['avg_input_tokens'] + metrics['avg_output_tokens']\n",
    "        \n",
    "    else:\n",
    "        # Set defaults for failed methods\n",
    "        for key in ['avg_processing_time', 'median_processing_time', 'std_processing_time',\n",
    "                   'avg_llm_calls', 'median_llm_calls', 'std_llm_calls',\n",
    "                   'avg_input_tokens', 'avg_output_tokens', 'avg_total_tokens']:\n",
    "            metrics[key] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORMANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze each method overall\n",
    "method_results = {}\n",
    "for method_name, df in dfs.items():\n",
    "    print(f\"\\nAnalyzing {method_name}...\")\n",
    "    metrics = analyze_method_performance(df)\n",
    "    method_results[method_name] = metrics\n",
    "    \n",
    "    print(f\"  • Total samples: {metrics['total_samples']}\")\n",
    "    print(f\"  • Success rate: {metrics['success_rate']:.1%}\")\n",
    "    print(f\"  • Accuracy (successful only): {metrics['accuracy']:.1%}\")\n",
    "    print(f\"  • Overall accuracy: {metrics['accuracy_overall']:.1%}\")\n",
    "    if metrics['successful_samples'] > 0:\n",
    "        print(f\"  • Avg LLM calls: {metrics['avg_llm_calls']:.1f}\")\n",
    "        print(f\"  • Avg processing time: {metrics['avg_processing_time']:.1f}ms\")\n",
    "        print(f\"  • Avg total tokens: {metrics['avg_total_tokens']:.0f}\")\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame(method_results).T\n",
    "summary_df = summary_df.round(3)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df[['success_rate', 'accuracy_overall', 'avg_llm_calls', \n",
    "                  'avg_processing_time', 'avg_total_tokens']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOURCE-SPECIFIC ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SOURCE-SPECIFIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define the sources to analyze\n",
    "sources = ['GPQA-Diamond', 'MMLU-ML', 'MMLU-CP']\n",
    "\n",
    "# Analyze by source\n",
    "source_results = {}\n",
    "for source in sources:\n",
    "    print(f\"\\n--- {source} Dataset ---\")\n",
    "    source_results[source] = {}\n",
    "    \n",
    "    for method_name, df in dfs.items():\n",
    "        # Filter by source\n",
    "        source_df = df[df['source'] == source] if 'source' in df.columns else pd.DataFrame()\n",
    "        \n",
    "        if len(source_df) > 0:\n",
    "            metrics = analyze_method_performance(source_df)\n",
    "            source_results[source][method_name] = metrics\n",
    "            \n",
    "            print(f\"\\n{method_name}:\")\n",
    "            print(f\"  • Samples: {metrics['total_samples']}\")\n",
    "            print(f\"  • Success rate: {metrics['success_rate']:.1%}\")\n",
    "            print(f\"  • Overall accuracy: {metrics['accuracy_overall']:.1%}\")\n",
    "            if metrics['successful_samples'] > 0:\n",
    "                print(f\"  • Avg LLM calls: {metrics['avg_llm_calls']:.1f}\")\n",
    "                print(f\"  • Avg processing time: {metrics['avg_processing_time']:.1f}ms\")\n",
    "        else:\n",
    "            print(f\"\\n{method_name}: No samples found\")\n",
    "            source_results[source][method_name] = None\n",
    "\n",
    "# Create source-specific summary tables\n",
    "for source in sources:\n",
    "    if any(result is not None for result in source_results[source].values()):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{source} SUMMARY TABLE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        source_summary = {}\n",
    "        for method_name, metrics in source_results[source].items():\n",
    "            if metrics is not None:\n",
    "                source_summary[method_name] = metrics\n",
    "        \n",
    "        if source_summary:\n",
    "            source_df = pd.DataFrame(source_summary).T\n",
    "            source_df = source_df.round(3)\n",
    "            print(source_df[['success_rate', 'accuracy_overall', 'avg_llm_calls', \n",
    "                           'avg_processing_time', 'avg_total_tokens']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization with source-specific analysis\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# Create a grid layout: 3 rows (overall + 2 source rows), 4 columns\n",
    "gs = fig.add_gridspec(4, 4, height_ratios=[1, 1, 1, 1], hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Overall analysis (top row)\n",
    "fig.suptitle('TextGrad Method Comparison: Overall and Source-Specific Analysis', \n",
    "             fontsize=16, fontweight='bold', y=0.95)\n",
    "\n",
    "# Row 1: Overall Performance\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "methods = list(summary_df.index)\n",
    "colors = sns.color_palette(\"husl\", len(methods))\n",
    "\n",
    "# 1. Overall Success Rate\n",
    "success_rates = summary_df['success_rate'].values\n",
    "bars1 = ax1.bar(methods, success_rates, color=colors, alpha=0.8)\n",
    "ax1.set_title('Overall Success Rate', fontweight='bold')\n",
    "ax1.set_ylabel('Success Rate')\n",
    "ax1.set_ylim(0, 1.0)\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "for bar, rate in zip(bars1, success_rates):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{rate:.1%}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 2. Overall Accuracy\n",
    "accuracy_rates = summary_df['accuracy_overall'].values\n",
    "bars2 = ax2.bar(methods, accuracy_rates, color=colors, alpha=0.8)\n",
    "ax2.set_title('Overall Accuracy', fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1.0)\n",
    "ax2.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "\n",
    "for bar, acc in zip(bars2, accuracy_rates):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{acc:.1%}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 3. LLM Calls (successful methods only)\n",
    "successful_methods = []\n",
    "successful_llm_calls = []\n",
    "successful_colors = []\n",
    "\n",
    "for i, (method, metrics) in enumerate(method_results.items()):\n",
    "    if metrics['successful_samples'] > 0:\n",
    "        successful_methods.append(method)\n",
    "        successful_llm_calls.append(metrics['avg_llm_calls'])\n",
    "        successful_colors.append(colors[i])\n",
    "\n",
    "if successful_methods:\n",
    "    bars3 = ax3.bar(successful_methods, successful_llm_calls, color=successful_colors, alpha=0.8)\n",
    "    ax3.set_title('Avg LLM Calls (Successful)', fontweight='bold')\n",
    "    ax3.set_ylabel('LLM Calls')\n",
    "    ax3.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "    \n",
    "    for bar, calls in zip(bars3, successful_llm_calls):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                 f'{calls:.1f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# 4. Efficiency Score\n",
    "if successful_methods:\n",
    "    efficiency_scores = []\n",
    "    efficiency_labels = []\n",
    "    efficiency_colors = []\n",
    "    \n",
    "    for i, method in enumerate(successful_methods):\n",
    "        metrics = method_results[method]\n",
    "        if metrics['avg_llm_calls'] > 0:\n",
    "            efficiency = metrics['accuracy_overall'] / metrics['avg_llm_calls']\n",
    "            efficiency_scores.append(efficiency)\n",
    "            efficiency_labels.append(method)\n",
    "            efficiency_colors.append(successful_colors[i])\n",
    "    \n",
    "    if efficiency_scores:\n",
    "        bars4 = ax4.bar(efficiency_labels, efficiency_scores, color=efficiency_colors, alpha=0.8)\n",
    "        ax4.set_title('Efficiency (Accuracy/LLM Calls)', fontweight='bold')\n",
    "        ax4.set_ylabel('Efficiency Score')\n",
    "        ax4.tick_params(axis='x', rotation=45, labelsize=8)\n",
    "        \n",
    "        for bar, eff in zip(bars4, efficiency_scores):\n",
    "            height = bar.get_height()\n",
    "            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                     f'{eff:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Source-specific analysis (rows 2-4)\n",
    "sources = ['GPQA-Diamond', 'MMLU-ML', 'MMLU-CP']\n",
    "source_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for row_idx, source in enumerate(sources, start=1):\n",
    "    # Extract data for this source\n",
    "    source_methods = []\n",
    "    source_success_rates = []\n",
    "    source_accuracies = []\n",
    "    source_llm_calls = []\n",
    "    source_efficiencies = []\n",
    "    \n",
    "    if source in source_results:\n",
    "        for method_name, metrics in source_results[source].items():\n",
    "            if metrics is not None and metrics['total_samples'] > 0:\n",
    "                source_methods.append(method_name)\n",
    "                source_success_rates.append(metrics['success_rate'])\n",
    "                source_accuracies.append(metrics['accuracy_overall'])\n",
    "                \n",
    "                if metrics['successful_samples'] > 0:\n",
    "                    source_llm_calls.append(metrics['avg_llm_calls'])\n",
    "                    if metrics['avg_llm_calls'] > 0:\n",
    "                        source_efficiencies.append(metrics['accuracy_overall'] / metrics['avg_llm_calls'])\n",
    "                    else:\n",
    "                        source_efficiencies.append(0)\n",
    "                else:\n",
    "                    source_llm_calls.append(0)\n",
    "                    source_efficiencies.append(0)\n",
    "    \n",
    "    if source_methods:\n",
    "        # Success Rate for this source\n",
    "        ax_success = fig.add_subplot(gs[row_idx, 0])\n",
    "        bars = ax_success.bar(source_methods, source_success_rates, \n",
    "                             color=source_colors[row_idx-1], alpha=0.8)\n",
    "        ax_success.set_title(f'{source}\\nSuccess Rate', fontweight='bold', fontsize=10)\n",
    "        ax_success.set_ylabel('Success Rate', fontsize=9)\n",
    "        ax_success.set_ylim(0, 1.0)\n",
    "        ax_success.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "        \n",
    "        for bar, rate in zip(bars, source_success_rates):\n",
    "            height = bar.get_height()\n",
    "            ax_success.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                           f'{rate:.1%}', ha='center', va='bottom', fontsize=7)\n",
    "        \n",
    "        # Accuracy for this source\n",
    "        ax_acc = fig.add_subplot(gs[row_idx, 1])\n",
    "        bars = ax_acc.bar(source_methods, source_accuracies, \n",
    "                         color=source_colors[row_idx-1], alpha=0.8)\n",
    "        ax_acc.set_title(f'{source}\\nAccuracy', fontweight='bold', fontsize=10)\n",
    "        ax_acc.set_ylabel('Accuracy', fontsize=9)\n",
    "        ax_acc.set_ylim(0, 1.0)\n",
    "        ax_acc.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "        \n",
    "        for bar, acc in zip(bars, source_accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax_acc.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{acc:.1%}', ha='center', va='bottom', fontsize=7)\n",
    "        \n",
    "        # LLM Calls for this source (only successful methods)\n",
    "        ax_llm = fig.add_subplot(gs[row_idx, 2])\n",
    "        successful_source_methods = [m for i, m in enumerate(source_methods) if source_llm_calls[i] > 0]\n",
    "        successful_source_calls = [c for c in source_llm_calls if c > 0]\n",
    "        \n",
    "        if successful_source_methods:\n",
    "            bars = ax_llm.bar(successful_source_methods, successful_source_calls, \n",
    "                             color=source_colors[row_idx-1], alpha=0.8)\n",
    "            ax_llm.set_title(f'{source}\\nLLM Calls', fontweight='bold', fontsize=10)\n",
    "            ax_llm.set_ylabel('LLM Calls', fontsize=9)\n",
    "            ax_llm.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "            \n",
    "            for bar, calls in zip(bars, successful_source_calls):\n",
    "                height = bar.get_height()\n",
    "                ax_llm.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                           f'{calls:.1f}', ha='center', va='bottom', fontsize=7)\n",
    "        else:\n",
    "            ax_llm.text(0.5, 0.5, 'No successful\\nsamples', ha='center', va='center', \n",
    "                       transform=ax_llm.transAxes, fontsize=9)\n",
    "            ax_llm.set_title(f'{source}\\nLLM Calls', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Efficiency for this source\n",
    "        ax_eff = fig.add_subplot(gs[row_idx, 3])\n",
    "        efficient_methods = [m for i, m in enumerate(source_methods) if source_efficiencies[i] > 0]\n",
    "        efficient_scores = [e for e in source_efficiencies if e > 0]\n",
    "        \n",
    "        if efficient_methods:\n",
    "            bars = ax_eff.bar(efficient_methods, efficient_scores, \n",
    "                             color=source_colors[row_idx-1], alpha=0.8)\n",
    "            ax_eff.set_title(f'{source}\\nEfficiency', fontweight='bold', fontsize=10)\n",
    "            ax_eff.set_ylabel('Efficiency Score', fontsize=9)\n",
    "            ax_eff.tick_params(axis='x', rotation=45, labelsize=7)\n",
    "            \n",
    "            for bar, eff in zip(bars, efficient_scores):\n",
    "                height = bar.get_height()\n",
    "                ax_eff.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                           f'{eff:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "        else:\n",
    "            ax_eff.text(0.5, 0.5, 'No efficiency\\ndata', ha='center', va='center', \n",
    "                       transform=ax_eff.transAxes, fontsize=9)\n",
    "            ax_eff.set_title(f'{source}\\nEfficiency', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETAILED METRICS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED METRICS TABLE - OVERALL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "detailed_metrics = []\n",
    "for method_name, metrics in method_results.items():\n",
    "    row = {\n",
    "        'Method': method_name,\n",
    "        'Total Samples': metrics['total_samples'],\n",
    "        'Success Rate': f\"{metrics['success_rate']:.1%}\",\n",
    "        'Overall Accuracy': f\"{metrics['accuracy_overall']:.1%}\",\n",
    "        'Accuracy (Success Only)': f\"{metrics['accuracy']:.1%}\",\n",
    "        'Avg LLM Calls': f\"{metrics['avg_llm_calls']:.1f}\",\n",
    "        'Median LLM Calls': f\"{metrics['median_llm_calls']:.1f}\",\n",
    "        'Avg Processing Time (ms)': f\"{metrics['avg_processing_time']:.1f}\",\n",
    "        'Avg Input Tokens': f\"{metrics['avg_input_tokens']:.0f}\",\n",
    "        'Avg Output Tokens': f\"{metrics['avg_output_tokens']:.0f}\",\n",
    "        'Avg Total Tokens': f\"{metrics['avg_total_tokens']:.0f}\",\n",
    "    }\n",
    "    detailed_metrics.append(row)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_metrics)\n",
    "print(detailed_df.to_string(index=False))\n",
    "\n",
    "# Source-specific detailed tables\n",
    "for source in sources:\n",
    "    if source in source_results and any(result is not None for result in source_results[source].values()):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"DETAILED METRICS TABLE - {source}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        source_detailed_metrics = []\n",
    "        for method_name, metrics in source_results[source].items():\n",
    "            if metrics is not None:\n",
    "                row = {\n",
    "                    'Method': method_name,\n",
    "                    'Total Samples': metrics['total_samples'],\n",
    "                    'Success Rate': f\"{metrics['success_rate']:.1%}\",\n",
    "                    'Overall Accuracy': f\"{metrics['accuracy_overall']:.1%}\",\n",
    "                    'Accuracy (Success Only)': f\"{metrics['accuracy']:.1%}\",\n",
    "                    'Avg LLM Calls': f\"{metrics['avg_llm_calls']:.1f}\",\n",
    "                    'Avg Processing Time (ms)': f\"{metrics['avg_processing_time']:.1f}\",\n",
    "                    'Avg Total Tokens': f\"{metrics['avg_total_tokens']:.0f}\",\n",
    "                }\n",
    "                source_detailed_metrics.append(row)\n",
    "        \n",
    "        if source_detailed_metrics:\n",
    "            source_detailed_df = pd.DataFrame(source_detailed_metrics)\n",
    "            print(source_detailed_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best performing methods overall\n",
    "best_success_rate = max(method_results.items(), key=lambda x: x[1]['success_rate'])\n",
    "best_accuracy = max(method_results.items(), key=lambda x: x[1]['accuracy_overall'])\n",
    "\n",
    "successful_methods_results = {k: v for k, v in method_results.items() if v['successful_samples'] > 0}\n",
    "if successful_methods_results:\n",
    "    best_efficiency = min(successful_methods_results.items(), \n",
    "                         key=lambda x: x[1]['avg_llm_calls'] if x[1]['avg_llm_calls'] > 0 else float('inf'))\n",
    "    best_speed = min(successful_methods_results.items(), \n",
    "                    key=lambda x: x[1]['avg_processing_time'])\n",
    "\n",
    "print(f\"\\n🏆 OVERALL BEST PERFORMERS:\")\n",
    "print(f\"  • Highest Success Rate: {best_success_rate[0]} ({best_success_rate[1]['success_rate']:.1%})\")\n",
    "print(f\"  • Highest Overall Accuracy: {best_accuracy[0]} ({best_accuracy[1]['accuracy_overall']:.1%})\")\n",
    "\n",
    "if successful_methods_results:\n",
    "    print(f\"  • Most Efficient (fewest LLM calls): {best_efficiency[0]} ({best_efficiency[1]['avg_llm_calls']:.1f} calls)\")\n",
    "    print(f\"  • Fastest Processing: {best_speed[0]} ({best_speed[1]['avg_processing_time']:.1f}ms)\")\n",
    "\n",
    "# Find best performing methods per dataset source\n",
    "print(f\"\\n🎯 BEST PERFORMERS BY DATASET:\")\n",
    "for source in sources:\n",
    "    if source in source_results and any(result is not None for result in source_results[source].values()):\n",
    "        print(f\"\\n--- {source} ---\")\n",
    "        \n",
    "        # Find best methods for this source\n",
    "        source_methods = {k: v for k, v in source_results[source].items() if v is not None}\n",
    "        \n",
    "        if source_methods:\n",
    "            best_source_success = max(source_methods.items(), key=lambda x: x[1]['success_rate'])\n",
    "            best_source_accuracy = max(source_methods.items(), key=lambda x: x[1]['accuracy_overall'])\n",
    "            \n",
    "            successful_source_methods = {k: v for k, v in source_methods.items() if v['successful_samples'] > 0}\n",
    "            \n",
    "            print(f\"  • Highest Success Rate: {best_source_success[0]} ({best_source_success[1]['success_rate']:.1%})\")\n",
    "            print(f\"  • Highest Accuracy: {best_source_accuracy[0]} ({best_source_accuracy[1]['accuracy_overall']:.1%})\")\n",
    "            \n",
    "            if successful_source_methods:\n",
    "                best_source_efficiency = min(successful_source_methods.items(), \n",
    "                                           key=lambda x: x[1]['avg_llm_calls'] if x[1]['avg_llm_calls'] > 0 else float('inf'))\n",
    "                best_source_speed = min(successful_source_methods.items(), \n",
    "                                      key=lambda x: x[1]['avg_processing_time'])\n",
    "                \n",
    "                print(f\"  • Most Efficient: {best_source_efficiency[0]} ({best_source_efficiency[1]['avg_llm_calls']:.1f} calls)\")\n",
    "                print(f\"  • Fastest: {best_source_speed[0]} ({best_source_speed[1]['avg_processing_time']:.1f}ms)\")\n",
    "                \n",
    "                # Calculate efficiency score for this source\n",
    "                efficiency_scores = {}\n",
    "                for method, metrics in successful_source_methods.items():\n",
    "                    if metrics['avg_llm_calls'] > 0:\n",
    "                        efficiency_scores[method] = metrics['accuracy_overall'] / metrics['avg_llm_calls']\n",
    "                \n",
    "                if efficiency_scores:\n",
    "                    most_efficient_overall = max(efficiency_scores.items(), key=lambda x: x[1])\n",
    "                    print(f\"  • Best Efficiency Score: {most_efficient_overall[0]} ({most_efficient_overall[1]:.3f})\")\n",
    "\n",
    "print(f\"\\n📊 KEY INSIGHTS:\")\n",
    "\n",
    "# Analyze TextualVerifier impact\n",
    "tv_methods = [m for m in method_results.keys() if 'tv' in m and method_results[m]['successful_samples'] > 0]\n",
    "non_tv_methods = [m for m in method_results.keys() if 'tv' not in m and method_results[m]['successful_samples'] > 0]\n",
    "\n",
    "if tv_methods and non_tv_methods:\n",
    "    avg_accuracy_tv = np.mean([method_results[m]['accuracy_overall'] for m in tv_methods])\n",
    "    avg_accuracy_non_tv = np.mean([method_results[m]['accuracy_overall'] for m in non_tv_methods])\n",
    "    \n",
    "    avg_calls_tv = np.mean([method_results[m]['avg_llm_calls'] for m in tv_methods])\n",
    "    avg_calls_non_tv = np.mean([method_results[m]['avg_llm_calls'] for m in non_tv_methods])\n",
    "    \n",
    "    print(f\"  • TextualVerifier Impact:\")\n",
    "    print(f\"    - Accuracy: TV methods {avg_accuracy_tv:.1%} vs Non-TV {avg_accuracy_non_tv:.1%}\")\n",
    "    print(f\"    - LLM Calls: TV methods {avg_calls_tv:.1f} vs Non-TV {avg_calls_non_tv:.1f}\")\n",
    "\n",
    "# Analyze component-specific TextualVerifier impact\n",
    "loss_only_methods = [m for m in method_results.keys() if 'tv-l' in m and method_results[m]['successful_samples'] > 0]\n",
    "optimizer_methods = [m for m in method_results.keys() if 'tv-o' in m and method_results[m]['successful_samples'] > 0]\n",
    "combined_methods = [m for m in method_results.keys() if 'tv-lo' in m and method_results[m]['successful_samples'] > 0]\n",
    "\n",
    "if loss_only_methods:\n",
    "    avg_acc_loss = np.mean([method_results[m]['accuracy_overall'] for m in loss_only_methods])\n",
    "    print(f\"  • Loss-only TextualVerifier: {avg_acc_loss:.1%} accuracy\")\n",
    "\n",
    "if optimizer_methods:\n",
    "    avg_acc_opt = np.mean([method_results[m]['accuracy_overall'] for m in optimizer_methods])\n",
    "    print(f\"  • Optimizer-only TextualVerifier: {avg_acc_opt:.1%} accuracy\")\n",
    "\n",
    "if combined_methods:\n",
    "    avg_acc_combined = np.mean([method_results[m]['accuracy_overall'] for m in combined_methods])\n",
    "    print(f\"  • Combined Loss+Optimizer TextualVerifier: {avg_acc_combined:.1%} accuracy\")\n",
    "\n",
    "# Analyze variant impact\n",
    "variant_1_methods = [m for m in method_results.keys() if '1v' in m and method_results[m]['successful_samples'] > 0]\n",
    "variant_3_methods = [m for m in method_results.keys() if '3v' in m and method_results[m]['successful_samples'] > 0]\n",
    "\n",
    "if variant_1_methods and variant_3_methods:\n",
    "    avg_accuracy_1v = np.mean([method_results[m]['accuracy_overall'] for m in variant_1_methods])\n",
    "    avg_accuracy_3v = np.mean([method_results[m]['accuracy_overall'] for m in variant_3_methods])\n",
    "    \n",
    "    print(f\"  • Multi-variant Impact:\")\n",
    "    print(f\"    - 1 variant: {avg_accuracy_1v:.1%} accuracy\")\n",
    "    print(f\"    - 3 variants: {avg_accuracy_3v:.1%} accuracy\")\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(f\"  1. For highest success rate: Use {best_success_rate[0]}\")\n",
    "print(f\"  2. For best accuracy: Use {best_accuracy[0]}\")\n",
    "if successful_methods_results:\n",
    "    print(f\"  3. For efficiency: Use {best_efficiency[0]} (good accuracy with fewer LLM calls)\")\n",
    "    print(f\"  4. For speed: Use {best_speed[0]} (fastest processing)\")\n",
    "\n",
    "print(f\"\\n🔍 TEXTUALVERIFIER ANALYSIS:\")\n",
    "if tv_methods and non_tv_methods:\n",
    "    improvement = avg_accuracy_tv - avg_accuracy_non_tv\n",
    "    cost_increase = avg_calls_tv - avg_calls_non_tv\n",
    "    print(f\"  • TextualVerifier provides {improvement:+.1%} accuracy improvement\")\n",
    "    print(f\"  • At cost of {cost_increase:+.1f} additional LLM calls per sample\")\n",
    "    print(f\"  • ROI: {improvement/cost_increase:.3f} accuracy points per additional call\" if cost_increase > 0 else \"\")\n",
    "\n",
    "print(f\"\\n⚠️  NOTE: Methods with 0% success rate likely had implementation issues\")\n",
    "print(f\"    and should be debugged before production use.\")\n",
    "\n",
    "print(f\"\\n📋 METHODOLOGY INSIGHTS:\")\n",
    "print(f\"  • TextualVerifier (TV) adds verification layers to TextGrad optimization\")\n",
    "print(f\"  • 'L' variants verify loss calculations\")\n",
    "print(f\"  • 'O' variants verify optimizer steps\") \n",
    "print(f\"  • 'LO' variants verify both loss and optimizer\")\n",
    "print(f\"  • Multiple variants (1v vs 3v) test robustness through repetition\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit ('3.11.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b649c18c59123e9cb819750d1a320031f2b93bee7a3106a6e5d9c7574eec0da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

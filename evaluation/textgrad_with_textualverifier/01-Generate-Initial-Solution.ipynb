{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Initial Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textgrad as tg\n",
    "from textgrad.engine import get_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "- GPQA Diamond\n",
    "- MMLU Machine Learning\n",
    "- MMLU College Physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "dataset_gpqa = pd.read_csv(\"hf://datasets/Idavidrein/gpqa/gpqa_diamond.csv\")\n",
    "dataset_mmlu_ml = pd.read_parquet(\"hf://datasets/cais/mmlu/machine_learning/test-00000-of-00001.parquet\")\n",
    "dataset_mmlu_cp = pd.read_parquet(\"hf://datasets/cais/mmlu/college_physics/test-00000-of-00001.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gpqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mmlu_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mmlu_cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GeneralDatasets & Result Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame with proper GeneralDatasets schema\n",
    "general_datasets_columns = [\n",
    "    'id',                    # integer - unique identifier\n",
    "    'question',              # string - the question text\n",
    "    'correct_answer',        # string - correct answer\n",
    "    'incorrect_answer_1',    # string - first incorrect option\n",
    "    'incorrect_answer_2',    # string - second incorrect option  \n",
    "    'incorrect_answer_3',    # string - third incorrect option\n",
    "    'source',               # string - dataset source (GPQA, MMLU, etc.)\n",
    "    'subject'               # string - subject/topic area\n",
    "]\n",
    "general_datasets = pd.DataFrame(columns=general_datasets_columns)\n",
    "general_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame with proper Result schema\n",
    "result_columns = [\n",
    "    'id',                # integer - links to GeneralDatasets.id\n",
    "    'raw_solution',      # string - initial solution before optimization\n",
    "    'solution_1',        # string - first TextGrad iteration\n",
    "    'solution_2',        # string - second TextGrad iteration  \n",
    "    'solution_3',        # string - third TextGrad iteration\n",
    "    'solution_4',        # string - fourth TextGrad iteration\n",
    "    'solution_5',        # string - fifth TextGrad iteration (final)\n",
    "    'correct_answer',    # string - ground truth answer\n",
    "    'source',           # string - dataset source (GPQA, MMLU)\n",
    "    'subject'           # string - subject area\n",
    "]\n",
    "result = pd.DataFrame(columns=result_columns)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset general_datasets DataFrame\n",
    "general_datasets = pd.DataFrame(columns=general_datasets_columns)\n",
    "\n",
    "i = 1\n",
    "# GPQA Diamond\n",
    "for index, row in dataset_gpqa.iterrows():\n",
    "    general_datasets.loc[len(general_datasets)] = {\n",
    "        \"id\": i,\n",
    "        \"question\": row[\"Question\"],\n",
    "        \"correct_answer\": row[\"Correct Answer\"],\n",
    "        \"incorrect_answer_1\": row[\"Incorrect Answer 1\"],\n",
    "        \"incorrect_answer_2\": row[\"Incorrect Answer 2\"],\n",
    "        \"incorrect_answer_3\": row[\"Incorrect Answer 3\"],\n",
    "        \"source\": \"GPQA-Diamond\",\n",
    "        \"subject\": \"-\"\n",
    "    }\n",
    "    i += 1\n",
    "\n",
    "# MMLU Machine Learning\n",
    "for index, row in dataset_mmlu_ml.iterrows():\n",
    "    available_choices = [0, 1, 2, 3]\n",
    "    available_choices.remove(row[\"answer\"])\n",
    "    general_datasets.loc[len(general_datasets)] = {\n",
    "        \"id\": i,\n",
    "        \"question\": row[\"question\"],\n",
    "        \"correct_answer\": row[\"choices\"][row[\"answer\"]],\n",
    "        \"incorrect_answer_1\": row[\"choices\"][available_choices[0]],\n",
    "        \"incorrect_answer_2\": row[\"choices\"][available_choices[1]],\n",
    "        \"incorrect_answer_3\": row[\"choices\"][available_choices[2]],\n",
    "        \"source\": \"MMLU-ML\",\n",
    "        \"subject\": row[\"subject\"]\n",
    "    }\n",
    "    i += 1\n",
    "\n",
    "# MMLU College Physics\n",
    "for index, row in dataset_mmlu_cp.iterrows():\n",
    "    available_choices = [0, 1, 2, 3]\n",
    "    available_choices.remove(row[\"answer\"])\n",
    "    general_datasets.loc[len(general_datasets)] = {\n",
    "        \"id\": i,\n",
    "        \"question\": row[\"question\"],\n",
    "        \"correct_answer\": row[\"choices\"][row[\"answer\"]],\n",
    "        \"incorrect_answer_1\": row[\"choices\"][available_choices[0]],\n",
    "        \"incorrect_answer_2\": row[\"choices\"][available_choices[1]],\n",
    "        \"incorrect_answer_3\": row[\"choices\"][available_choices[2]],\n",
    "        \"source\": \"MMLU-CP\",\n",
    "        \"subject\": row[\"subject\"]\n",
    "    }\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Experiment Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine(\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "QUERY_TEMPLATE_MULTICHOICE = \"\"\"\n",
    "Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\n",
    "\n",
    "{Question}\n",
    "\n",
    "A) {A}\n",
    "B) {B}\n",
    "C) {C}\n",
    "D) {D}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Generate datasets initial_solution.csv\n",
    "def generate_initial_solution(\n",
    "    question, \n",
    "    correct_answer, \n",
    "    incorrect_answer_1, \n",
    "    incorrect_answer_2, \n",
    "    incorrect_answer_3\n",
    "    ):\n",
    "    \n",
    "    # Randomize\n",
    "    answers = [correct_answer, incorrect_answer_1, incorrect_answer_2, incorrect_answer_3]\n",
    "    letter = ['A', 'B', 'C', 'D']\n",
    "    correct_index = random.randint(0,3)\n",
    "    for i in range(correct_index):\n",
    "        answers[i] = answers[i+1]\n",
    "        answers[i+1] = correct_answer\n",
    "        letter.pop(0)\n",
    "\n",
    "    choices_dict = dict(\n",
    "        A=answers[0], B=answers[1], C=answers[2], D=answers[3], Question=question\n",
    "    ) \n",
    "\n",
    "    correct_answer_letter = letter[0]\n",
    "    formatted_question = QUERY_TEMPLATE_MULTICHOICE.format(**choices_dict)\n",
    "\n",
    "    STARTING_SYSTEM_PROMPT = f\"\"\"\n",
    "        You are Gemini, a large language model trained by Google, based on the Gemini-1.5-Pro architecture.\n",
    "        \\nKnowledge cutoff: 2024-12\\nCurrent date: 2025-05-01\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT, requires_grad=False, role_description=\"System prompt to the language model\")\n",
    "    model = tg.BlackboxLLM(engine, system_prompt)\n",
    "    initial_solution = model(tg.Variable(formatted_question, requires_grad=False, role_description=\"Question to the language model\"))\n",
    "    return formatted_question, initial_solution, correct_answer_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_single_row(row_data):\n",
    "    \"\"\"Process a single row for multithreading\"\"\"\n",
    "    try:\n",
    "        formatted_question, solution, correct_answer_letter = generate_initial_solution(\n",
    "            question=row_data['question'],\n",
    "            correct_answer=row_data['correct_answer'],\n",
    "            incorrect_answer_1=row_data['incorrect_answer_1'],\n",
    "            incorrect_answer_2=row_data['incorrect_answer_2'],\n",
    "            incorrect_answer_3=row_data['incorrect_answer_3']\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"id\": row_data[\"id\"],\n",
    "            \"formatted_question\": formatted_question,\n",
    "            \"raw_solution\": solution,\n",
    "            \"correct_answer\": correct_answer_letter,\n",
    "            \"source\": row_data[\"source\"],\n",
    "            \"subject\": row_data[\"subject\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing problem {row_data['id']}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreaded processing\n",
    "all_rows = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [\n",
    "        executor.submit(process_single_row, row.to_dict()) \n",
    "        for _, row in general_datasets.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Collect results\n",
    "    completed = 0\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            all_rows.append(result)\n",
    "        \n",
    "        completed += 1\n",
    "        if completed % 10 == 0:\n",
    "            print(f\"Progress: {completed}/{len(futures)} ({completed/len(futures)*100:.1f}%)\")\n",
    "\n",
    "initial_solution = pd.DataFrame(all_rows)\n",
    "initial_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution.to_csv('datasets/initial_solution.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "860e0dc2175a55dd9a80ac360791d93c13f4935a3c9aca3a9a76262c7d69eace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

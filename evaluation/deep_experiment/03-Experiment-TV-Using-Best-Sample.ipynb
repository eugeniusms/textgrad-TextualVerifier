{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment TextualVerifier Using Best Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import time\n",
    "import json\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from threading import Lock\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import textgrad as tg\n",
    "from textgrad.engine import get_engine\n",
    "from textgrad.variable import Variable\n",
    "from textgrad.verifier import TextualVerifierExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labeler</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>problem</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>total_steps</th>\n",
       "      <th>steps</th>\n",
       "      <th>neg_1</th>\n",
       "      <th>zero</th>\n",
       "      <th>pos_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-30T14:37:13.296218</td>\n",
       "      <td>There are an infinite number of vectors $\\math...</td>\n",
       "      <td>\\begin{pmatrix} -7 \\\\ 16 \\\\ 5 \\end{pmatrix}</td>\n",
       "      <td>34</td>\n",
       "      <td>[{'text': \"Let's set $\\\\mathbf{v} = \\\\begin{pm...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-30T13:26:58.414691</td>\n",
       "      <td>When rolling a certain unfair six-sided die wi...</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>[{'text': \"Well, let's think about this for a ...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-31T14:39:30.588403</td>\n",
       "      <td>Find all solutions to\\n\\[\\sin \\left( \\tan^{-1}...</td>\n",
       "      <td>3 \\pm 2 \\sqrt{2}</td>\n",
       "      <td>34</td>\n",
       "      <td>[{'text': \"Let's set $y = \\\\tan^{-1} x$.\", 'ra...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-29T07:48:01.714041</td>\n",
       "      <td>The solutions of the equation $z^4+4z^3i-6z^2-...</td>\n",
       "      <td>11</td>\n",
       "      <td>40</td>\n",
       "      <td>[{'text': 'There is a formula for the area of ...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>e90a38f3-3135-4465-87af-3e6322e3d772</td>\n",
       "      <td>2022-07-22T20:02:50.866783</td>\n",
       "      <td>A sequence $(a_n)$ is defined as follows:\\n\\[a...</td>\n",
       "      <td>-1</td>\n",
       "      <td>36</td>\n",
       "      <td>[{'text': \"So we're given that $a_{i + 1} = \\\\...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>440</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-28T08:12:20.344377</td>\n",
       "      <td>Find the product $CD$ of the integers $C$ and ...</td>\n",
       "      <td>-5</td>\n",
       "      <td>17</td>\n",
       "      <td>[{'text': 'I think the first step here is to f...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>442</td>\n",
       "      <td>d8aa7923-b970-45e1-9734-e4a7f6c4a7db</td>\n",
       "      <td>2022-07-31T22:47:06.498122</td>\n",
       "      <td>What real values of $x$ are not in the domain ...</td>\n",
       "      <td>-4</td>\n",
       "      <td>31</td>\n",
       "      <td>[{'text': 'To find values of $x$ that are not ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>444</td>\n",
       "      <td>d8aa7923-b970-45e1-9734-e4a7f6c4a7db</td>\n",
       "      <td>2022-07-24T10:40:50.685197</td>\n",
       "      <td>How many license plates can be formed if every...</td>\n",
       "      <td>58,500</td>\n",
       "      <td>14</td>\n",
       "      <td>[{'text': 'So we need to count the number of p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>445</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-30T11:25:46.657657</td>\n",
       "      <td>If $f(x)=5x^2+3x+4$, what is the value of $f(-...</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>[{'text': 'To find f(-2), we just need to plug...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>446</td>\n",
       "      <td>debabc6d-f79c-4ee5-a9db-5e284390254c</td>\n",
       "      <td>2022-07-28T07:53:49.057201</td>\n",
       "      <td>Evaluate $\\log_264$.</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>[{'text': \"What does it mean when there's a sm...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                               labeler                   timestamp  \\\n",
       "0     1  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-30T14:37:13.296218   \n",
       "1     2  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-30T13:26:58.414691   \n",
       "2     3  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-31T14:39:30.588403   \n",
       "3     4  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-29T07:48:01.714041   \n",
       "4     5  e90a38f3-3135-4465-87af-3e6322e3d772  2022-07-22T20:02:50.866783   \n",
       "..  ...                                   ...                         ...   \n",
       "66  440  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-28T08:12:20.344377   \n",
       "67  442  d8aa7923-b970-45e1-9734-e4a7f6c4a7db  2022-07-31T22:47:06.498122   \n",
       "68  444  d8aa7923-b970-45e1-9734-e4a7f6c4a7db  2022-07-24T10:40:50.685197   \n",
       "69  445  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-30T11:25:46.657657   \n",
       "70  446  debabc6d-f79c-4ee5-a9db-5e284390254c  2022-07-28T07:53:49.057201   \n",
       "\n",
       "                                              problem  \\\n",
       "0   There are an infinite number of vectors $\\math...   \n",
       "1   When rolling a certain unfair six-sided die wi...   \n",
       "2   Find all solutions to\\n\\[\\sin \\left( \\tan^{-1}...   \n",
       "3   The solutions of the equation $z^4+4z^3i-6z^2-...   \n",
       "4   A sequence $(a_n)$ is defined as follows:\\n\\[a...   \n",
       "..                                                ...   \n",
       "66  Find the product $CD$ of the integers $C$ and ...   \n",
       "67  What real values of $x$ are not in the domain ...   \n",
       "68  How many license plates can be formed if every...   \n",
       "69  If $f(x)=5x^2+3x+4$, what is the value of $f(-...   \n",
       "70                               Evaluate $\\log_264$.   \n",
       "\n",
       "                            ground_truth_answer  total_steps  \\\n",
       "0   \\begin{pmatrix} -7 \\\\ 16 \\\\ 5 \\end{pmatrix}           34   \n",
       "1                                            29           35   \n",
       "2                              3 \\pm 2 \\sqrt{2}           34   \n",
       "3                                            11           40   \n",
       "4                                            -1           36   \n",
       "..                                          ...          ...   \n",
       "66                                           -5           17   \n",
       "67                                           -4           31   \n",
       "68                                       58,500           14   \n",
       "69                                           18            7   \n",
       "70                                            6            5   \n",
       "\n",
       "                                                steps  neg_1  zero  pos_1  \n",
       "0   [{'text': \"Let's set $\\\\mathbf{v} = \\\\begin{pm...     19     6      9  \n",
       "1   [{'text': \"Well, let's think about this for a ...     18     1     16  \n",
       "2   [{'text': \"Let's set $y = \\\\tan^{-1} x$.\", 'ra...     11     1     22  \n",
       "3   [{'text': 'There is a formula for the area of ...     16     2     21  \n",
       "4   [{'text': \"So we're given that $a_{i + 1} = \\\\...      7     3     26  \n",
       "..                                                ...    ...   ...    ...  \n",
       "66  [{'text': 'I think the first step here is to f...      3     0     14  \n",
       "67  [{'text': 'To find values of $x$ that are not ...      1     0     30  \n",
       "68  [{'text': 'So we need to count the number of p...      2     2     10  \n",
       "69  [{'text': 'To find f(-2), we just need to plug...      1     0      6  \n",
       "70  [{'text': \"What does it mean when there's a sm...      3     0      2  \n",
       "\n",
       "[71 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"dataset/sample/prm800k-03-algo3-clean.csv\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugeniusms/Development/SKRIPSI/sevet/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "engine = get_engine(\"gemini-1.5-pro\")\n",
    "tg.set_backward_engine(\"gemini-1.5-pro\", override=True)\n",
    "model_name=\"gemini-1.5-pro\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: No specific tokenizer for gemini-1.5-pro, using fallback\n",
      "âœ“ Tracker initialized successfully for model: gemini-1.5-pro\n",
      "Test token count: 9 tokens for: 'This is a test sentence for token counting.'\n",
      "âœ“ Robust tracker ready for use!\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class LLMCallMetrics:\n",
    "    \"\"\"Track individual LLM calls\"\"\"\n",
    "    call_id: str\n",
    "    stage: str  # 'variant_generation', 'voting', 'decision'\n",
    "    step_index: int\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_ms: float\n",
    "    timestamp: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class StepMetrics:\n",
    "    \"\"\"Track metrics for each step verification\"\"\"\n",
    "    step_index: int\n",
    "    original_step: str\n",
    "    variants_generated: int\n",
    "    variants_successful: int\n",
    "    selected_variant: str\n",
    "    selection_reason: str\n",
    "    step_processing_time_ms: float\n",
    "    llm_calls: List[LLMCallMetrics]\n",
    "\n",
    "@dataclass\n",
    "class ExperimentMetrics:\n",
    "    \"\"\"Comprehensive experiment tracking\"\"\"\n",
    "    # Basic Info\n",
    "    experiment_id: str\n",
    "    problem_id: str\n",
    "    problem_text: str\n",
    "    timestamp: float\n",
    "    \n",
    "    # Input/Output\n",
    "    original_solution: str\n",
    "    verified_solution: str\n",
    "    final_decision: str  # 'REPLACE' or 'SUFFICIENT'\n",
    "    \n",
    "    # Performance Metrics\n",
    "    total_processing_time_ms: float\n",
    "    total_llm_calls: int\n",
    "    total_tokens: int\n",
    "    total_prompt_tokens: int\n",
    "    total_completion_tokens: int\n",
    "    \n",
    "    # Step-by-Step Metrics\n",
    "    total_steps: int\n",
    "    steps_modified: int\n",
    "    step_metrics: List[StepMetrics]\n",
    "    \n",
    "    # Quality Metrics\n",
    "    original_has_errors: bool\n",
    "    verified_fixes_errors: bool\n",
    "    improvement_score: float  # -1 to 1, where 1 is significant improvement\n",
    "    \n",
    "    # Cost Estimation (if using paid APIs)\n",
    "    estimated_cost_usd: float\n",
    "    \n",
    "    # Detailed Logs\n",
    "    stage_outputs: Dict[str, Any]\n",
    "    error_log: List[str]\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Robust tracking for textual verifier experiments - supports all model types\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt-3.5-turbo\", enable_token_counting: bool = True):\n",
    "        self.model_name = model_name\n",
    "        self.enable_token_counting = enable_token_counting\n",
    "        self.current_experiment: Optional[ExperimentMetrics] = None\n",
    "        self.all_experiments: List[ExperimentMetrics] = []\n",
    "        \n",
    "        # Initialize token counter with fallback support\n",
    "        self.encoding = None\n",
    "        if enable_token_counting:\n",
    "            self.encoding = self._get_safe_encoding(model_name)\n",
    "        \n",
    "        # Enhanced token pricing with more models\n",
    "        self.token_prices = {\n",
    "            # OpenAI models\n",
    "            \"gpt-3.5-turbo\": {\"prompt\": 0.001/1000, \"completion\": 0.002/1000},\n",
    "            \"gpt-4\": {\"prompt\": 0.03/1000, \"completion\": 0.06/1000},\n",
    "            \"gpt-4-turbo\": {\"prompt\": 0.01/1000, \"completion\": 0.03/1000},\n",
    "            \"gpt-4o\": {\"prompt\": 0.005/1000, \"completion\": 0.015/1000},\n",
    "            \n",
    "            # Anthropic models\n",
    "            \"claude-3-haiku\": {\"prompt\": 0.00025/1000, \"completion\": 0.00125/1000},\n",
    "            \"claude-3-sonnet\": {\"prompt\": 0.003/1000, \"completion\": 0.015/1000},\n",
    "            \"claude-3-opus\": {\"prompt\": 0.015/1000, \"completion\": 0.075/1000},\n",
    "            \"claude-3.5-sonnet\": {\"prompt\": 0.003/1000, \"completion\": 0.015/1000},\n",
    "            \n",
    "            # Google models (estimated pricing)\n",
    "            \"gemini-1.5-pro\": {\"prompt\": 0.0035/1000, \"completion\": 0.0105/1000},\n",
    "            \"gemini-1.5-flash\": {\"prompt\": 0.00075/1000, \"completion\": 0.003/1000},\n",
    "            \"gemini-pro\": {\"prompt\": 0.0005/1000, \"completion\": 0.0015/1000},\n",
    "            \n",
    "            # Default fallback\n",
    "            \"default\": {\"prompt\": 0.001/1000, \"completion\": 0.003/1000}\n",
    "        }\n",
    "    \n",
    "    def _get_safe_encoding(self, model_name: str):\n",
    "        \"\"\"Get encoding with safe fallbacks for unsupported models\"\"\"\n",
    "        try:\n",
    "            # Try to get model-specific encoding\n",
    "            return tiktoken.encoding_for_model(model_name)\n",
    "        except KeyError:\n",
    "            print(f\"INFO: No specific tokenizer for {model_name}, using fallback\")\n",
    "            try:\n",
    "                # Common fallbacks for different model families\n",
    "                if \"gpt-4\" in model_name.lower():\n",
    "                    return tiktoken.encoding_for_model(\"gpt-4\")\n",
    "                elif \"gpt-3.5\" in model_name.lower():\n",
    "                    return tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "                elif \"claude\" in model_name.lower():\n",
    "                    # Claude uses similar tokenization to GPT-4\n",
    "                    return tiktoken.get_encoding(\"cl100k_base\")\n",
    "                elif \"gemini\" in model_name.lower():\n",
    "                    # Use general tokenizer for Gemini\n",
    "                    return tiktoken.get_encoding(\"cl100k_base\")\n",
    "                else:\n",
    "                    # Universal fallback\n",
    "                    return tiktoken.get_encoding(\"cl100k_base\")\n",
    "            except Exception as e:\n",
    "                print(f\"WARN: Could not initialize tokenizer: {e}\")\n",
    "                return None\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens with robust fallback estimation\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "            \n",
    "        text = str(text)  # Ensure it's a string\n",
    "        \n",
    "        if not self.enable_token_counting or self.encoding is None:\n",
    "            # Fallback: estimate based on word count\n",
    "            # Different models have different token-to-word ratios\n",
    "            words = len(text.split())\n",
    "            if \"gemini\" in self.model_name.lower():\n",
    "                return int(words * 1.2)  # Gemini tends to be more efficient\n",
    "            elif \"claude\" in self.model_name.lower():\n",
    "                return int(words * 1.3)  # Claude similar to GPT\n",
    "            else:\n",
    "                return int(words * 1.3)  # General estimation\n",
    "        \n",
    "        try:\n",
    "            return len(self.encoding.encode(text))\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: Token counting failed: {e}\")\n",
    "            # Final fallback: character-based estimation\n",
    "            return len(text) // 4  # Rough approximation: 4 chars per token\n",
    "    \n",
    "    def get_token_price(self, model_name: str) -> Dict[str, float]:\n",
    "        \"\"\"Get token pricing with fallback\"\"\"\n",
    "        # Direct match\n",
    "        if model_name in self.token_prices:\n",
    "            return self.token_prices[model_name]\n",
    "        \n",
    "        # Fuzzy matching for model families\n",
    "        model_lower = model_name.lower()\n",
    "        for key in self.token_prices:\n",
    "            if key.replace(\"-\", \"\").replace(\".\", \"\") in model_lower.replace(\"-\", \"\").replace(\".\", \"\"):\n",
    "                return self.token_prices[key]\n",
    "        \n",
    "        # Final fallback\n",
    "        print(f\"INFO: Using default pricing for unknown model: {model_name}\")\n",
    "        return self.token_prices[\"default\"]\n",
    "    \n",
    "    def start_experiment(self, problem_id: str, problem_text: str, original_solution: str) -> str:\n",
    "        \"\"\"Start tracking a new experiment\"\"\"\n",
    "        experiment_id = f\"exp_{int(time.time())}_{problem_id}\"\n",
    "        \n",
    "        self.current_experiment = ExperimentMetrics(\n",
    "            experiment_id=experiment_id,\n",
    "            problem_id=problem_id,\n",
    "            problem_text=problem_text,\n",
    "            timestamp=time.time(),\n",
    "            original_solution=original_solution,\n",
    "            verified_solution=\"\",\n",
    "            final_decision=\"\",\n",
    "            total_processing_time_ms=0,\n",
    "            total_llm_calls=0,\n",
    "            total_tokens=0,\n",
    "            total_prompt_tokens=0,\n",
    "            total_completion_tokens=0,\n",
    "            total_steps=0,\n",
    "            steps_modified=0,\n",
    "            step_metrics=[],\n",
    "            original_has_errors=False,\n",
    "            verified_fixes_errors=False,\n",
    "            improvement_score=0.0,\n",
    "            estimated_cost_usd=0.0,\n",
    "            stage_outputs={},\n",
    "            error_log=[]\n",
    "        )\n",
    "        \n",
    "        return experiment_id\n",
    "    \n",
    "    def track_llm_call(self, stage: str, step_index: int, prompt: str, \n",
    "                      response: str, latency_ms: float, success: bool = True, \n",
    "                      error: str = None) -> LLMCallMetrics:\n",
    "        \"\"\"Track individual LLM call with robust token counting\"\"\"\n",
    "        call_id = f\"{stage}_{step_index}_{int(time.time())}\"\n",
    "        \n",
    "        # Safe token counting\n",
    "        try:\n",
    "            prompt_tokens = self.count_tokens(prompt)\n",
    "            completion_tokens = self.count_tokens(response) if response else 0\n",
    "        except Exception as e:\n",
    "            print(f\"WARN: Token counting failed for call {call_id}: {e}\")\n",
    "            # Emergency fallback\n",
    "            prompt_tokens = len(str(prompt).split()) if prompt else 0\n",
    "            completion_tokens = len(str(response).split()) if response else 0\n",
    "        \n",
    "        total_tokens = prompt_tokens + completion_tokens\n",
    "        \n",
    "        call_metrics = LLMCallMetrics(\n",
    "            call_id=call_id,\n",
    "            stage=stage,\n",
    "            step_index=step_index,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            completion_tokens=completion_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            latency_ms=latency_ms,\n",
    "            timestamp=time.time(),\n",
    "            success=success,\n",
    "            error_message=error\n",
    "        )\n",
    "        \n",
    "        # Update experiment totals\n",
    "        if self.current_experiment:\n",
    "            self.current_experiment.total_llm_calls += 1\n",
    "            self.current_experiment.total_tokens += total_tokens\n",
    "            self.current_experiment.total_prompt_tokens += prompt_tokens\n",
    "            self.current_experiment.total_completion_tokens += completion_tokens\n",
    "            \n",
    "            # Update cost estimation\n",
    "            try:\n",
    "                prices = self.get_token_price(self.model_name)\n",
    "                call_cost = (prompt_tokens * prices[\"prompt\"] + \n",
    "                           completion_tokens * prices[\"completion\"])\n",
    "                self.current_experiment.estimated_cost_usd += call_cost\n",
    "            except Exception as e:\n",
    "                print(f\"WARN: Cost calculation failed: {e}\")\n",
    "        \n",
    "        return call_metrics\n",
    "    \n",
    "    def track_step_verification(self, step_index: int, original_step: str, \n",
    "                              variants: List[str], selected_variant: str,\n",
    "                              selection_reason: str, processing_time_ms: float,\n",
    "                              llm_calls: List[LLMCallMetrics]):\n",
    "        \"\"\"Track step-level verification metrics\"\"\"\n",
    "        step_metrics = StepMetrics(\n",
    "            step_index=step_index,\n",
    "            original_step=original_step,\n",
    "            variants_generated=len(variants),\n",
    "            variants_successful=len([v for v in variants if v and v.strip()]),\n",
    "            selected_variant=selected_variant,\n",
    "            selection_reason=selection_reason,\n",
    "            step_processing_time_ms=processing_time_ms,\n",
    "            llm_calls=llm_calls\n",
    "        )\n",
    "        \n",
    "        if self.current_experiment:\n",
    "            self.current_experiment.step_metrics.append(step_metrics)\n",
    "            if original_step != selected_variant:\n",
    "                self.current_experiment.steps_modified += 1\n",
    "    \n",
    "    def track_stage_output(self, stage: str, output: Any):\n",
    "        \"\"\"Track outputs from different stages\"\"\"\n",
    "        if self.current_experiment:\n",
    "            self.current_experiment.stage_outputs[stage] = output\n",
    "    \n",
    "    def track_error(self, error: str):\n",
    "        \"\"\"Track errors during processing\"\"\"\n",
    "        if self.current_experiment:\n",
    "            self.current_experiment.error_log.append(f\"{time.time()}: {error}\")\n",
    "    \n",
    "    def finish_experiment(self, verified_solution: str, final_decision: str,\n",
    "                         total_processing_time_ms: float, improvement_score: float = 0.0):\n",
    "        \"\"\"Complete experiment tracking\"\"\"\n",
    "        if self.current_experiment:\n",
    "            self.current_experiment.verified_solution = verified_solution\n",
    "            self.current_experiment.final_decision = final_decision\n",
    "            self.current_experiment.total_processing_time_ms = total_processing_time_ms\n",
    "            self.current_experiment.total_steps = len(self.current_experiment.step_metrics)\n",
    "            self.current_experiment.improvement_score = improvement_score\n",
    "            \n",
    "            # Add to completed experiments\n",
    "            self.all_experiments.append(self.current_experiment)\n",
    "            self.current_experiment = None\n",
    "    \n",
    "    def get_summary_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary statistics across all experiments\"\"\"\n",
    "        if not self.all_experiments:\n",
    "            return {}\n",
    "        \n",
    "        total_experiments = len(self.all_experiments)\n",
    "        successful_experiments = [exp for exp in self.all_experiments if not exp.error_log]\n",
    "        \n",
    "        return {\n",
    "            \"total_experiments\": total_experiments,\n",
    "            \"successful_experiments\": len(successful_experiments),\n",
    "            \"success_rate\": len(successful_experiments) / total_experiments,\n",
    "            \"total_llm_calls\": sum(exp.total_llm_calls for exp in self.all_experiments),\n",
    "            \"total_tokens\": sum(exp.total_tokens for exp in self.all_experiments),\n",
    "            \"total_cost_usd\": sum(exp.estimated_cost_usd for exp in self.all_experiments),\n",
    "            \"avg_processing_time_ms\": sum(exp.total_processing_time_ms for exp in self.all_experiments) / total_experiments,\n",
    "            \"replacement_rate\": sum(1 for exp in self.all_experiments if exp.final_decision == \"REPLACE\") / total_experiments,\n",
    "            \"avg_steps_per_problem\": sum(exp.total_steps for exp in self.all_experiments) / total_experiments,\n",
    "            \"avg_modifications_per_problem\": sum(exp.steps_modified for exp in self.all_experiments) / total_experiments,\n",
    "            \"avg_improvement_score\": sum(exp.improvement_score for exp in self.all_experiments) / total_experiments,\n",
    "            \"avg_tokens_per_call\": sum(exp.total_tokens for exp in self.all_experiments) / sum(exp.total_llm_calls for exp in self.all_experiments) if sum(exp.total_llm_calls for exp in self.all_experiments) > 0 else 0,\n",
    "            \"model_name\": self.model_name\n",
    "        }\n",
    "    \n",
    "    def export_detailed_results(self, filename: str):\n",
    "        \"\"\"Export detailed results to JSON with error handling\"\"\"\n",
    "        try:\n",
    "            export_data = {\n",
    "                \"summary\": self.get_summary_stats(),\n",
    "                \"experiments\": [asdict(exp) for exp in self.all_experiments],\n",
    "                \"metadata\": {\n",
    "                    \"export_timestamp\": time.time(),\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"token_counting_enabled\": self.enable_token_counting,\n",
    "                    \"total_experiments\": len(self.all_experiments)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(export_data, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"âœ“ Results exported successfully to {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Export failed: {e}\")\n",
    "            # Try simplified export\n",
    "            try:\n",
    "                simple_data = {\n",
    "                    \"summary\": self.get_summary_stats(),\n",
    "                    \"experiment_count\": len(self.all_experiments),\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "                fallback_filename = f\"fallback_{filename}\"\n",
    "                with open(fallback_filename, 'w') as f:\n",
    "                    json.dump(simple_data, f, indent=2, default=str)\n",
    "                print(f\"âœ“ Fallback export saved to {fallback_filename}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âœ— Even fallback export failed: {e2}\")\n",
    "    \n",
    "    def print_experiment_summary(self, experiment_id: str = None):\n",
    "        \"\"\"Print summary for specific experiment or latest\"\"\"\n",
    "        exp = None\n",
    "        if experiment_id:\n",
    "            exp = next((e for e in self.all_experiments if e.experiment_id == experiment_id), None)\n",
    "        else:\n",
    "            exp = self.all_experiments[-1] if self.all_experiments else None\n",
    "        \n",
    "        if not exp:\n",
    "            print(\"No experiment found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EXPERIMENT SUMMARY: {exp.experiment_id}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"Problem: {exp.problem_text[:100]}...\")\n",
    "        print(f\"Total Steps: {exp.total_steps}\")\n",
    "        print(f\"Steps Modified: {exp.steps_modified}\")\n",
    "        print(f\"Final Decision: {exp.final_decision}\")\n",
    "        print(f\"Processing Time: {exp.total_processing_time_ms:.2f}ms\")\n",
    "        print(f\"LLM Calls: {exp.total_llm_calls}\")\n",
    "        print(f\"Total Tokens: {exp.total_tokens:,}\")\n",
    "        print(f\"Estimated Cost: ${exp.estimated_cost_usd:.4f}\")\n",
    "        print(f\"Improvement Score: {exp.improvement_score:.2f}\")\n",
    "        \n",
    "        if exp.error_log:\n",
    "            print(f\"\\nErrors ({len(exp.error_log)}):\")\n",
    "            for error in exp.error_log[-3:]:  # Show last 3 errors\n",
    "                print(f\"  - {error}\")\n",
    "        \n",
    "        print(f\"\\nStep Breakdown:\")\n",
    "        for i, step in enumerate(exp.step_metrics):\n",
    "            modified = \"âœ“\" if step.original_step != step.selected_variant else \"â—‹\"\n",
    "            print(f\"  Step {i+1} {modified}: {step.variants_generated} variants, \"\n",
    "                  f\"{step.step_processing_time_ms:.1f}ms, \"\n",
    "                  f\"{len(step.llm_calls)} LLM calls\")\n",
    "\n",
    "    def print_batch_summary(self):\n",
    "        \"\"\"Print comprehensive batch summary\"\"\"\n",
    "        stats = self.get_summary_stats()\n",
    "        if not stats:\n",
    "            print(\"No experiments completed yet\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"BATCH EXPERIMENT SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"Total Experiments: {stats['total_experiments']}\")\n",
    "        print(f\"Successful: {stats['successful_experiments']}\")\n",
    "        print(f\"Success Rate: {stats['success_rate']:.1%}\")\n",
    "        print(f\"Total LLM Calls: {stats['total_llm_calls']:,}\")\n",
    "        print(f\"Total Tokens: {stats['total_tokens']:,}\")\n",
    "        print(f\"Total Cost: ${stats['total_cost_usd']:.4f}\")\n",
    "        print(f\"Avg Processing Time: {stats['avg_processing_time_ms']:.0f}ms\")\n",
    "        print(f\"Replacement Rate: {stats['replacement_rate']:.1%}\")\n",
    "        print(f\"Avg Steps per Problem: {stats['avg_steps_per_problem']:.1f}\")\n",
    "        print(f\"Avg Modifications per Problem: {stats['avg_modifications_per_problem']:.1f}\")\n",
    "        print(f\"Avg Tokens per Call: {stats['avg_tokens_per_call']:.0f}\")\n",
    "\n",
    "# Safe context manager for experiments\n",
    "class SafeExperimentContext:\n",
    "    \"\"\"Context manager that ensures experiments are properly finished even on errors\"\"\"\n",
    "    \n",
    "    def __init__(self, tracker: ExperimentTracker, problem_id: str, problem_text: str, original_solution: str):\n",
    "        self.tracker = tracker\n",
    "        self.problem_id = problem_id\n",
    "        self.problem_text = problem_text\n",
    "        self.original_solution = original_solution\n",
    "        self.start_time = None\n",
    "        self.experiment_id = None\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.experiment_id = self.tracker.start_experiment(\n",
    "            self.problem_id, self.problem_text, self.original_solution\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        total_time = (time.time() - self.start_time) * 1000 if self.start_time else 0\n",
    "        \n",
    "        if exc_type is not None:\n",
    "            # Error occurred\n",
    "            self.tracker.track_error(f\"Experiment failed: {exc_val}\")\n",
    "            self.tracker.finish_experiment(\n",
    "                verified_solution=\"\",\n",
    "                final_decision=\"ERROR\",\n",
    "                total_processing_time_ms=total_time,\n",
    "                improvement_score=-1.0\n",
    "            )\n",
    "        elif self.tracker.current_experiment:\n",
    "            # Experiment wasn't manually finished\n",
    "            self.tracker.finish_experiment(\n",
    "                verified_solution=\"\",\n",
    "                final_decision=\"INCOMPLETE\",\n",
    "                total_processing_time_ms=total_time,\n",
    "                improvement_score=0.0\n",
    "            )\n",
    "\n",
    "# Additional Analysis Functions (updated for robustness)\n",
    "def analyze_variant_quality(tracker: ExperimentTracker) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze variant generation quality with error handling\"\"\"\n",
    "    try:\n",
    "        all_steps = []\n",
    "        for exp in tracker.all_experiments:\n",
    "            all_steps.extend(exp.step_metrics)\n",
    "        \n",
    "        if not all_steps:\n",
    "            return {\"error\": \"No step metrics available\"}\n",
    "        \n",
    "        total_variants = sum(s.variants_generated for s in all_steps)\n",
    "        total_successful = sum(s.variants_successful for s in all_steps)\n",
    "        \n",
    "        return {\n",
    "            \"total_steps_analyzed\": len(all_steps),\n",
    "            \"avg_variants_per_step\": total_variants / len(all_steps),\n",
    "            \"variant_success_rate\": total_successful / total_variants if total_variants > 0 else 0,\n",
    "            \"steps_with_multiple_variants\": len([s for s in all_steps if s.variants_generated > 1]),\n",
    "            \"modification_rate\": len([s for s in all_steps if s.original_step != s.selected_variant]) / len(all_steps)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Analysis failed: {e}\"}\n",
    "\n",
    "def analyze_efficiency_patterns(tracker: ExperimentTracker) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze efficiency patterns with error handling\"\"\"\n",
    "    try:\n",
    "        if not tracker.all_experiments:\n",
    "            return {\"error\": \"No experiments available\"}\n",
    "        \n",
    "        successful_experiments = [exp for exp in tracker.all_experiments if not exp.error_log]\n",
    "        efficient_experiments = [exp for exp in successful_experiments \n",
    "                               if exp.total_processing_time_ms < 30000]  # < 30 seconds\n",
    "        \n",
    "        total_tokens = sum(exp.total_tokens for exp in successful_experiments)\n",
    "        total_steps = sum(exp.total_steps for exp in successful_experiments)\n",
    "        total_calls = sum(exp.total_llm_calls for exp in successful_experiments)\n",
    "        \n",
    "        return {\n",
    "            \"total_successful_experiments\": len(successful_experiments),\n",
    "            \"efficiency_rate\": len(efficient_experiments) / len(successful_experiments) if successful_experiments else 0,\n",
    "            \"avg_tokens_per_step\": total_tokens / total_steps if total_steps > 0 else 0,\n",
    "            \"avg_calls_per_step\": total_calls / total_steps if total_steps > 0 else 0,\n",
    "            \"avg_processing_time_ms\": sum(exp.total_processing_time_ms for exp in successful_experiments) / len(successful_experiments) if successful_experiments else 0,\n",
    "            \"fastest_experiment_ms\": min(exp.total_processing_time_ms for exp in successful_experiments) if successful_experiments else 0,\n",
    "            \"slowest_experiment_ms\": max(exp.total_processing_time_ms for exp in successful_experiments) if successful_experiments else 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Efficiency analysis failed: {e}\"}\n",
    "\n",
    "# Quick integration function for immediate use\n",
    "def create_robust_tracker(model_name: str, enable_token_counting: bool = True) -> ExperimentTracker:\n",
    "    \"\"\"Create a tracker with robust error handling\"\"\"\n",
    "    try:\n",
    "        tracker = ExperimentTracker(model_name=model_name, enable_token_counting=enable_token_counting)\n",
    "        print(f\"âœ“ Tracker initialized successfully for model: {model_name}\")\n",
    "        return tracker\n",
    "    except Exception as e:\n",
    "        print(f\"WARN: Tracker initialization had issues: {e}\")\n",
    "        print(\"Continuing with fallback configuration...\")\n",
    "        return ExperimentTracker(model_name=model_name, enable_token_counting=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the robust tracker\n",
    "    tracker = create_robust_tracker(\"gemini-1.5-pro\")\n",
    "    \n",
    "    # Test token counting\n",
    "    test_text = \"This is a test sentence for token counting.\"\n",
    "    tokens = tracker.count_tokens(test_text)\n",
    "    print(f\"Test token count: {tokens} tokens for: '{test_text}'\")\n",
    "    \n",
    "    print(\"âœ“ Robust tracker ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expriment Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_steps(steps):\n",
    "    formatted_steps = \"\"\n",
    "    for step in steps:\n",
    "        new_step = f\"<Step>{step['text']}</Step>\\n\"\n",
    "        formatted_steps += new_step\n",
    "    return formatted_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread-safe results collection\n",
    "results_lock = Lock()\n",
    "results = []\n",
    "\n",
    "def thread_safe_evaluate_sample(row_dict, engine, model_name=\"gemini-1.5-pro\"):\n",
    "    \"\"\"\n",
    "    Thread-safe wrapper for evaluation with individual tracker per thread\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tracker = create_robust_tracker(model_name, enable_token_counting=True)\n",
    "        \n",
    "        # Extract problem data\n",
    "        problem = row_dict['problem']\n",
    "        steps_list = ast.literal_eval(row_dict['steps']) if isinstance(row_dict['steps'], str) else row_dict['steps']\n",
    "        solution_steps = format_steps(steps_list)\n",
    "        problem_id = f\"problem_{hash(problem)}\"\n",
    "        \n",
    "        # Use safe context manager\n",
    "        with SafeExperimentContext(tracker, problem_id, problem, solution_steps) as ctx:\n",
    "            # Create TextGrad variables\n",
    "            question = Variable(problem, requires_grad=True, role_description=\"math question\")\n",
    "            solution = Variable(solution_steps, requires_grad=True, role_description=\"solution\")\n",
    "            verification_prompt = Variable(\"Verify and improve this mathematical solution step by step.\",\n",
    "                                          requires_grad=False, role_description=\"verification prompt\")\n",
    "            \n",
    "            # Create tracked verifier\n",
    "            verifier = TextualVerifierExperiment(\n",
    "                verifier_engine=engine,\n",
    "                step_eval_iterations=3,\n",
    "                logger=False,  # Disable logging in concurrent mode\n",
    "                tracker=tracker\n",
    "            )\n",
    "            \n",
    "            # Perform verification\n",
    "            verified_result = verifier.verify(\n",
    "                instance=question,\n",
    "                prompt=verification_prompt,\n",
    "                calculation=solution\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            improvement_score = 0.5 if verified_result.value != solution_steps else 0.0\n",
    "            final_decision = \"REPLACE\" if verified_result.value != solution_steps else \"SUFFICIENT\"\n",
    "            \n",
    "            # Manually finish the experiment since we're using context manager\n",
    "            if tracker.current_experiment:\n",
    "                total_time = (time.time() - ctx.start_time) * 1000\n",
    "                tracker.finish_experiment(\n",
    "                    verified_solution=verified_result.value,\n",
    "                    final_decision=final_decision,\n",
    "                    total_processing_time_ms=total_time,\n",
    "                    improvement_score=improvement_score\n",
    "                )\n",
    "            \n",
    "            # Extract metrics for return\n",
    "            experiment_data = {\n",
    "                'problem_id': problem_id,\n",
    "                'original_problem': problem,\n",
    "                'original_solution': solution_steps,\n",
    "                'verified_solution': verified_result.value,\n",
    "                'final_decision': final_decision,\n",
    "                'improvement_score': improvement_score,\n",
    "                'processing_time_ms': tracker.all_experiments[-1].total_processing_time_ms if tracker.all_experiments else 0,\n",
    "                'total_llm_calls': tracker.all_experiments[-1].total_llm_calls if tracker.all_experiments else 0,\n",
    "                'total_tokens': tracker.all_experiments[-1].total_tokens if tracker.all_experiments else 0,\n",
    "                'estimated_cost': tracker.all_experiments[-1].estimated_cost_usd if tracker.all_experiments else 0,\n",
    "                'steps_processed': tracker.all_experiments[-1].total_steps if tracker.all_experiments else 0,\n",
    "                'steps_modified': tracker.all_experiments[-1].steps_modified if tracker.all_experiments else 0,\n",
    "                'success': True,\n",
    "                'error_message': None\n",
    "            }\n",
    "            \n",
    "            return experiment_data\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Return error information\n",
    "        return {\n",
    "            'problem_id': f\"problem_{hash(row_dict.get('problem', 'unknown'))}\",\n",
    "            'original_problem': row_dict.get('problem', ''),\n",
    "            'original_solution': '',\n",
    "            'verified_solution': '',\n",
    "            'final_decision': 'ERROR',\n",
    "            'improvement_score': -1.0,\n",
    "            'processing_time_ms': 0,\n",
    "            'total_llm_calls': 0,\n",
    "            'total_tokens': 0,\n",
    "            'estimated_cost': 0,\n",
    "            'steps_processed': 0,\n",
    "            'steps_modified': 0,\n",
    "            'success': False,\n",
    "            'error_message': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Running Concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch tracking aggregator\n",
    "class BatchTracker:\n",
    "    \"\"\"Aggregate results from multiple thread-local trackers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def add_result(self, result):\n",
    "        with results_lock:\n",
    "            self.results.append(result)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        if not self.results:\n",
    "            return {}\n",
    "        \n",
    "        successful = [r for r in self.results if r['success']]\n",
    "        failed = [r for r in self.results if not r['success']]\n",
    "        \n",
    "        return {\n",
    "            'total_experiments': len(self.results),\n",
    "            'successful': len(successful),\n",
    "            'failed': len(failed),\n",
    "            'success_rate': len(successful) / len(self.results) if self.results else 0,\n",
    "            'total_processing_time': time.time() - self.start_time,\n",
    "            'total_llm_calls': sum(r['total_llm_calls'] for r in successful),\n",
    "            'total_tokens': sum(r['total_tokens'] for r in successful),\n",
    "            'total_cost': sum(r['estimated_cost'] for r in successful),\n",
    "            'replacement_rate': sum(1 for r in successful if r['final_decision'] == 'REPLACE') / len(successful) if successful else 0,\n",
    "            'avg_processing_time_ms': sum(r['processing_time_ms'] for r in successful) / len(successful) if successful else 0,\n",
    "            'avg_improvement_score': sum(r['improvement_score'] for r in successful) / len(successful) if successful else 0\n",
    "        }\n",
    "    \n",
    "    def print_summary(self):\n",
    "        summary = self.get_summary()\n",
    "        if not summary:\n",
    "            print(\"No results to summarize\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CONCURRENT BATCH EXPERIMENT SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total Experiments: {summary['total_experiments']}\")\n",
    "        print(f\"Successful: {summary['successful']}\")\n",
    "        print(f\"Failed: {summary['failed']}\")\n",
    "        print(f\"Success Rate: {summary['success_rate']:.1%}\")\n",
    "        print(f\"Total Processing Time: {summary['total_processing_time']:.1f}s\")\n",
    "        print(f\"Total LLM Calls: {summary['total_llm_calls']:,}\")\n",
    "        print(f\"Total Tokens: {summary['total_tokens']:,}\")\n",
    "        print(f\"Total Cost: ${summary['total_cost']:.4f}\")\n",
    "        print(f\"Replacement Rate: {summary['replacement_rate']:.1%}\")\n",
    "        print(f\"Avg Processing Time: {summary['avg_processing_time_ms']:.0f}ms\")\n",
    "        print(f\"Avg Improvement Score: {summary['avg_improvement_score']:.2f}\")\n",
    "        \n",
    "        if summary['failed'] > 0:\n",
    "            print(f\"\\nError Analysis:\")\n",
    "            error_results = [r for r in self.results if not r['success']]\n",
    "            error_types = {}\n",
    "            for error_result in error_results:\n",
    "                error_msg = error_result['error_message']\n",
    "                error_type = error_msg.split(':')[0] if error_msg else 'Unknown'\n",
    "                error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "            \n",
    "            for error_type, count in error_types.items():\n",
    "                print(f\"  {error_type}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_concurrent_experiments(sample_data, engine, model_name=\"gemini-1.5-pro\", max_workers=8):\n",
    "    \"\"\"\n",
    "    Fixed version of your concurrent processing code\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize batch tracker\n",
    "    batch_tracker = BatchTracker()\n",
    "    \n",
    "    print(f\"Starting concurrent processing with {max_workers} workers...\")\n",
    "    print(f\"Processing {len(sample_data)} samples...\")\n",
    "    \n",
    "    # Reduce max_workers to avoid overwhelming the system\n",
    "    # 128 workers is usually too many and can cause issues\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks with proper parameters\n",
    "        futures = [\n",
    "            executor.submit(thread_safe_evaluate_sample, row.to_dict(), engine, model_name) \n",
    "            for _, row in sample_data.iterrows()\n",
    "        ]\n",
    "        \n",
    "        # Process results with progress tracking\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "            try:\n",
    "                result = future.result(timeout=300)  # 5-minute timeout per task\n",
    "                if result is not None:\n",
    "                    batch_tracker.add_result(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed: {e}\")\n",
    "                # Add error result\n",
    "                batch_tracker.add_result({\n",
    "                    'problem_id': 'unknown',\n",
    "                    'success': False,\n",
    "                    'error_message': str(e),\n",
    "                    'final_decision': 'ERROR',\n",
    "                    'improvement_score': -1.0,\n",
    "                    'processing_time_ms': 0,\n",
    "                    'total_llm_calls': 0,\n",
    "                    'total_tokens': 0,\n",
    "                    'estimated_cost': 0,\n",
    "                    'steps_processed': 0,\n",
    "                    'steps_modified': 0\n",
    "                })\n",
    "    \n",
    "    # Print summary\n",
    "    batch_tracker.print_summary()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    experiment_df = pd.DataFrame(batch_tracker.results)\n",
    "    \n",
    "    return experiment_df, batch_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: No specific tokenizer for gemini-1.5-pro, using fallback\n",
      "âœ“ Tracker initialized successfully for model: gemini-1.5-pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 0.1 seconds\n",
      "\n",
      "============================================================\n",
      "CONCURRENT BATCH EXPERIMENT SUMMARY\n",
      "============================================================\n",
      "Total Experiments: 1\n",
      "Successful: 1\n",
      "Failed: 0\n",
      "Success Rate: 100.0%\n",
      "Total Processing Time: 0.1s\n",
      "Total LLM Calls: 61\n",
      "Total Tokens: 112,035\n",
      "Total Cost: $0.4857\n",
      "Replacement Rate: 100.0%\n",
      "Avg Processing Time: 145ms\n",
      "Avg Improvement Score: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# OPTION 1: Direct replacement of your code (with fixes)\n",
    "batch_tracker = BatchTracker()\n",
    "\n",
    "# Reduce max_workers for stability (128 is too high)\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:  # Changed from 128 to 8\n",
    "    # Fixed: Added missing engine parameter and used proper function\n",
    "    futures = [\n",
    "        executor.submit(thread_safe_evaluate_sample, row.to_dict(), engine, \"gemini-1.5-pro\") \n",
    "        for _, row in sample[6:7].iterrows()  # Your slice\n",
    "    ]\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        try:\n",
    "            result = future.result(timeout=300)  # Add timeout\n",
    "            if result is not None:\n",
    "                batch_tracker.add_result(result)\n",
    "                results.append(result)  # Keep your original results list\n",
    "        except Exception as e:\n",
    "            print(f\"Task failed: {e}\")\n",
    "            # Add error to results\n",
    "            error_result = {\n",
    "                'success': False,\n",
    "                'error_message': str(e),\n",
    "                'final_decision': 'ERROR'\n",
    "            }\n",
    "            batch_tracker.add_result(error_result)\n",
    "            results.append(error_result)\n",
    "\n",
    "# Create DataFrame from results\n",
    "experiment_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Completed in {time.time() - start_time:.1f} seconds\")\n",
    "batch_tracker.print_summary()\n",
    "\n",
    "# Save results\n",
    "experiment_df.to_csv('results/prm800k-03-algo3-clean-result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "860e0dc2175a55dd9a80ac360791d93c13f4935a3c9aca3a9a76262c7d69eace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_textgrad = pd.read_csv(\"results/raw_textgrad.csv\")\n",
    "tv_textgrad = pd.read_csv(\"results/tv_textgrad.csv\")\n",
    "tv2_textgrad = pd.read_csv(\"results/tv2_textgrad.csv\")\n",
    "tv3_textgrad = pd.read_csv(\"results/tv3_textgrad.csv\")\n",
    "tv4_textgrad = pd.read_csv(\"results/tv4_textgrad.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {\n",
    "    \"GPQA-Diamond\": \"GPQA-Diamond\",\n",
    "    \"MMLU-ML\": \"MMLU-ML\",\n",
    "    \"MMLU-CP\": \"MMLU-CP\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    # Now extract correctly\n",
    "    match = re.search(r\"(?i)Answer\\s*:\\s*([A-D])\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    return \"Z\" # Z -> means for None (Incorrect Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(row_data):\n",
    "    data = {\n",
    "        \"source\": row_data[\"source\"],\n",
    "        \"correct_answer\": row_data[\"correct_answer\"],\n",
    "        \"initial_answer\": extract_answer(row_data[\"raw_solution\"]),\n",
    "        \"zero_shot_answer\": extract_answer(row_data[\"solution_1\"]),\n",
    "        \"solution_answer_2\": extract_answer(row_data[\"solution_2\"]),\n",
    "        \"solution_answer_3\": extract_answer(row_data[\"solution_3\"]),\n",
    "        \"solution_answer_4\": extract_answer(row_data[\"solution_4\"]),\n",
    "        \"final_answer\": extract_answer(row_data[\"solution_5\"])\n",
    "    }\n",
    "    # Get zero-shot & final result\n",
    "    data[\"zero_shot_result\"] = data[\"correct_answer\"] == data[\"zero_shot_answer\"]\n",
    "    data[\"final_result\"] = data[\"correct_answer\"] == data[\"final_answer\"]\n",
    "\n",
    "    # Majority voting among solution_1 to solution_5\n",
    "    voted_answers = [\n",
    "        data[\"zero_shot_answer\"],\n",
    "        data[\"solution_answer_2\"],\n",
    "        data[\"solution_answer_3\"],\n",
    "        data[\"solution_answer_4\"],\n",
    "        data[\"final_answer\"]\n",
    "    ]\n",
    "    \n",
    "    # Filter out None values\n",
    "    voted_answers = [a for a in voted_answers if a is not None]\n",
    "\n",
    "    # Get majority answer\n",
    "    if voted_answers:\n",
    "        majority_vote = Counter(voted_answers).most_common(1)[0][0]\n",
    "    else:\n",
    "        majority_vote = None\n",
    "\n",
    "    data[\"majority_answer\"] = majority_vote\n",
    "    data[\"majority_result\"] = data[\"correct_answer\"] == majority_vote\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result_data(result_df):\n",
    "    processed_answer = []\n",
    "    for index, row in result_df.iterrows():\n",
    "        processed_answer.append(process_answer(row))\n",
    "    return processed_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_percentages(df, columns):\n",
    "    result = {}\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            result[col] = None\n",
    "            continue\n",
    "        non_null_count = df[col].notnull().sum()\n",
    "        if non_null_count == 0:\n",
    "            result[col] = None\n",
    "            continue\n",
    "        true_count = df[col].sum()\n",
    "        result[col] = round((true_count / non_null_count) * 100, 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_processed_data(dataset):\n",
    "    processed_answer_list = process_result_data(dataset)\n",
    "    processed_answer = pd.DataFrame(processed_answer_list)\n",
    "    clean_processed_answer = processed_answer.dropna()\n",
    "    return clean_processed_answer\n",
    "\n",
    "def analyze_percentage_data(dataset):\n",
    "    columns_to_check = [\"zero_shot_result\", \"final_result\", \"majority_result\"]\n",
    "    clean_df = get_clean_processed_data(dataset)\n",
    "    overall_result = {}\n",
    "\n",
    "    for name, source_key in {**sources, \"Combined\": None}.items():\n",
    "        if source_key is not None:\n",
    "            df_source = clean_df[clean_df[\"source\"] == source_key]\n",
    "        else:\n",
    "            df_source = clean_df\n",
    "        result = calculate_true_percentages(df_source, columns_to_check)\n",
    "        overall_result[name] = result\n",
    "\n",
    "    return overall_result\n",
    "\n",
    "def analyze_visual_iteration_data(dataset):\n",
    "    iterations = [\"zero_shot_answer\", \"solution_answer_2\", \"solution_answer_3\", \"solution_answer_4\", \"final_answer\"]\n",
    "    clean_df = get_clean_processed_data(dataset)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for name, source_key in {**sources, \"Combined\": None}.items():\n",
    "        if source_key is not None:\n",
    "            df_source = clean_df[clean_df[\"source\"] == source_key]\n",
    "        else:\n",
    "            df_source = clean_df\n",
    "\n",
    "        accuracies = []\n",
    "        for col in iterations:\n",
    "            correct = df_source[col] == df_source[\"correct_answer\"]\n",
    "            accuracy = correct.mean() * 100  # convert to percentage\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "        plt.plot(iterations, accuracies, marker='o', label=name)\n",
    "\n",
    "    plt.title(\"Accuracy at Each Iteration by Source\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis Raw TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "analyze_visual_iteration_data(raw_textgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages\n",
    "result = analyze_percentage_data(raw_textgrad)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis TextGrad with TextualVerifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "analyze_visual_iteration_data(tv_textgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages\n",
    "tv_result = analyze_percentage_data(tv_textgrad)\n",
    "tv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysis TextGrad with TextualVerifierV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "analyze_visual_iteration_data(tv2_textgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages\n",
    "tv2_result = analyze_percentage_data(tv2_textgrad)\n",
    "tv2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis TextGrad with TextualVerifierV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "analyze_visual_iteration_data(tv3_textgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages\n",
    "tv3_result = analyze_percentage_data(tv3_textgrad)\n",
    "tv3_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis TextGrad with TextualVerifierV4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "analyze_visual_iteration_data(tv4_textgrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentages\n",
    "tv3_result = analyze_percentage_data(tv4_textgrad)\n",
    "tv3_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "860e0dc2175a55dd9a80ac360791d93c13f4935a3c9aca3a9a76262c7d69eace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Optimization Evaluaton TV TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textgrad as tg\n",
    "from textgrad.engine import get_engine\n",
    "from textgrad.variable import Variable\n",
    "from textgrad.optimizer import TextualGradientDescent\n",
    "from textgrad.verifier import TextualVerifier\n",
    "from textgrad.loss import TextLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = pd.read_csv(\"csv/initial_solution.csv\")\n",
    "initial_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test size only 50 rows each datasets (Total 150 rows)\n",
    "\n",
    "df_gpqa = initial_solution[initial_solution['source'] == 'GPQA-Diamond'].head(50)\n",
    "df_mmlu_ml = initial_solution[initial_solution['source'] == 'MMLU-ML'].head(50)\n",
    "df_mmlu_cp = initial_solution[initial_solution['source'] == 'MMLU-CP'].head(50)\n",
    "df_test = pd.concat([df_gpqa, df_mmlu_ml, df_mmlu_cp], ignore_index=True)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = get_engine(\"gemini-1.5-pro\")\n",
    "tg.set_backward_engine(\"gemini-1.5-pro\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_raw_textgrad(row_data):\n",
    "    match = initial_solution[initial_solution[\"id\"] == row_data[\"id\"]]\n",
    "    if match.empty:\n",
    "        return None  # or raise error\n",
    "    formatted_question = match.iloc[0][\"formatted_question\"]\n",
    "    result = {\n",
    "        \"id\": row_data[\"id\"],\n",
    "        \"raw_solution\": row_data[\"raw_solution\"],\n",
    "        \"correct_answer\": row_data[\"correct_answer\"],\n",
    "        \"source\": row_data[\"source\"],\n",
    "        \"subject\": row_data[\"subject\"]\n",
    "    }\n",
    "    \n",
    "    solution = Variable(row_data[\"raw_solution\"],\n",
    "                    requires_grad=True,\n",
    "                    role_description=f\"Solution to the math question: {formatted_question}\")\n",
    "    loss_system_prompt = Variable(\"\"\"You will evaluate a solution to a math question. \n",
    "                                    Do not attempt to solve it yourself, do not give a solution, \n",
    "                                    only identify errors. Be super concise.\"\"\",\n",
    "                                    requires_grad=False,\n",
    "                                    role_description=\"system prompt\")\n",
    "    optimizer = TextualGradientDescent([solution])\n",
    "    loss = TextLoss(loss_system_prompt, engine=engine)\n",
    "\n",
    "    # TextualVerifier\n",
    "    verifier = TextualVerifier(verifier_engine=engine, step_eval_iterations=3, logger=False)\n",
    "    \n",
    "    # Iterate 5 times\n",
    "    for i in range(1, 6):\n",
    "        optimizer.zero_grad()  # Clean gradients\n",
    "        loss_result = loss(solution)\n",
    "\n",
    "        # TextualVerifier\n",
    "        verified_result = verifier.verify(instance=solution, \n",
    "                                    prompt=loss_system_prompt,\n",
    "                                    calculation=loss_result)\n",
    "        loss_result.set_value(verified_result.value) \n",
    "        \n",
    "        loss_result.backward()\n",
    "        optimizer.step()\n",
    "        result[f\"solution_{i}\"] = solution.value\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [\n",
    "        executor.submit(evaluate_with_raw_textgrad, row.to_dict()) \n",
    "        for _, row in initial_solution.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "\n",
    "raw_textgrad = pd.DataFrame(results)\n",
    "\n",
    "print(f\"Completed in {time.time() - start_time:.1f} seconds\")\n",
    "raw_textgrad.to_csv('results/tv_textgrad.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "860e0dc2175a55dd9a80ac360791d93c13f4935a3c9aca3a9a76262c7d69eace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
